{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current feature                                 ps_reg_01_plus_ps_car_04_cat    2 in   0.1\n",
      "Fold  0\n",
      "  Gini =  0.2858087926293843\n",
      "\n",
      "Fold  1\n",
      "  Gini =  0.28065255002558875\n",
      "\n",
      "Fold  2\n",
      "  Gini =  0.27326279519590446\n",
      "\n",
      "Fold  3\n",
      "  Gini =  0.2984693981259734\n",
      "\n",
      "Fold  4\n",
      "  Gini =  0.28646559797023086\n",
      "\n",
      "Gini for full training set:\n"
     ]
    }
   ],
   "source": [
    "MAX_ROUNDS = 370\n",
    "OPTIMIZE_ROUNDS = False\n",
    "LEARNING_RATE = 0.07\n",
    "EARLY_STOPPING_ROUNDS = 50 \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from numba import jit\n",
    "import time\n",
    "import gc\n",
    "import data_util \n",
    "\n",
    "@jit\n",
    "def eval_gini(y_true, y_prob):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_true = y_true[np.argsort(y_prob)]\n",
    "    ntrue = 0\n",
    "    gini = 0\n",
    "    delta = 0\n",
    "    n = len(y_true)\n",
    "    for i in range(n-1, -1, -1):\n",
    "        y_i = y_true[i]\n",
    "        ntrue += y_i\n",
    "        gini += y_i * delta\n",
    "        delta += 1 - y_i\n",
    "    gini = 1 - 2 * gini / (ntrue * (n - ntrue))\n",
    "    return gini\n",
    "\n",
    "def add_noise(series, noise_level):\n",
    "    return series * (1 + noise_level * np.random.randn(len(series)))\n",
    "\n",
    "\n",
    "train_df = data_util.load_train_data()\n",
    "test_df = data_util.load_test_data()\n",
    "\n",
    "train_features = [\n",
    "    \"ps_car_13\",  #            : 1571.65 / shadow  609.23\n",
    "    \"ps_reg_03\",  #            : 1408.42 / shadow  511.15\n",
    "    \"ps_ind_05_cat\",  #        : 1387.87 / shadow   84.72\n",
    "    \"ps_ind_03\",  #            : 1219.47 / shadow  230.55\n",
    "    \"ps_ind_15\",  #            :  922.18 / shadow  242.00\n",
    "    \"ps_reg_02\",  #            :  920.65 / shadow  267.50\n",
    "    \"ps_car_14\",  #            :  798.48 / shadow  549.58\n",
    "    \"ps_car_12\",  #            :  731.93 / shadow  293.62\n",
    "    \"ps_car_01_cat\",  #        :  698.07 / shadow  178.72\n",
    "    \"ps_car_07_cat\",  #        :  694.53 / shadow   36.35\n",
    "    \"ps_ind_17_bin\",  #        :  620.77 / shadow   23.15\n",
    "    \"ps_car_03_cat\",  #        :  611.73 / shadow   50.67\n",
    "    \"ps_reg_01\",  #            :  598.60 / shadow  178.57\n",
    "    \"ps_car_15\",  #            :  593.35 / shadow  226.43\n",
    "    \"ps_ind_01\",  #            :  547.32 / shadow  154.58\n",
    "    \"ps_ind_16_bin\",  #        :  475.37 / shadow   34.17\n",
    "    \"ps_ind_07_bin\",  #        :  435.28 / shadow   28.92\n",
    "    \"ps_car_06_cat\",  #        :  398.02 / shadow  212.43\n",
    "    \"ps_car_04_cat\",  #        :  376.87 / shadow   76.98\n",
    "    \"ps_ind_06_bin\",  #        :  370.97 / shadow   36.13\n",
    "    \"ps_car_09_cat\",  #        :  214.12 / shadow   81.38\n",
    "    \"ps_car_02_cat\",  #        :  203.03 / shadow   26.67\n",
    "    \"ps_ind_02_cat\",  #        :  189.47 / shadow   65.68\n",
    "    \"ps_car_11\",  #            :  173.28 / shadow   76.45\n",
    "    \"ps_car_05_cat\",  #        :  172.75 / shadow   62.92\n",
    "    \"ps_calc_09\",  #           :  169.13 / shadow  129.72\n",
    "    \"ps_calc_05\",  #           :  148.83 / shadow  120.68\n",
    "    \"ps_ind_08_bin\",  #        :  140.73 / shadow   27.63\n",
    "    \"ps_car_08_cat\",  #        :  120.87 / shadow   28.82\n",
    "    \"ps_ind_09_bin\",  #        :  113.92 / shadow   27.05\n",
    "    \"ps_ind_04_cat\",  #        :  107.27 / shadow   37.43\n",
    "    \"ps_ind_18_bin\",  #        :   77.42 / shadow   25.97\n",
    "    \"ps_ind_12_bin\",  #        :   39.67 / shadow   15.52\n",
    "    \"ps_ind_14\",  #            :   37.37 / shadow   16.65\n",
    "]\n",
    "\n",
    "# add combinations\n",
    "combs = [\n",
    "    ('ps_reg_01', 'ps_car_02_cat'),  \n",
    "    ('ps_reg_01', 'ps_car_04_cat'),\n",
    "]\n",
    "\n",
    "\n",
    "def target_encode(trn_series=None,    # Revised to encode validation series\n",
    "                  val_series=None,\n",
    "                  tst_series=None,\n",
    "                  target=None,\n",
    "                  min_samples_leaf=1,\n",
    "                  smoothing=1,\n",
    "                  noise_level=0):\n",
    "    \"\"\"\n",
    "    Smoothing is computed like in the following paper by Daniele Micci-Barreca\n",
    "    https://kaggle2.blob.core.windows.net/forum-message-attachments/225952/7441/high%20cardinality%20categoricals.pdf\n",
    "    trn_series : training categorical feature as a pd.Series\n",
    "    tst_series : test categorical feature as a pd.Series\n",
    "    target : target data as a pd.Series\n",
    "    min_samples_leaf (int) : minimum samples to take category average into account\n",
    "    smoothing (int) : smoothing effect to balance categorical average vs prior\n",
    "    \"\"\"\n",
    "    assert len(trn_series) == len(target)\n",
    "    assert trn_series.name == tst_series.name\n",
    "    temp = pd.concat([trn_series, target], axis=1)\n",
    "    # Compute target mean\n",
    "    averages = temp.groupby(by=trn_series.name)[target.name].agg([\"mean\", \"count\"])\n",
    "    # Compute smoothing\n",
    "    smoothing = 1 / (1 + np.exp(-(averages[\"count\"] - min_samples_leaf) / smoothing))\n",
    "    # Apply average function to all target data\n",
    "    prior = target.mean()\n",
    "    # The bigger the count the less full_avg is taken into account\n",
    "    averages[target.name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\n",
    "    averages.drop([\"mean\", \"count\"], axis=1, inplace=True)\n",
    "    # Apply averages to trn and tst series\n",
    "    ft_trn_series = pd.merge(\n",
    "        trn_series.to_frame(trn_series.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on=trn_series.name,\n",
    "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    # pd.merge does not keep the index so restore it\n",
    "    ft_trn_series.index = trn_series.index\n",
    "    ft_val_series = pd.merge(\n",
    "        val_series.to_frame(val_series.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on=val_series.name,\n",
    "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    # pd.merge does not keep the index so restore it\n",
    "    ft_val_series.index = val_series.index\n",
    "    ft_tst_series = pd.merge(\n",
    "        tst_series.to_frame(tst_series.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on=tst_series.name,\n",
    "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    # pd.merge does not keep the index so restore it\n",
    "    ft_tst_series.index = tst_series.index\n",
    "    return add_noise(ft_trn_series, noise_level), add_noise(ft_val_series, noise_level), add_noise(ft_tst_series, noise_level)\n",
    "\n",
    "\n",
    "# Process data\n",
    "id_test = test_df['id'].values\n",
    "id_train = train_df['id'].values\n",
    "y = train_df['target']\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "combs = [\n",
    "    ('ps_reg_01', 'ps_car_02_cat'),  \n",
    "    ('ps_reg_01', 'ps_car_04_cat'),\n",
    "]\n",
    "for n_c, (f1, f2) in enumerate(combs):\n",
    "    name1 = f1 + \"_plus_\" + f2\n",
    "    print('current feature %60s %4d in %5.1f'\n",
    "          % (name1, n_c + 1, (time.time() - start) / 60), end='')\n",
    "    print('\\r' * 75, end='')\n",
    "    train_df[name1] = train_df[f1].apply(lambda x: str(x)) + \"_\" + train_df[f2].apply(lambda x: str(x))\n",
    "    test_df[name1] = test_df[f1].apply(lambda x: str(x)) + \"_\" + test_df[f2].apply(lambda x: str(x))\n",
    "    # Label Encode\n",
    "    lbl = LabelEncoder()\n",
    "    lbl.fit(list(train_df[name1].values) + list(test_df[name1].values))\n",
    "    train_df[name1] = lbl.transform(list(train_df[name1].values))\n",
    "    test_df[name1] = lbl.transform(list(test_df[name1].values))\n",
    "\n",
    "    train_features.append(name1)\n",
    "\n",
    "    \n",
    "f_cats = [f for f in X.columns if \"_cat\" in f]    \n",
    "    \n",
    "#########################\n",
    "# train_df['na_sum'] = (train_df == -1).sum(axis=1)\n",
    "# train_df['na_ca_t_sum'] = (train_df[f_cats] == -1).sum(axis=1)\n",
    "# test_df['na_sum'] = (test_df == -1).sum(axis=1)\n",
    "# test_df['na_ca_t_sum'] = (test_df[f_cats] == -1).sum(axis=1)\n",
    "\n",
    "# train_features += ['na_ca_t_sum', 'na_sum']\n",
    "\n",
    "train_df['ps_car_13_+_ps_reg_03'] = train_df['ps_car_13'] + train_df['ps_reg_03']\n",
    "train_df['ps_car_13_-_ps_reg_03'] = train_df['ps_car_13'] - train_df['ps_reg_03']\n",
    "train_df['ps_car_13_x_ps_reg_03'] = train_df['ps_car_13'] * train_df['ps_reg_03']\n",
    "train_df['ps_car_13_/_ps_reg_03'] = train_df['ps_car_13'] / train_df['ps_reg_03']\n",
    "\n",
    "test_df['ps_car_13_+_ps_reg_03'] = test_df['ps_car_13'] + test_df['ps_reg_03']\n",
    "test_df['ps_car_13_-_ps_reg_03'] = test_df['ps_car_13'] - test_df['ps_reg_03']\n",
    "test_df['ps_car_13_x_ps_reg_03'] = test_df['ps_car_13'] * test_df['ps_reg_03']\n",
    "test_df['ps_car_13_/_ps_reg_03'] = test_df['ps_car_13'] / test_df['ps_reg_03']\n",
    "\n",
    "\n",
    "train_features+= ['ps_car_13_x_ps_reg_03', 'ps_car_13_/_ps_reg_03','ps_car_13_+_ps_reg_03','ps_car_13_-_ps_reg_03']\n",
    "#########################\n",
    "X = train_df[train_features]\n",
    "test_df = test_df[train_features]\n",
    "\n",
    "y_valid_pred = 0*y\n",
    "y_test_pred = 0\n",
    "\n",
    "# Set up folds\n",
    "K = 5\n",
    "kf = KFold(n_splits = K, random_state = 1, shuffle = True)\n",
    "np.random.seed(0)\n",
    "\n",
    "# Set up classifier\n",
    "model = XGBClassifier(    \n",
    "                        n_estimators=MAX_ROUNDS,\n",
    "                        max_depth=4,\n",
    "                        objective=\"binary:logistic\",\n",
    "                        learning_rate=LEARNING_RATE, \n",
    "                        subsample=.8,\n",
    "                        min_child_weight=.77,\n",
    "                        colsample_bytree=.8,\n",
    "                        scale_pos_weight=1.6,\n",
    "                        gamma=10,\n",
    "                        reg_alpha=8,\n",
    "                        reg_lambda=1.3,\n",
    "                     )\n",
    "\n",
    "# Run CV\n",
    "y_test_pred_all = np.zeros((len(test_df),5))\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(kf.split(train_df)):\n",
    "    \n",
    "    # Create data for this fold\n",
    "    y_train, y_valid = y.iloc[train_index].copy(), y.iloc[test_index]\n",
    "    X_train, X_valid = X.iloc[train_index,:].copy(), X.iloc[test_index,:].copy()\n",
    "    X_test = test_df.copy()\n",
    "    print( \"\\nFold \", i)\n",
    "    \n",
    "    \n",
    "    for f in f_cats:\n",
    "        X_train[f + \"_avg\"], X_valid[f + \"_avg\"], X_test[f + \"_avg\"] = target_encode(\n",
    "                                                        trn_series=X_train[f],\n",
    "                                                        val_series=X_valid[f],\n",
    "                                                        tst_series=X_test[f],\n",
    "                                                        target=y_train,\n",
    "                                                        min_samples_leaf=200,\n",
    "                                                        smoothing=10,\n",
    "                                                        noise_level=0\n",
    "                                                        )\n",
    "    \n",
    "    # Run model for this fold\n",
    "    if OPTIMIZE_ROUNDS:\n",
    "        eval_set=[(X_valid,y_valid)]\n",
    "        fit_model = model.fit( X_train, y_train, \n",
    "                               eval_set=eval_set,\n",
    "                               eval_metric='auc',\n",
    "                               early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n",
    "                               verbose=False\n",
    "                             )\n",
    "        print( \"  Best N trees = \", model.best_ntree_limit )\n",
    "        print( \"  Best gini = \", model.best_score )\n",
    "    else:\n",
    "        fit_model = model.fit( X_train, y_train )\n",
    "        \n",
    "    # Generate validation predictions for this fold\n",
    "    pred = fit_model.predict_proba(X_valid)[:,1]\n",
    "    print( \"  Gini = \", eval_gini(y_valid, pred) )\n",
    "    y_valid_pred.iloc[test_index] = pred\n",
    "    \n",
    "    # Accumulate test set predictions\n",
    "    y_test_pred_all[:,i] = fit_model.predict_proba(X_test)[:,1]\n",
    "    \n",
    "    del X_test, X_train, X_valid, y_train\n",
    "    \n",
    "y_test_pred = y_test_pred_all.mean(axis=1)   # Average test set predictions\n",
    "\n",
    "print( \"\\nGini for full training set:\" )\n",
    "eval_gini(y, y_valid_pred)\n",
    "\n",
    "val = pd.DataFrame()\n",
    "val['id'] = id_train\n",
    "val['target'] = y_valid_pred.values\n",
    "val.to_csv('xgb_valid.csv', float_format='%.6f', index=False)\n",
    "\n",
    "# Create submission file\n",
    "sub = pd.DataFrame()\n",
    "sub['id'] = id_test\n",
    "sub['target'] = y_test_pred\n",
    "sub.to_csv('submission1.csv', float_format='%.6f', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame(y_test_pred_all)\n",
    "\n",
    "test_df[\"target\"] = (test_df.rank() / test_df.shape[0]).mean(axis=1)\n",
    "test_df.drop([0,1,2,3,4], axis=1, inplace=True)\n",
    "test_df['id'] = id_test\n",
    "sub.to_csv('submission2.csv', float_format='%.6f', index=False)\n",
    "\n",
    "##LB 0.282"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(892816, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "current feature                                 ps_reg_01_plus_ps_car_04_cat    2 in   0.0\n",
    "\n",
    "Fold  0\n",
    "  Gini =  0.2854809390812456\n",
    "\n",
    "Fold  1\n",
    "  Gini =  0.2811618880082646\n",
    "\n",
    "Fold  2\n",
    "  Gini =  0.2752654261724574\n",
    "\n",
    "Fold  3\n",
    "  Gini =  0.29924437297640194\n",
    "\n",
    "Fold  4\n",
    "  Gini =  0.285688230990234\n",
    "\n",
    "Gini for full training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current feature                                 ps_reg_01_plus_ps_car_04_cat    2 in   0.0\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m--------------------------------------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |     alpha |   colsample_bytree |       eta |     gamma |   max_depth |   min_child_weight |   reg_alpha |   reg_lambda |   subsample | \n",
      "Multiple eval metrics have been passed: 'test-auc' will be used for early stopping.\n",
      "\n",
      "Will train until test-auc hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[2]\ttrain-auc:0.627339+0.0026179\ttest-auc:0.611496+0.00351362\n",
      "\n",
      "    1 | 01m18s | \u001b[35m   0.61150\u001b[0m | \u001b[32m   5.1013\u001b[0m | \u001b[32m            0.4138\u001b[0m | \u001b[32m   0.1942\u001b[0m | \u001b[32m   2.2636\u001b[0m | \u001b[32m    11.6591\u001b[0m | \u001b[32m            0.0305\u001b[0m | \u001b[32m     0.7427\u001b[0m | \u001b[32m      1.0219\u001b[0m | \u001b[32m     0.5211\u001b[0m | \n",
      "Multiple eval metrics have been passed: 'test-auc' will be used for early stopping.\n",
      "\n",
      "Will train until test-auc hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[0]\ttrain-auc:0.56399+0.000598153\ttest-auc:0.561405+0.000333591\n",
      "\n",
      "    2 | 00m56s |    0.56141 |    0.6265 |             0.4505 |    0.0672 |    4.9423 |     11.3270 |             0.6079 |      4.7774 |       0.1833 |      0.7314 | \n",
      "Multiple eval metrics have been passed: 'test-auc' will be used for early stopping.\n",
      "\n",
      "Will train until test-auc hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[0]\ttrain-auc:0.564231+0.00194965\ttest-auc:0.56205+0.00104674\n",
      "\n",
      "    3 | 00m44s |    0.56205 |    2.4524 |             0.4198 |    0.0898 |    1.1584 |     12.3604 |             0.5673 |      4.6927 |       4.7055 |      0.9230 | \n",
      "Multiple eval metrics have been passed: 'test-auc' will be used for early stopping.\n",
      "\n",
      "Will train until test-auc hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1]\ttrain-auc:0.605905+0.00252973\ttest-auc:0.593989+0.00814162\n",
      "\n",
      "    4 | 01m39s |    0.59399 |    1.1470 |             0.6378 |    0.2111 |    9.1933 |     12.0454 |             0.6165 |      2.5158 |       5.2103 |      0.7239 | \n",
      "Multiple eval metrics have been passed: 'test-auc' will be used for early stopping.\n",
      "\n",
      "Will train until test-auc hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1]\ttrain-auc:0.586151+0.00527349\ttest-auc:0.58163+0.00490145\n",
      "\n",
      "    5 | 00m33s |    0.58163 |    5.3107 |             0.2798 |    0.0928 |    9.7632 |      7.2681 |             0.9864 |      2.6980 |       8.8520 |      0.7257 | \n",
      "\u001b[31mBayesian Optimization\u001b[0m\n",
      "\u001b[94m--------------------------------------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |     alpha |   colsample_bytree |       eta |     gamma |   max_depth |   min_child_weight |   reg_alpha |   reg_lambda |   subsample | \n",
      "Multiple eval metrics have been passed: 'test-auc' will be used for early stopping.\n",
      "\n",
      "Will train until test-auc hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "\n",
      "    6 | 03m00s |    0.50000 |   10.0000 |             1.0000 |    0.0000 |   10.0000 |     15.0000 |             1.0000 |      0.0000 |       0.0000 |      1.0000 | \n",
      "Multiple eval metrics have been passed: 'test-auc' will be used for early stopping.\n",
      "\n",
      "Will train until test-auc hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "\n",
      "    7 | 00m57s |    0.50000 |    0.0000 |             1.0000 |    0.0000 |    0.0000 |      5.0000 |             1.0000 |      0.0000 |      10.0000 |      1.0000 | \n",
      "Multiple eval metrics have been passed: 'test-auc' will be used for early stopping.\n",
      "\n",
      "Will train until test-auc hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[0]\ttrain-auc:0.542148+0.0100283\ttest-auc:0.540712+0.00860008\n",
      "\n",
      "    8 | 01m05s |    0.54071 |   10.0000 |             1.0000 |    0.3000 |    0.0000 |      5.0000 |             0.0000 |     10.0000 |       0.0000 |      0.5000 | \n",
      "Multiple eval metrics have been passed: 'test-auc' will be used for early stopping.\n",
      "\n",
      "Will train until test-auc hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "\n",
      "    9 | 00m52s |    0.50000 |   10.0000 |             0.1000 |    0.3000 |    0.0000 |     15.0000 |             0.0000 |      0.0000 |      10.0000 |      0.5000 | \n",
      "Multiple eval metrics have been passed: 'test-auc' will be used for early stopping.\n",
      "\n",
      "Will train until test-auc hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "\n",
      "   10 | 00m43s |    0.50000 |    0.0000 |             0.1000 |    0.3000 |   10.0000 |      5.0000 |             0.0000 |      0.0000 |       0.0000 |      1.0000 | \n",
      "Multiple eval metrics have been passed: 'test-auc' will be used for early stopping.\n",
      "\n",
      "Will train until test-auc hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "\n",
      "   11 | 00m52s |    0.50000 |    0.0000 |             0.1000 |    0.3000 |   10.0000 |     15.0000 |             0.0000 |     10.0000 |      10.0000 |      0.5000 | \n",
      "Multiple eval metrics have been passed: 'test-auc' will be used for early stopping.\n",
      "\n",
      "Will train until test-auc hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "\n",
      "   12 | 03m22s |    0.50000 |    0.0000 |             1.0000 |    0.3000 |    0.0000 |     15.0000 |             1.0000 |      0.0000 |       0.0000 |      0.5000 | \n",
      "Multiple eval metrics have been passed: 'test-auc' will be used for early stopping.\n",
      "\n",
      "Will train until test-auc hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "\n",
      "   13 | 01m03s |    0.50000 |   10.0000 |             0.1000 |    0.0000 |    0.0000 |      5.0000 |             1.0000 |      0.0000 |       0.0000 |      0.5000 | \n",
      "Multiple eval metrics have been passed: 'test-auc' will be used for early stopping.\n",
      "\n",
      "Will train until test-auc hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[0]\ttrain-auc:0.542254+0.00987503\ttest-auc:0.540758+0.0085452\n",
      "\n",
      "   14 | 01m30s |    0.54076 |   10.0000 |             1.0000 |    0.3000 |   10.0000 |      9.1054 |             0.0000 |     10.0000 |       3.2729 |      0.5000 | \n",
      "Multiple eval metrics have been passed: 'test-auc' will be used for early stopping.\n",
      "\n",
      "Will train until test-auc hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[0]\ttrain-auc:0.577936+0.00831734\ttest-auc:0.574516+0.009287\n",
      "\n",
      "   15 | 01m17s |    0.57452 |    7.6370 |             0.6990 |    0.2674 |    0.2705 |      6.0165 |             0.5554 |      4.8786 |       2.2004 |      0.7968 | \u001b[31mWarning: Test point chose at random due to repeated sample.\u001b[0m\n",
      "\n",
      "Multiple eval metrics have been passed: 'test-auc' will be used for early stopping.\n",
      "\n",
      "Will train until test-auc hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[57]\ttrain-auc:0.500616+0.000314239\ttest-auc:0.500502+0.000391424\n",
      "\n",
      "   16 | 01m51s |    0.50050 |    0.0000 |             1.0000 |    0.3000 |   10.0000 |      5.0000 |             0.0000 |     10.0000 |      10.0000 |      0.5000 | \n",
      "Multiple eval metrics have been passed: 'test-auc' will be used for early stopping.\n",
      "\n",
      "Will train until test-auc hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "\n",
      "   17 | 01m50s |    0.50000 |    6.1243 |             1.0000 |    0.3000 |   10.0000 |     12.8917 |             0.0000 |      0.0000 |      10.0000 |      0.5000 | \n",
      "Multiple eval metrics have been passed: 'test-auc' will be used for early stopping.\n",
      "\n",
      "Will train until test-auc hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1]\ttrain-auc:0.499995+1e-05\ttest-auc:0.500005+1.04e-05\n",
      "\n",
      "   18 | 00m48s |    0.50001 |   10.0000 |             0.1000 |    0.0000 |    0.0000 |      5.0000 |             0.0000 |     10.0000 |      10.0000 |      0.5000 | \n",
      "Multiple eval metrics have been passed: 'test-auc' will be used for early stopping.\n",
      "\n",
      "Will train until test-auc hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[0]\ttrain-auc:0.546611+0.00140233\ttest-auc:0.545513+0.00143255\n",
      "\n",
      "   19 | 01m20s |    0.54551 |    9.7210 |             0.6759 |    0.0121 |    8.7505 |     13.9577 |             0.0308 |      7.2071 |       9.8287 |      0.8723 | \u001b[31mWarning: Test point chose at random due to repeated sample.\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple eval metrics have been passed: 'test-auc' will be used for early stopping.\n",
      "\n",
      "Will train until test-auc hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[0]\ttrain-auc:0.56126+0.00476945\ttest-auc:0.559255+0.005906\n",
      "\n",
      "   20 | 01m33s |    0.55925 |    5.0789 |             0.9030 |    0.1432 |    8.6097 |     10.3297 |             0.7825 |      9.2630 |       1.6545 |      0.9095 | \u001b[31mWarning: Test point chose at random due to repeated sample.\u001b[0m\n",
      "\n",
      "Multiple eval metrics have been passed: 'test-auc' will be used for early stopping.\n",
      "\n",
      "Will train until test-auc hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[3]\ttrain-auc:0.639028+0.000560512\ttest-auc:0.617557+0.0025995\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'funcalls': 51, 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'nit': 5, 'grad': array([ -1.70423196e-05])}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   21 | 01m20s | \u001b[35m   0.61756\u001b[0m | \u001b[32m   3.9230\u001b[0m | \u001b[32m            0.4898\u001b[0m | \u001b[32m   0.2065\u001b[0m | \u001b[32m   1.0829\u001b[0m | \u001b[32m     6.0368\u001b[0m | \u001b[32m            0.8441\u001b[0m | \u001b[32m     1.2355\u001b[0m | \u001b[32m      3.5993\u001b[0m | \u001b[32m     0.8361\u001b[0m | \u001b[31mWarning: Test point chose at random due to repeated sample.\u001b[0m\n",
      "\n",
      "Multiple eval metrics have been passed: 'test-auc' will be used for early stopping.\n",
      "\n",
      "Will train until test-auc hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "\n",
      "   22 | 00m49s |    0.50000 |   10.0000 |             0.1000 |    0.3000 |    0.0000 |     15.0000 |             1.0000 |     10.0000 |       0.0000 |      0.5000 | \n",
      "Multiple eval metrics have been passed: 'test-auc' will be used for early stopping.\n",
      "\n",
      "Will train until test-auc hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[23]\ttrain-auc:0.642431+0.00299279\ttest-auc:0.616457+0.00394929\n",
      "\n",
      "   23 | 01m29s |    0.61646 |    4.6586 |             0.1000 |    0.3000 |    4.9957 |      9.9573 |             1.0000 |      0.9364 |       3.9338 |      0.5000 | \n",
      "Multiple eval metrics have been passed: 'test-auc' will be used for early stopping.\n",
      "\n",
      "Will train until test-auc hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[17]\ttrain-auc:0.664288+0.00103853\ttest-auc:0.635296+0.00357627\n",
      "\n",
      "   24 | 01m34s | \u001b[35m   0.63530\u001b[0m | \u001b[32m   5.0661\u001b[0m | \u001b[32m            1.0000\u001b[0m | \u001b[32m   0.0000\u001b[0m | \u001b[32m   4.7302\u001b[0m | \u001b[32m     5.0000\u001b[0m | \u001b[32m            0.0000\u001b[0m | \u001b[32m     0.5479\u001b[0m | \u001b[32m      5.4488\u001b[0m | \u001b[32m     1.0000\u001b[0m | \n",
      "Multiple eval metrics have been passed: 'test-auc' will be used for early stopping.\n",
      "\n",
      "Will train until test-auc hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[0]\ttrain-auc:0.550545+0.00549641\ttest-auc:0.548849+0.00297066\n",
      "\n",
      "   25 | 00m49s |    0.54885 |    3.1915 |             0.1000 |    0.0000 |    5.0952 |      5.0000 |             0.0000 |      4.5417 |       5.0041 |      1.0000 | \n",
      "Multiple eval metrics have been passed: 'test-auc' will be used for early stopping.\n",
      "\n",
      "Will train until test-auc hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "\n",
      "   26 | 01m16s |    0.50000 |   10.0000 |             1.0000 |    0.3000 |    0.0000 |      5.0000 |             0.0000 |      0.0000 |      10.0000 |      1.0000 | \n",
      "Multiple eval metrics have been passed: 'test-auc' will be used for early stopping.\n",
      "\n",
      "Will train until test-auc hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[0]\ttrain-auc:0.500717+9.14374e-05\ttest-auc:0.500718+0.000363852\n",
      "\n",
      "   27 | 00m56s |    0.50072 |   10.0000 |             1.0000 |    0.0000 |    0.0000 |     15.0000 |             1.0000 |     10.0000 |      10.0000 |      0.5000 | \n",
      "Multiple eval metrics have been passed: 'test-auc' will be used for early stopping.\n",
      "\n",
      "Will train until test-auc hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[0]\ttrain-auc:0.548402+0.00181826\ttest-auc:0.547222+0.00140002\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'funcalls': 51, 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'nit': 2, 'grad': array([ -1.39560347e-05])}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   28 | 00m49s |    0.54722 |   10.0000 |             1.0000 |    0.0000 |   10.0000 |      5.0000 |             1.0000 |     10.0000 |      10.0000 |      1.0000 | \n",
      "Multiple eval metrics have been passed: 'test-auc' will be used for early stopping.\n",
      "\n",
      "Will train until test-auc hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[0]\ttrain-auc:0.5+0\ttest-auc:0.5+0\n",
      "\n",
      "   29 | 01m37s |    0.50000 |    3.7470 |             1.0000 |    0.0000 |    3.1364 |      9.1274 |             1.0000 |      0.0000 |       2.8466 |      1.0000 | \n",
      "Multiple eval metrics have been passed: 'test-auc' will be used for early stopping.\n",
      "\n",
      "Will train until test-auc hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[0]\ttrain-auc:0.546815+0.00131184\ttest-auc:0.545887+0.00171506\n",
      "\n",
      "   30 | 01m05s |    0.54589 |    2.4288 |             0.5228 |    0.0577 |    2.3620 |      7.4441 |             0.4422 |      9.1825 |       4.0079 |      0.8516 | \u001b[31mWarning: Test point chose at random due to repeated sample.\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from numba import jit\n",
    "import time\n",
    "import gc\n",
    "import data_util \n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "\n",
    "@jit\n",
    "def eval_gini(y_true, y_prob):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_true = y_true[np.argsort(y_prob)]\n",
    "    ntrue = 0\n",
    "    gini = 0\n",
    "    delta = 0\n",
    "    n = len(y_true)\n",
    "    for i in range(n-1, -1, -1):\n",
    "        y_i = y_true[i]\n",
    "        ntrue += y_i\n",
    "        gini += y_i * delta\n",
    "        delta += 1 - y_i\n",
    "    gini = 1 - 2 * gini / (ntrue * (n - ntrue))\n",
    "    return gini\n",
    "\n",
    "train_df = data_util.load_train_data()\n",
    "test_df = data_util.load_test_data()\n",
    "\n",
    "train_features = [\n",
    "    \"ps_car_13\",  #            : 1571.65 / shadow  609.23\n",
    "    \"ps_reg_03\",  #            : 1408.42 / shadow  511.15\n",
    "    \"ps_ind_05_cat\",  #        : 1387.87 / shadow   84.72\n",
    "    \"ps_ind_03\",  #            : 1219.47 / shadow  230.55\n",
    "    \"ps_ind_15\",  #            :  922.18 / shadow  242.00\n",
    "    \"ps_reg_02\",  #            :  920.65 / shadow  267.50\n",
    "    \"ps_car_14\",  #            :  798.48 / shadow  549.58\n",
    "    \"ps_car_12\",  #            :  731.93 / shadow  293.62\n",
    "    \"ps_car_01_cat\",  #        :  698.07 / shadow  178.72\n",
    "    \"ps_car_07_cat\",  #        :  694.53 / shadow   36.35\n",
    "    \"ps_ind_17_bin\",  #        :  620.77 / shadow   23.15\n",
    "    \"ps_car_03_cat\",  #        :  611.73 / shadow   50.67\n",
    "    \"ps_reg_01\",  #            :  598.60 / shadow  178.57\n",
    "    \"ps_car_15\",  #            :  593.35 / shadow  226.43\n",
    "    \"ps_ind_01\",  #            :  547.32 / shadow  154.58\n",
    "    \"ps_ind_16_bin\",  #        :  475.37 / shadow   34.17\n",
    "    \"ps_ind_07_bin\",  #        :  435.28 / shadow   28.92\n",
    "    \"ps_car_06_cat\",  #        :  398.02 / shadow  212.43\n",
    "    \"ps_car_04_cat\",  #        :  376.87 / shadow   76.98\n",
    "    \"ps_ind_06_bin\",  #        :  370.97 / shadow   36.13\n",
    "    \"ps_car_09_cat\",  #        :  214.12 / shadow   81.38\n",
    "    \"ps_car_02_cat\",  #        :  203.03 / shadow   26.67\n",
    "    \"ps_ind_02_cat\",  #        :  189.47 / shadow   65.68\n",
    "    \"ps_car_11\",  #            :  173.28 / shadow   76.45\n",
    "    \"ps_car_05_cat\",  #        :  172.75 / shadow   62.92\n",
    "    \"ps_calc_09\",  #           :  169.13 / shadow  129.72\n",
    "    \"ps_calc_05\",  #           :  148.83 / shadow  120.68\n",
    "    \"ps_ind_08_bin\",  #        :  140.73 / shadow   27.63\n",
    "    \"ps_car_08_cat\",  #        :  120.87 / shadow   28.82\n",
    "    \"ps_ind_09_bin\",  #        :  113.92 / shadow   27.05\n",
    "    \"ps_ind_04_cat\",  #        :  107.27 / shadow   37.43\n",
    "    \"ps_ind_18_bin\",  #        :   77.42 / shadow   25.97\n",
    "    \"ps_ind_12_bin\",  #        :   39.67 / shadow   15.52\n",
    "    \"ps_ind_14\",  #            :   37.37 / shadow   16.65\n",
    "]\n",
    "# add combinations\n",
    "combs = [\n",
    "    ('ps_reg_01', 'ps_car_02_cat'),  \n",
    "    ('ps_reg_01', 'ps_car_04_cat'),\n",
    "]\n",
    "\n",
    "# Process data\n",
    "id_test = test_df['id'].values\n",
    "id_train = train_df['id'].values\n",
    "y = train_df['target']\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "combs = [\n",
    "    ('ps_reg_01', 'ps_car_02_cat'),  \n",
    "    ('ps_reg_01', 'ps_car_04_cat'),\n",
    "]\n",
    "for n_c, (f1, f2) in enumerate(combs):\n",
    "    name1 = f1 + \"_plus_\" + f2\n",
    "    print('current feature %60s %4d in %5.1f'\n",
    "          % (name1, n_c + 1, (time.time() - start) / 60), end='')\n",
    "    print('\\r' * 75, end='')\n",
    "    train_df[name1] = train_df[f1].apply(lambda x: str(x)) + \"_\" + train_df[f2].apply(lambda x: str(x))\n",
    "    test_df[name1] = test_df[f1].apply(lambda x: str(x)) + \"_\" + test_df[f2].apply(lambda x: str(x))\n",
    "    # Label Encode\n",
    "    lbl = LabelEncoder()\n",
    "    lbl.fit(list(train_df[name1].values) + list(test_df[name1].values))\n",
    "    train_df[name1] = lbl.transform(list(train_df[name1].values))\n",
    "    test_df[name1] = lbl.transform(list(test_df[name1].values))\n",
    "\n",
    "    train_features.append(name1)\n",
    "    \n",
    "X = train_df[train_features]\n",
    "test_df = test_df[train_features]\n",
    "\n",
    "y_valid_pred = 0*y\n",
    "y_test_pred = 0\n",
    "\n",
    "xgbtrain = xgb.DMatrix(X, label= y)\n",
    "\n",
    "# Set up classifier\n",
    "\n",
    "num_rounds = 3000\n",
    "random_state = 2016\n",
    "num_iter = 25\n",
    "init_points = 5\n",
    "params = {\n",
    "    'silent': 1,\n",
    "    'objective':'binary:logistic',\n",
    "    'eval_metric': 'auc',\n",
    "    'verbose_eval': True,\n",
    "    'seed': random_state\n",
    "}\n",
    "\n",
    "def xgb_evaluate(min_child_weight,\n",
    "                 colsample_bytree,\n",
    "                 max_depth,\n",
    "                 subsample,\n",
    "                 gamma,\n",
    "                 alpha,\n",
    "                reg_lambda,\n",
    "                reg_alpha,\n",
    "                eta):\n",
    "\n",
    "    params['min_child_weight'] = int(min_child_weight)\n",
    "    params['colsample_bytree'] = max(min(colsample_bytree, 1), 0)\n",
    "    params['max_depth'] = int(max_depth)\n",
    "    params['subsample'] = max(min(subsample, 1), 0)\n",
    "    params['gamma'] = max(gamma, 0) #minimum loss reduction required to make a further partition on a leaf node of\n",
    "                                    #the tree. The larger, the more conservative the algorithm will be.\n",
    "    params['alpha'] = max(alpha, 0)\n",
    "    params['reg_lambda'] = max(reg_lambda, 0)\n",
    "    params['reg_alpha'] = max(reg_alpha, 0)\n",
    "    params['eta'] = max(reg_alpha, 0)\n",
    "\n",
    "    cv_result = xgb.cv(params, xgbtrain, num_boost_round=num_rounds, nfold=5,\n",
    "             seed=random_state,\n",
    "             early_stopping_rounds=50)\n",
    "    \n",
    "    return cv_result['test-auc-mean'].values[-1]\n",
    "\n",
    "\n",
    "xgbBO = BayesianOptimization(xgb_evaluate, {'min_child_weight': (0, 1),\n",
    "                                            'colsample_bytree': (0.1, 1),\n",
    "                                            'max_depth': (5, 15),\n",
    "                                            'subsample': (0.5, 1),\n",
    "                                            'gamma': (0, 10),\n",
    "                                            'alpha': (0, 10),\n",
    "                                            'reg_lambda': (0, 10),\n",
    "                                            'reg_alpha':(0, 10),\n",
    "                                            'eta':(0,0.3)\n",
    "                                            })\n",
    "\n",
    "\n",
    "XGB_BO.explore({\n",
    "              'max_depth':            [3, 8, 3, 8, 8, 3, 8, 3],\n",
    "              'gamma':                [0.5, 8, 0.2, 9, 0.5, 8, 0.2, 9],\n",
    "              'min_child_weight':     [0.2, 0.2, 0.2, 0.2, 12, 12, 12, 12],\n",
    "              'max_delta_step':       [1, 2, 2, 1, 2, 1, 1, 2],\n",
    "              'subsample':            [0.6, 0.8, 0.6, 0.8, 0.6, 0.8, 0.6, 0.8],\n",
    "              'colsample_bytree':     [0.6, 0.8, 0.6, 0.8, 0.6, 0.8, 0.6, 0.8],\n",
    "              })\n",
    "\n",
    "\n",
    "\n",
    "xgbBO.maximize(init_points=init_points, n_iter=num_iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
