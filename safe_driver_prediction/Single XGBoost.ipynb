{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current feature                                 ps_reg_01_plus_ps_car_04_cat    2 in   0.0\n",
      "Fold  0\n",
      "  Gini =  0.2703606613911892\n",
      "\n",
      "Fold  1\n",
      "  Gini =  0.270001302590679\n",
      "\n",
      "Fold  2\n",
      "  Gini =  0.2614763598190103\n",
      "\n",
      "Fold  3\n",
      "  Gini =  0.2863712227339569\n",
      "\n",
      "Fold  4\n",
      "  Gini =  0.27514977994995016\n",
      "\n",
      "Gini for full training set:\n"
     ]
    }
   ],
   "source": [
    "MAX_ROUNDS = 2000\n",
    "OPTIMIZE_ROUNDS = False\n",
    "LEARNING_RATE = 0.07\n",
    "EARLY_STOPPING_ROUNDS = 50 \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from numba import jit\n",
    "import time\n",
    "import gc\n",
    "import data_util \n",
    "\n",
    "@jit\n",
    "def eval_gini(y_true, y_prob):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_true = y_true[np.argsort(y_prob)]\n",
    "    ntrue = 0\n",
    "    gini = 0\n",
    "    delta = 0\n",
    "    n = len(y_true)\n",
    "    for i in range(n-1, -1, -1):\n",
    "        y_i = y_true[i]\n",
    "        ntrue += y_i\n",
    "        gini += y_i * delta\n",
    "        delta += 1 - y_i\n",
    "    gini = 1 - 2 * gini / (ntrue * (n - ntrue))\n",
    "    return gini\n",
    "\n",
    "def add_noise(series, noise_level):\n",
    "    return series * (1 + noise_level * np.random.randn(len(series)))\n",
    "\n",
    "\n",
    "train_df = data_util.load_train_data()\n",
    "test_df = data_util.load_test_data()\n",
    "\n",
    "train_features = [\n",
    "    \"ps_car_13\",  #            : 1571.65 / shadow  609.23\n",
    "    \"ps_reg_03\",  #            : 1408.42 / shadow  511.15\n",
    "    \"ps_ind_05_cat\",  #        : 1387.87 / shadow   84.72\n",
    "    \"ps_ind_03\",  #            : 1219.47 / shadow  230.55\n",
    "    \"ps_ind_15\",  #            :  922.18 / shadow  242.00\n",
    "    \"ps_reg_02\",  #            :  920.65 / shadow  267.50\n",
    "    \"ps_car_14\",  #            :  798.48 / shadow  549.58\n",
    "    \"ps_car_12\",  #            :  731.93 / shadow  293.62\n",
    "    \"ps_car_01_cat\",  #        :  698.07 / shadow  178.72\n",
    "    \"ps_car_07_cat\",  #        :  694.53 / shadow   36.35\n",
    "    \"ps_ind_17_bin\",  #        :  620.77 / shadow   23.15\n",
    "    \"ps_car_03_cat\",  #        :  611.73 / shadow   50.67\n",
    "    \"ps_reg_01\",  #            :  598.60 / shadow  178.57\n",
    "    \"ps_car_15\",  #            :  593.35 / shadow  226.43\n",
    "    \"ps_ind_01\",  #            :  547.32 / shadow  154.58\n",
    "    \"ps_ind_16_bin\",  #        :  475.37 / shadow   34.17\n",
    "    \"ps_ind_07_bin\",  #        :  435.28 / shadow   28.92\n",
    "    \"ps_car_06_cat\",  #        :  398.02 / shadow  212.43\n",
    "    \"ps_car_04_cat\",  #        :  376.87 / shadow   76.98\n",
    "    \"ps_ind_06_bin\",  #        :  370.97 / shadow   36.13\n",
    "    \"ps_car_09_cat\",  #        :  214.12 / shadow   81.38\n",
    "    \"ps_car_02_cat\",  #        :  203.03 / shadow   26.67\n",
    "    \"ps_ind_02_cat\",  #        :  189.47 / shadow   65.68\n",
    "    \"ps_car_11\",  #            :  173.28 / shadow   76.45\n",
    "    \"ps_car_05_cat\",  #        :  172.75 / shadow   62.92\n",
    "    \"ps_calc_09\",  #           :  169.13 / shadow  129.72\n",
    "    \"ps_calc_05\",  #           :  148.83 / shadow  120.68\n",
    "    \"ps_ind_08_bin\",  #        :  140.73 / shadow   27.63\n",
    "    \"ps_car_08_cat\",  #        :  120.87 / shadow   28.82\n",
    "    \"ps_ind_09_bin\",  #        :  113.92 / shadow   27.05\n",
    "    \"ps_ind_04_cat\",  #        :  107.27 / shadow   37.43\n",
    "    \"ps_ind_18_bin\",  #        :   77.42 / shadow   25.97\n",
    "    \"ps_ind_12_bin\",  #        :   39.67 / shadow   15.52\n",
    "    \"ps_ind_14\",  #            :   37.37 / shadow   16.65\n",
    "]\n",
    "\n",
    "# add combinations\n",
    "combs = [\n",
    "    ('ps_reg_01', 'ps_car_02_cat'),  \n",
    "    ('ps_reg_01', 'ps_car_04_cat'),\n",
    "]\n",
    "\n",
    "\n",
    "def target_encode(trn_series=None,    # Revised to encode validation series\n",
    "                  val_series=None,\n",
    "                  tst_series=None,\n",
    "                  target=None,\n",
    "                  min_samples_leaf=1,\n",
    "                  smoothing=1,\n",
    "                  noise_level=0):\n",
    "    \"\"\"\n",
    "    Smoothing is computed like in the following paper by Daniele Micci-Barreca\n",
    "    https://kaggle2.blob.core.windows.net/forum-message-attachments/225952/7441/high%20cardinality%20categoricals.pdf\n",
    "    trn_series : training categorical feature as a pd.Series\n",
    "    tst_series : test categorical feature as a pd.Series\n",
    "    target : target data as a pd.Series\n",
    "    min_samples_leaf (int) : minimum samples to take category average into account\n",
    "    smoothing (int) : smoothing effect to balance categorical average vs prior\n",
    "    \"\"\"\n",
    "    assert len(trn_series) == len(target)\n",
    "    assert trn_series.name == tst_series.name\n",
    "    temp = pd.concat([trn_series, target], axis=1)\n",
    "    # Compute target mean\n",
    "    averages = temp.groupby(by=trn_series.name)[target.name].agg([\"mean\", \"count\"])\n",
    "    # Compute smoothing\n",
    "    smoothing = 1 / (1 + np.exp(-(averages[\"count\"] - min_samples_leaf) / smoothing))\n",
    "    # Apply average function to all target data\n",
    "    prior = target.mean()\n",
    "    # The bigger the count the less full_avg is taken into account\n",
    "    averages[target.name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\n",
    "    averages.drop([\"mean\", \"count\"], axis=1, inplace=True)\n",
    "    # Apply averages to trn and tst series\n",
    "    ft_trn_series = pd.merge(\n",
    "        trn_series.to_frame(trn_series.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on=trn_series.name,\n",
    "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    # pd.merge does not keep the index so restore it\n",
    "    ft_trn_series.index = trn_series.index\n",
    "    ft_val_series = pd.merge(\n",
    "        val_series.to_frame(val_series.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on=val_series.name,\n",
    "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    # pd.merge does not keep the index so restore it\n",
    "    ft_val_series.index = val_series.index\n",
    "    ft_tst_series = pd.merge(\n",
    "        tst_series.to_frame(tst_series.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on=tst_series.name,\n",
    "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    # pd.merge does not keep the index so restore it\n",
    "    ft_tst_series.index = tst_series.index\n",
    "    return add_noise(ft_trn_series, noise_level), add_noise(ft_val_series, noise_level), add_noise(ft_tst_series, noise_level)\n",
    "\n",
    "\n",
    "# Process data\n",
    "id_test = test_df['id'].values\n",
    "id_train = train_df['id'].values\n",
    "y = train_df['target']\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "combs = [\n",
    "    ('ps_reg_01', 'ps_car_02_cat'),  \n",
    "    ('ps_reg_01', 'ps_car_04_cat'),\n",
    "]\n",
    "for n_c, (f1, f2) in enumerate(combs):\n",
    "    name1 = f1 + \"_plus_\" + f2\n",
    "    print('current feature %60s %4d in %5.1f'\n",
    "          % (name1, n_c + 1, (time.time() - start) / 60), end='')\n",
    "    print('\\r' * 75, end='')\n",
    "    train_df[name1] = train_df[f1].apply(lambda x: str(x)) + \"_\" + train_df[f2].apply(lambda x: str(x))\n",
    "    test_df[name1] = test_df[f1].apply(lambda x: str(x)) + \"_\" + test_df[f2].apply(lambda x: str(x))\n",
    "    # Label Encode\n",
    "    lbl = LabelEncoder()\n",
    "    lbl.fit(list(train_df[name1].values) + list(test_df[name1].values))\n",
    "    train_df[name1] = lbl.transform(list(train_df[name1].values))\n",
    "    test_df[name1] = lbl.transform(list(test_df[name1].values))\n",
    "\n",
    "    train_features.append(name1)\n",
    "\n",
    "        \n",
    "    \n",
    "#########################\n",
    "# train_df['na_sum'] = (train_df == -1).sum(axis=1)\n",
    "# train_df['na_ca_t_sum'] = (train_df[f_cats] == -1).sum(axis=1)\n",
    "# test_df['na_sum'] = (test_df == -1).sum(axis=1)\n",
    "# test_df['na_ca_t_sum'] = (test_df[f_cats] == -1).sum(axis=1)\n",
    "\n",
    "# train_features += ['na_ca_t_sum', 'na_sum']\n",
    "\n",
    "train_df['ps_car_13_+_ps_reg_03'] = train_df['ps_car_13'] + train_df['ps_reg_03']\n",
    "train_df['ps_car_13_-_ps_reg_03'] = train_df['ps_car_13'] - train_df['ps_reg_03']\n",
    "train_df['ps_car_13_x_ps_reg_03'] = train_df['ps_car_13'] * train_df['ps_reg_03']\n",
    "train_df['ps_car_13_/_ps_reg_03'] = train_df['ps_car_13'] / train_df['ps_reg_03']\n",
    "\n",
    "test_df['ps_car_13_+_ps_reg_03'] = test_df['ps_car_13'] + test_df['ps_reg_03']\n",
    "test_df['ps_car_13_-_ps_reg_03'] = test_df['ps_car_13'] - test_df['ps_reg_03']\n",
    "test_df['ps_car_13_x_ps_reg_03'] = test_df['ps_car_13'] * test_df['ps_reg_03']\n",
    "test_df['ps_car_13_/_ps_reg_03'] = test_df['ps_car_13'] / test_df['ps_reg_03']\n",
    "\n",
    "\n",
    "train_features+= ['ps_car_13_x_ps_reg_03', 'ps_car_13_/_ps_reg_03','ps_car_13_+_ps_reg_03','ps_car_13_-_ps_reg_03']\n",
    "#########################\n",
    "X = train_df[train_features]\n",
    "test_df = test_df[train_features]\n",
    "\n",
    "f_cats = [f for f in X.columns if \"_cat\" in f]\n",
    "\n",
    "y_valid_pred = 0*y\n",
    "y_test_pred = 0\n",
    "\n",
    "# Set up folds\n",
    "K = 5\n",
    "kf = KFold(n_splits = K, random_state = 1, shuffle = True)\n",
    "np.random.seed(0)\n",
    "\n",
    "# Set up classifier\n",
    "model = XGBClassifier(    \n",
    "                        n_estimators=MAX_ROUNDS,\n",
    "                        max_depth=6,\n",
    "                        objective=\"binary:logistic\",\n",
    "                        learning_rate=LEARNING_RATE, \n",
    "                        subsample=.8,\n",
    "                        min_child_weight=0.2,\n",
    "                        colsample_bytree=.8,\n",
    "                        eta=0.03,\n",
    "                        gamma=9,\n",
    "                        reg_alpha=1.5,\n",
    "                        reg_lambda=1.5,\n",
    "                     )\n",
    "\n",
    "                        #scale_pos_weight=1.6,\n",
    "\n",
    "# Run CV\n",
    "y_test_pred_all = np.zeros((len(test_df),5))\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(kf.split(train_df)):\n",
    "    \n",
    "    # Create data for this fold\n",
    "    y_train, y_valid = y.iloc[train_index].copy(), y.iloc[test_index]\n",
    "    X_train, X_valid = X.iloc[train_index,:].copy(), X.iloc[test_index,:].copy()\n",
    "    X_test = test_df.copy()\n",
    "    print( \"\\nFold \", i)\n",
    "    \n",
    "    \n",
    "    for f in f_cats:\n",
    "        X_train[f + \"_avg\"], X_valid[f + \"_avg\"], X_test[f + \"_avg\"] = target_encode(\n",
    "                                                        trn_series=X_train[f],\n",
    "                                                        val_series=X_valid[f],\n",
    "                                                        tst_series=X_test[f],\n",
    "                                                        target=y_train,\n",
    "                                                        min_samples_leaf=200,\n",
    "                                                        smoothing=10,\n",
    "                                                        noise_level=0\n",
    "                                                        )\n",
    "    \n",
    "    # Run model for this fold\n",
    "    if OPTIMIZE_ROUNDS:\n",
    "        eval_set=[(X_valid,y_valid)]\n",
    "        fit_model = model.fit( X_train, y_train, \n",
    "                               eval_set=eval_set,\n",
    "                               eval_metric='auc',\n",
    "                               early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n",
    "                               verbose=False\n",
    "                             )\n",
    "        print( \"  Best N trees = \", model.best_ntree_limit )\n",
    "        print( \"  Best gini = \", model.best_score )\n",
    "    else:\n",
    "        fit_model = model.fit( X_train, y_train )\n",
    "        \n",
    "    # Generate validation predictions for this fold\n",
    "    pred = fit_model.predict_proba(X_valid)[:,1]\n",
    "    print( \"  Gini = \", eval_gini(y_valid, pred) )\n",
    "    y_valid_pred.iloc[test_index] = pred\n",
    "    \n",
    "    # Accumulate test set predictions\n",
    "    y_test_pred_all[:,i] = fit_model.predict_proba(X_test)[:,1]\n",
    "    \n",
    "    del X_test, X_train, X_valid, y_train\n",
    "    \n",
    "y_test_pred = y_test_pred_all.mean(axis=1)   # Average test set predictions\n",
    "\n",
    "print( \"\\nGini for full training set:\" )\n",
    "eval_gini(y, y_valid_pred)\n",
    "\n",
    "val = pd.DataFrame()\n",
    "val['id'] = id_train\n",
    "val['target'] = y_valid_pred.values\n",
    "val.to_csv('xgb_valid.csv', float_format='%.6f', index=False)\n",
    "\n",
    "# Create submission file\n",
    "sub = pd.DataFrame()\n",
    "sub['id'] = id_test\n",
    "sub['target'] = y_test_pred\n",
    "sub.to_csv('submission1.csv', float_format='%.6f', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame(y_test_pred_all)\n",
    "\n",
    "test_df[\"target\"] = (test_df.rank() / test_df.shape[0]).mean(axis=1)\n",
    "test_df.drop([0,1,2,3,4], axis=1, inplace=True)\n",
    "test_df['id'] = id_test\n",
    "sub.to_csv('submission2.csv', float_format='%.6f', index=False)\n",
    "\n",
    "##LB 0.282"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(892816, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "current feature                                 ps_reg_01_plus_ps_car_04_cat    2 in   0.0\n",
    "\n",
    "Fold  0\n",
    "  Gini =  0.2854809390812456\n",
    "\n",
    "Fold  1\n",
    "  Gini =  0.2811618880082646\n",
    "\n",
    "Fold  2\n",
    "  Gini =  0.2752654261724574\n",
    "\n",
    "Fold  3\n",
    "  Gini =  0.29924437297640194\n",
    "\n",
    "Fold  4\n",
    "  Gini =  0.285688230990234\n",
    "\n",
    "Gini for full training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current feature                                 ps_reg_01_plus_ps_car_04_cat    2 in   0.0\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   colsample_bytree |     gamma |   min_child_weight |   reg_alpha |   reg_lambda |   subsample | \n",
      " Stopped after 325 iterations with train-auc = 0.705605 val-auc = 0.641126 ( diff = 0.064479 ) train-gini = 0.411210 val-gini = 0.282252\n",
      "    1 | 05m57s | \u001b[35m   0.64113\u001b[0m | \u001b[32m            0.6000\u001b[0m | \u001b[32m   0.5000\u001b[0m | \u001b[32m            0.2000\u001b[0m | \u001b[32m     0.1000\u001b[0m | \u001b[32m      0.1000\u001b[0m | \u001b[32m     0.6000\u001b[0m | \n",
      " Stopped after 560 iterations with train-auc = 0.694964 val-auc = 0.642879 ( diff = 0.052085 ) train-gini = 0.389928 val-gini = 0.285758\n",
      "    2 | 10m23s | \u001b[35m   0.64288\u001b[0m | \u001b[32m            0.8000\u001b[0m | \u001b[32m   8.0000\u001b[0m | \u001b[32m            0.2000\u001b[0m | \u001b[32m     0.5000\u001b[0m | \u001b[32m      0.5000\u001b[0m | \u001b[32m     0.8000\u001b[0m | \n",
      " Stopped after 436 iterations with train-auc = 0.712978 val-auc = 0.642338 ( diff = 0.070640 ) train-gini = 0.425956 val-gini = 0.284677\n",
      "    3 | 07m43s |    0.64234 |             0.6000 |    0.2000 |             0.2000 |      1.0000 |       1.0000 |      0.6000 | \n",
      " Stopped after 874 iterations with train-auc = 0.680457 val-auc = 0.643464 ( diff = 0.036993 ) train-gini = 0.360914 val-gini = 0.286927\n",
      "    4 | 15m40s | \u001b[35m   0.64346\u001b[0m | \u001b[32m            0.8000\u001b[0m | \u001b[32m   9.0000\u001b[0m | \u001b[32m            0.2000\u001b[0m | \u001b[32m     1.5000\u001b[0m | \u001b[32m      1.5000\u001b[0m | \u001b[32m     0.8000\u001b[0m | \n",
      " Stopped after 437 iterations with train-auc = 0.701777 val-auc = 0.642434 ( diff = 0.059342 ) train-gini = 0.403553 val-gini = 0.284868\n",
      "    5 | 07m38s |    0.64243 |             0.6000 |    0.5000 |            12.0000 |      0.1000 |       0.1000 |      0.6000 | \n",
      " Stopped after 629 iterations with train-auc = 0.684564 val-auc = 0.642806 ( diff = 0.041758 ) train-gini = 0.369128 val-gini = 0.285611\n",
      "    6 | 11m26s |    0.64281 |             0.8000 |    8.0000 |            12.0000 |      0.5000 |       0.5000 |      0.8000 | \n",
      " Stopped after 439 iterations with train-auc = 0.701447 val-auc = 0.642642 ( diff = 0.058805 ) train-gini = 0.402893 val-gini = 0.285284\n",
      "    7 | 07m42s |    0.64264 |             0.6000 |    0.2000 |            12.0000 |      1.0000 |       1.0000 |      0.6000 | \n",
      " Stopped after 883 iterations with train-auc = 0.676868 val-auc = 0.643350 ( diff = 0.033518 ) train-gini = 0.353736 val-gini = 0.286699\n",
      "    8 | 15m37s |    0.64335 |             0.8000 |    9.0000 |            12.0000 |      1.5000 |       1.5000 |      0.8000 | \n",
      " Stopped after 619 iterations with train-auc = 0.687590 val-auc = 0.642847 ( diff = 0.044743 ) train-gini = 0.375180 val-gini = 0.285694\n",
      "    9 | 06m33s |    0.64285 |             0.1985 |    2.4142 |             8.9658 |      3.3682 |       0.6523 |      0.9372 | \n",
      " Stopped after 395 iterations with train-auc = 0.689355 val-auc = 0.642954 ( diff = 0.046401 ) train-gini = 0.378711 val-gini = 0.285908\n",
      "   10 | 07m52s |    0.64295 |             0.8864 |    2.5297 |            11.6148 |      4.1991 |       8.6920 |      0.8278 | \n",
      " Stopped after 1198 iterations with train-auc = 0.670877 val-auc = 0.643246 ( diff = 0.027631 ) train-gini = 0.341754 val-gini = 0.286492\n",
      "   11 | 20m31s |    0.64325 |             0.7659 |    7.0969 |             6.8029 |      7.9018 |       2.7949 |      0.7939 | \n",
      " Stopped after 895 iterations with train-auc = 0.680023 val-auc = 0.641947 ( diff = 0.038076 ) train-gini = 0.360046 val-gini = 0.283894\n",
      "   12 | 08m28s |    0.64195 |             0.1183 |    2.3250 |            10.1035 |      1.2412 |       9.1741 |      0.9635 | \n",
      " Stopped after 456 iterations with train-auc = 0.688668 val-auc = 0.643014 ( diff = 0.045654 ) train-gini = 0.377336 val-gini = 0.286027\n",
      "   13 | 08m01s |    0.64301 |             0.6175 |    1.3444 |             6.5577 |      7.7781 |       5.2692 |      0.6325 | \n",
      "\u001b[31mBayesian Optimization\u001b[0m\n",
      "\u001b[94m------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   colsample_bytree |     gamma |   min_child_weight |   reg_alpha |   reg_lambda |   subsample | \n",
      " Stopped after 1815 iterations with train-auc = 0.657990 val-auc = 0.641586 ( diff = 0.016404 ) train-gini = 0.315980 val-gini = 0.283173\n",
      "   14 | 29m05s |    0.64159 |             0.7585 |    9.4470 |            19.8035 |      9.8152 |       0.3519 |      0.9861 | \n",
      " Stopped after 1209 iterations with train-auc = 0.664271 val-auc = 0.643014 ( diff = 0.021258 ) train-gini = 0.328543 val-gini = 0.286027\n",
      "   15 | 15m37s |    0.64301 |             0.4057 |    9.2930 |            19.9916 |      0.6499 |       9.8426 |      0.8396 | \n",
      " Stopped after 882 iterations with train-auc = 0.668272 val-auc = 0.643159 ( diff = 0.025113 ) train-gini = 0.336543 val-gini = 0.286318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'nit': 5, 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 52, 'grad': array([ 0.00030312]), 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'nit': 3, 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 47, 'grad': array([-0.0002777]), 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   16 | 16m47s |    0.64316 |             0.8032 |    9.6860 |             0.9558 |      0.0260 |       9.8890 |      0.6319 | \n",
      " Stopped after 396 iterations with train-auc = 0.689744 val-auc = 0.642290 ( diff = 0.047454 ) train-gini = 0.379489 val-gini = 0.284580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'nit': 5, 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 52, 'grad': array([ 0.00040204]), 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/gaussian_process/gpr.py:335: UserWarning: Predicted variances smaller than 0. Setting those variances to 0.\n",
      "  warnings.warn(\"Predicted variances smaller than 0. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   17 | 08m07s |    0.64229 |             0.9184 |    0.1238 |             0.2118 |      8.8873 |       9.7835 |      0.9941 | \n",
      " Stopped after 3000 iterations with train-auc = 0.641711 val-auc = 0.636260 ( diff = 0.005451 ) train-gini = 0.283422 val-gini = 0.272520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'nit': 6, 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 55, 'grad': array([-0.00092615]), 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   18 | 27m27s |    0.63626 |             0.1020 |    9.9536 |            19.9692 |      9.3889 |       9.0639 |      0.5265 | \n",
      " Stopped after 409 iterations with train-auc = 0.687393 val-auc = 0.642048 ( diff = 0.045345 ) train-gini = 0.374786 val-gini = 0.284095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'nit': 5, 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 51, 'grad': array([ 0.00141364]), 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'nit': 3, 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 53, 'grad': array([-0.00011607]), 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   19 | 09m05s |    0.64205 |             0.8341 |    0.8274 |             0.2583 |      9.7055 |       0.0398 |      0.5119 | \n",
      " Stopped after 387 iterations with train-auc = 0.696906 val-auc = 0.642523 ( diff = 0.054384 ) train-gini = 0.393813 val-gini = 0.285046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'nit': 5, 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 58, 'grad': array([ -1.23220637e-05]), 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   20 | 07m31s |    0.64252 |             0.8373 |    2.1814 |            19.9216 |      0.7211 |       0.1966 |      0.9909 | \n",
      " Stopped after 325 iterations with train-auc = 0.692765 val-auc = 0.641853 ( diff = 0.050912 ) train-gini = 0.385530 val-gini = 0.283706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'nit': 4, 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 51, 'grad': array([-0.00293461]), 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'nit': 6, 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 53, 'grad': array([-0.00012153]), 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   21 | 07m06s |    0.64185 |             0.9996 |    2.2988 |             4.2164 |      1.2859 |       4.4718 |      0.9978 | \n",
      " Stopped after 2053 iterations with train-auc = 0.667644 val-auc = 0.642621 ( diff = 0.025022 ) train-gini = 0.335288 val-gini = 0.285243\n",
      "   22 | 29m19s |    0.64262 |             0.4503 |    9.9117 |            19.4087 |      2.6403 |       0.3963 |      0.5040 | \n",
      " Stopped after 439 iterations with train-auc = 0.690181 val-auc = 0.642328 ( diff = 0.047854 ) train-gini = 0.380362 val-gini = 0.284655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'nit': 5, 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 56, 'grad': array([-0.00053334]), 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   23 | 07m22s |    0.64233 |             0.4938 |    0.0836 |            18.8760 |      0.0207 |       9.7771 |      0.5218 | \n",
      " Stopped after 453 iterations with train-auc = 0.686038 val-auc = 0.642118 ( diff = 0.043920 ) train-gini = 0.372075 val-gini = 0.284236\n",
      "   24 | 09m03s |    0.64212 |             0.7326 |    0.7545 |            19.4815 |      9.7535 |       0.0422 |      0.5150 | \n",
      " Stopped after 420 iterations with train-auc = 0.701860 val-auc = 0.642568 ( diff = 0.059291 ) train-gini = 0.403719 val-gini = 0.285137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'nit': 2, 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'funcalls': 52, 'grad': array([-0.00179539]), 'warnflag': 2}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   25 | 08m54s |    0.64257 |             0.7529 |    0.4131 |             0.0548 |      0.5871 |       9.9772 |      0.5610 | \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from numba import jit\n",
    "import time\n",
    "import gc\n",
    "import data_util \n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "\n",
    "@jit\n",
    "def eval_gini(y_true, y_prob):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_true = y_true[np.argsort(y_prob)]\n",
    "    ntrue = 0\n",
    "    gini = 0\n",
    "    delta = 0\n",
    "    n = len(y_true)\n",
    "    for i in range(n-1, -1, -1):\n",
    "        y_i = y_true[i]\n",
    "        ntrue += y_i\n",
    "        gini += y_i * delta\n",
    "        delta += 1 - y_i\n",
    "    gini = 1 - 2 * gini / (ntrue * (n - ntrue))\n",
    "    return gini\n",
    "\n",
    "train_df = data_util.load_train_data()\n",
    "test_df = data_util.load_test_data()\n",
    "\n",
    "train_features = [\n",
    "    \"ps_car_13\",  #            : 1571.65 / shadow  609.23\n",
    "    \"ps_reg_03\",  #            : 1408.42 / shadow  511.15\n",
    "    \"ps_ind_05_cat\",  #        : 1387.87 / shadow   84.72\n",
    "    \"ps_ind_03\",  #            : 1219.47 / shadow  230.55\n",
    "    \"ps_ind_15\",  #            :  922.18 / shadow  242.00\n",
    "    \"ps_reg_02\",  #            :  920.65 / shadow  267.50\n",
    "    \"ps_car_14\",  #            :  798.48 / shadow  549.58\n",
    "    \"ps_car_12\",  #            :  731.93 / shadow  293.62\n",
    "    \"ps_car_01_cat\",  #        :  698.07 / shadow  178.72\n",
    "    \"ps_car_07_cat\",  #        :  694.53 / shadow   36.35\n",
    "    \"ps_ind_17_bin\",  #        :  620.77 / shadow   23.15\n",
    "    \"ps_car_03_cat\",  #        :  611.73 / shadow   50.67\n",
    "    \"ps_reg_01\",  #            :  598.60 / shadow  178.57\n",
    "    \"ps_car_15\",  #            :  593.35 / shadow  226.43\n",
    "    \"ps_ind_01\",  #            :  547.32 / shadow  154.58\n",
    "    \"ps_ind_16_bin\",  #        :  475.37 / shadow   34.17\n",
    "    \"ps_ind_07_bin\",  #        :  435.28 / shadow   28.92\n",
    "    \"ps_car_06_cat\",  #        :  398.02 / shadow  212.43\n",
    "    \"ps_car_04_cat\",  #        :  376.87 / shadow   76.98\n",
    "    \"ps_ind_06_bin\",  #        :  370.97 / shadow   36.13\n",
    "    \"ps_car_09_cat\",  #        :  214.12 / shadow   81.38\n",
    "    \"ps_car_02_cat\",  #        :  203.03 / shadow   26.67\n",
    "    \"ps_ind_02_cat\",  #        :  189.47 / shadow   65.68\n",
    "    \"ps_car_11\",  #            :  173.28 / shadow   76.45\n",
    "    \"ps_car_05_cat\",  #        :  172.75 / shadow   62.92\n",
    "    \"ps_calc_09\",  #           :  169.13 / shadow  129.72\n",
    "    \"ps_calc_05\",  #           :  148.83 / shadow  120.68\n",
    "    \"ps_ind_08_bin\",  #        :  140.73 / shadow   27.63\n",
    "    \"ps_car_08_cat\",  #        :  120.87 / shadow   28.82\n",
    "    \"ps_ind_09_bin\",  #        :  113.92 / shadow   27.05\n",
    "    \"ps_ind_04_cat\",  #        :  107.27 / shadow   37.43\n",
    "    \"ps_ind_18_bin\",  #        :   77.42 / shadow   25.97\n",
    "    \"ps_ind_12_bin\",  #        :   39.67 / shadow   15.52\n",
    "    \"ps_ind_14\",  #            :   37.37 / shadow   16.65\n",
    "]\n",
    "# add combinations\n",
    "combs = [\n",
    "    ('ps_reg_01', 'ps_car_02_cat'),  \n",
    "    ('ps_reg_01', 'ps_car_04_cat'),\n",
    "]\n",
    "\n",
    "# Process data\n",
    "id_test = test_df['id'].values\n",
    "id_train = train_df['id'].values\n",
    "y = train_df['target']\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "combs = [\n",
    "    ('ps_reg_01', 'ps_car_02_cat'),  \n",
    "    ('ps_reg_01', 'ps_car_04_cat'),\n",
    "]\n",
    "for n_c, (f1, f2) in enumerate(combs):\n",
    "    name1 = f1 + \"_plus_\" + f2\n",
    "    print('current feature %60s %4d in %5.1f'\n",
    "          % (name1, n_c + 1, (time.time() - start) / 60), end='')\n",
    "    print('\\r' * 75, end='')\n",
    "    train_df[name1] = train_df[f1].apply(lambda x: str(x)) + \"_\" + train_df[f2].apply(lambda x: str(x))\n",
    "    test_df[name1] = test_df[f1].apply(lambda x: str(x)) + \"_\" + test_df[f2].apply(lambda x: str(x))\n",
    "    # Label Encode\n",
    "    lbl = LabelEncoder()\n",
    "    lbl.fit(list(train_df[name1].values) + list(test_df[name1].values))\n",
    "    train_df[name1] = lbl.transform(list(train_df[name1].values))\n",
    "    test_df[name1] = lbl.transform(list(test_df[name1].values))\n",
    "\n",
    "    train_features.append(name1)\n",
    "    \n",
    "X = train_df[train_features]\n",
    "test_df = test_df[train_features]\n",
    "\n",
    "y_valid_pred = 0*y\n",
    "y_test_pred = 0\n",
    "\n",
    "xgbtrain = xgb.DMatrix(X, label= y)\n",
    "\n",
    "# Set up classifier\n",
    "\n",
    "num_rounds = 3000\n",
    "random_state = 2000\n",
    "num_iter = 25\n",
    "init_points = 5\n",
    "params = {\n",
    "    'silent': 1,\n",
    "    'objective':'binary:logistic',\n",
    "    'eval_metric': 'auc',\n",
    "    'verbose_eval': True,\n",
    "    'seed': random_state\n",
    "}\n",
    "\n",
    "def xgb_evaluate(min_child_weight,\n",
    "                 colsample_bytree,\n",
    "                 subsample,\n",
    "                 gamma,\n",
    "                reg_lambda,\n",
    "                reg_alpha):\n",
    "\n",
    "    params['min_child_weight'] = int(min_child_weight)\n",
    "    params['colsample_bytree'] = max(min(colsample_bytree, 1), 0)\n",
    "    params['subsample'] = max(min(subsample, 1), 0)\n",
    "    params['gamma'] = max(gamma, 0) #minimum loss reduction required to make a further partition on a leaf node of\n",
    "                                    #the tree. The larger, the more conservative the algorithm will be.\n",
    "    params['reg_lambda'] = max(reg_lambda, 0)\n",
    "    params['reg_alpha'] = max(reg_alpha, 0)\n",
    "    params['eta'] = 0.03\n",
    "    params['max_depth'] = 6\n",
    "    \n",
    "    \n",
    "    cv_result = xgb.cv(params, xgbtrain, num_boost_round=num_rounds, nfold=5,\n",
    "             seed=random_state,\n",
    "             early_stopping_rounds=50)\n",
    "    \n",
    "    val_score = cv_result['test-auc-mean'].iloc[-1]\n",
    "    train_score = cv_result['train-auc-mean'].iloc[-1]\n",
    "    print(' Stopped after %d iterations with train-auc = %f val-auc = %f ( diff = %f ) train-gini = %f val-gini = %f' % \n",
    "          ( len(cv_result), train_score, val_score, (train_score - val_score), (train_score*2-1),(val_score*2-1)) )\n",
    "    \n",
    "    \n",
    "    return cv_result['test-auc-mean'].values[-1]\n",
    "\n",
    "\n",
    "xgbBO = BayesianOptimization(xgb_evaluate, {'min_child_weight': (0, 20),\n",
    "                                            'colsample_bytree': (0.1, 1),\n",
    "                                            'subsample': (0.5, 1),\n",
    "                                            'gamma': (0, 10),\n",
    "                                            'reg_lambda': (0, 10),\n",
    "                                            'reg_alpha':(0, 10)\n",
    "                                            })\n",
    "\n",
    "\n",
    "xgbBO.explore({\n",
    "              'gamma':                [0.5, 8, 0.2, 9, 0.5, 8, 0.2, 9],\n",
    "              'min_child_weight':     [0.2, 0.2, 0.2, 0.2, 12, 12, 12, 12],\n",
    "              'subsample':            [0.6, 0.8, 0.6, 0.8, 0.6, 0.8, 0.6, 0.8],\n",
    "              'colsample_bytree':     [0.6, 0.8, 0.6, 0.8, 0.6, 0.8, 0.6, 0.8],\n",
    "              'reg_lambda':           [0.1, 0.5, 1, 1.5, 0.1, 0.5, 1, 1.5],\n",
    "              'reg_alpha':            [0.1, 0.5, 1, 1.5, 0.1, 0.5, 1, 1.5]\n",
    "              })\n",
    "\n",
    "\n",
    "\n",
    "xgbBO.maximize(init_points=init_points, n_iter=num_iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current feature                                 ps_reg_01_plus_ps_car_04_cat    2 in   0.0\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   colsample_bytree |     gamma |   min_child_weight |   reg_alpha |   reg_lambda |   subsample | \n",
      "\n",
      "Fold  0\n",
      "  Gini =  0.153944495242336\n",
      "\n",
      "Fold  1\n",
      "  Gini =  0.1542559131554726\n",
      "\n",
      "Fold  2\n",
      "  Gini =  0.14905018530024072\n",
      "\n",
      "Fold  3\n",
      "  Gini =  0.161701455099305\n",
      "\n",
      "Fold  4\n",
      "  Gini =  0.16959644611209712\n",
      "The avg gini value is 0.1577096989818903\n",
      "    1 | 182m39s | \u001b[35m   0.15771\u001b[0m | \u001b[32m            0.6000\u001b[0m | \u001b[32m   0.5000\u001b[0m | \u001b[32m            0.2000\u001b[0m | \u001b[32m     0.1000\u001b[0m | \u001b[32m      0.1000\u001b[0m | \u001b[32m     0.6000\u001b[0m | \n",
      "\n",
      "Fold  0\n",
      "  Gini =  0.2494474541692855\n",
      "\n",
      "Fold  1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-d31f349e5dee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m \u001b[0mxgbBO\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_points\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit_points\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0macq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ei'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36mmaximize\u001b[0;34m(self, init_points, n_iter, acq, kappa, xi, **gp_params)\u001b[0m\n\u001b[1;32m    268\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_points\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0my_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(self, init_points)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_points\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-d31f349e5dee>\u001b[0m in \u001b[0;36mxgb_evaluate\u001b[0;34m(min_child_weight, colsample_bytree, subsample, gamma, reg_lambda, reg_alpha)\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m\"  Best gini = \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m             \u001b[0mfit_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0;31m# Generate validation predictions for this fold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/xgboost-0.6-py3.5.egg/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model)\u001b[0m\n\u001b[1;32m    505\u001b[0m                               \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 507\u001b[0;31m                               verbose_eval=verbose, xgb_model=None)\n\u001b[0m\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"objective\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/xgboost-0.6-py3.5.egg/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[1;32m    202\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/xgboost-0.6-py3.5.egg/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/xgboost-0.6-py3.5.egg/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m    894\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[0;32m--> 896\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m    897\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from numba import jit\n",
    "import time\n",
    "import gc\n",
    "import data_util \n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "\n",
    "@jit\n",
    "def eval_gini(y_true, y_prob):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_true = y_true[np.argsort(y_prob)]\n",
    "    ntrue = 0\n",
    "    gini = 0\n",
    "    delta = 0\n",
    "    n = len(y_true)\n",
    "    for i in range(n-1, -1, -1):\n",
    "        y_i = y_true[i]\n",
    "        ntrue += y_i\n",
    "        gini += y_i * delta\n",
    "        delta += 1 - y_i\n",
    "    gini = 1 - 2 * gini / (ntrue * (n - ntrue))\n",
    "    return gini\n",
    "\n",
    "train_df = data_util.load_train_data()\n",
    "test_df = data_util.load_test_data()\n",
    "\n",
    "train_features = [\n",
    "    \"ps_car_13\",  #            : 1571.65 / shadow  609.23\n",
    "    \"ps_reg_03\",  #            : 1408.42 / shadow  511.15\n",
    "    \"ps_ind_05_cat\",  #        : 1387.87 / shadow   84.72\n",
    "    \"ps_ind_03\",  #            : 1219.47 / shadow  230.55\n",
    "    \"ps_ind_15\",  #            :  922.18 / shadow  242.00\n",
    "    \"ps_reg_02\",  #            :  920.65 / shadow  267.50\n",
    "    \"ps_car_14\",  #            :  798.48 / shadow  549.58\n",
    "    \"ps_car_12\",  #            :  731.93 / shadow  293.62\n",
    "    \"ps_car_01_cat\",  #        :  698.07 / shadow  178.72\n",
    "    \"ps_car_07_cat\",  #        :  694.53 / shadow   36.35\n",
    "    \"ps_ind_17_bin\",  #        :  620.77 / shadow   23.15\n",
    "    \"ps_car_03_cat\",  #        :  611.73 / shadow   50.67\n",
    "    \"ps_reg_01\",  #            :  598.60 / shadow  178.57\n",
    "    \"ps_car_15\",  #            :  593.35 / shadow  226.43\n",
    "    \"ps_ind_01\",  #            :  547.32 / shadow  154.58\n",
    "    \"ps_ind_16_bin\",  #        :  475.37 / shadow   34.17\n",
    "    \"ps_ind_07_bin\",  #        :  435.28 / shadow   28.92\n",
    "    \"ps_car_06_cat\",  #        :  398.02 / shadow  212.43\n",
    "    \"ps_car_04_cat\",  #        :  376.87 / shadow   76.98\n",
    "    \"ps_ind_06_bin\",  #        :  370.97 / shadow   36.13\n",
    "    \"ps_car_09_cat\",  #        :  214.12 / shadow   81.38\n",
    "    \"ps_car_02_cat\",  #        :  203.03 / shadow   26.67\n",
    "    \"ps_ind_02_cat\",  #        :  189.47 / shadow   65.68\n",
    "    \"ps_car_11\",  #            :  173.28 / shadow   76.45\n",
    "    \"ps_car_05_cat\",  #        :  172.75 / shadow   62.92\n",
    "    \"ps_calc_09\",  #           :  169.13 / shadow  129.72\n",
    "    \"ps_calc_05\",  #           :  148.83 / shadow  120.68\n",
    "    \"ps_ind_08_bin\",  #        :  140.73 / shadow   27.63\n",
    "    \"ps_car_08_cat\",  #        :  120.87 / shadow   28.82\n",
    "    \"ps_ind_09_bin\",  #        :  113.92 / shadow   27.05\n",
    "    \"ps_ind_04_cat\",  #        :  107.27 / shadow   37.43\n",
    "    \"ps_ind_18_bin\",  #        :   77.42 / shadow   25.97\n",
    "    \"ps_ind_12_bin\",  #        :   39.67 / shadow   15.52\n",
    "    \"ps_ind_14\",  #            :   37.37 / shadow   16.65\n",
    "]\n",
    "# add combinations\n",
    "combs = [\n",
    "    ('ps_reg_01', 'ps_car_02_cat'),  \n",
    "    ('ps_reg_01', 'ps_car_04_cat'),\n",
    "]\n",
    "\n",
    "# Process data\n",
    "id_test = test_df['id'].values\n",
    "id_train = train_df['id'].values\n",
    "y = train_df['target']\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "combs = [\n",
    "    ('ps_reg_01', 'ps_car_02_cat'),  \n",
    "    ('ps_reg_01', 'ps_car_04_cat'),\n",
    "]\n",
    "for n_c, (f1, f2) in enumerate(combs):\n",
    "    name1 = f1 + \"_plus_\" + f2\n",
    "    print('current feature %60s %4d in %5.1f'\n",
    "          % (name1, n_c + 1, (time.time() - start) / 60), end='')\n",
    "    print('\\r' * 75, end='')\n",
    "    train_df[name1] = train_df[f1].apply(lambda x: str(x)) + \"_\" + train_df[f2].apply(lambda x: str(x))\n",
    "    test_df[name1] = test_df[f1].apply(lambda x: str(x)) + \"_\" + test_df[f2].apply(lambda x: str(x))\n",
    "    # Label Encode\n",
    "    lbl = LabelEncoder()\n",
    "    lbl.fit(list(train_df[name1].values) + list(test_df[name1].values))\n",
    "    train_df[name1] = lbl.transform(list(train_df[name1].values))\n",
    "    test_df[name1] = lbl.transform(list(test_df[name1].values))\n",
    "\n",
    "    train_features.append(name1)\n",
    "    \n",
    "X = train_df[train_features]\n",
    "test_df = test_df[train_features]\n",
    "\n",
    "y_valid_pred = 0*y\n",
    "y_test_pred = 0\n",
    "\n",
    "f_cats = [f for f in X.columns if \"_cat\" in f]\n",
    "\n",
    "# Set up folds\n",
    "K = 5\n",
    "kf = KFold(n_splits = K, random_state = 1, shuffle = True)\n",
    "np.random.seed(0)\n",
    "\n",
    "# Set up classifier\n",
    "\n",
    "                        #scale_pos_weight=1.6,\n",
    "\n",
    "# Run CV\n",
    "# Set up classifier\n",
    "\n",
    "num_rounds = 3000\n",
    "random_state = 2000\n",
    "num_iter = 25\n",
    "init_points = 5\n",
    "params = {\n",
    "    'silent': 1,\n",
    "    'objective':'binary:logistic',\n",
    "    'eval_metric': 'auc',\n",
    "    'verbose_eval': True,\n",
    "    'seed': random_state\n",
    "}\n",
    "\n",
    "def xgb_evaluate(min_child_weight,\n",
    "                 colsample_bytree,\n",
    "                 subsample,\n",
    "                 gamma,\n",
    "                reg_lambda,\n",
    "                reg_alpha):\n",
    "\n",
    "    params['min_child_weight'] = int(min_child_weight)\n",
    "    params['colsample_bytree'] = max(min(colsample_bytree, 1), 0)\n",
    "    params['subsample'] = max(min(subsample, 1), 0)\n",
    "    params['gamma'] = max(gamma, 0) #minimum loss reduction required to make a further partition on a leaf node of\n",
    "                                    #the tree. The larger, the more conservative the algorithm will be.\n",
    "    params['reg_lambda'] = max(reg_lambda, 0)\n",
    "    params['reg_alpha'] = max(reg_alpha, 0)\n",
    "    params['eta'] = 0.03\n",
    "    params['max_depth'] = 6\n",
    "    \n",
    "    model = XGBClassifier(  \n",
    "                        n_estimators=MAX_ROUNDS,\n",
    "                        **params\n",
    "                     )\n",
    "    kf = KFold(n_splits = K, random_state = 1, shuffle = True)\n",
    "    y_test_pred_all = np.zeros((len(test_df),5))\n",
    "    ###############################\n",
    "    gini = 0\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(train_df)):\n",
    "\n",
    "        # Create data for this fold\n",
    "        y_train, y_valid = y.iloc[train_index].copy(), y.iloc[test_index]\n",
    "        X_train, X_valid = X.iloc[train_index,:].copy(), X.iloc[test_index,:].copy()\n",
    "        X_test = test_df.copy()\n",
    "        print( \"\\nFold \", i)\n",
    "\n",
    "\n",
    "        for f in f_cats:\n",
    "            X_train[f + \"_avg\"], X_valid[f + \"_avg\"], X_test[f + \"_avg\"] = target_encode(\n",
    "                                                            trn_series=X_train[f],\n",
    "                                                            val_series=X_valid[f],\n",
    "                                                            tst_series=X_test[f],\n",
    "                                                            target=y_train,\n",
    "                                                            min_samples_leaf=200,\n",
    "                                                            smoothing=10,\n",
    "                                                            noise_level=0\n",
    "                                                            )\n",
    "\n",
    "        # Run model for this fold\n",
    "        if OPTIMIZE_ROUNDS:\n",
    "            eval_set=[(X_valid,y_valid)]\n",
    "            fit_model = model.fit( X_train, y_train, \n",
    "                                   eval_set=eval_set,\n",
    "                                   eval_metric='auc',\n",
    "                                   early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n",
    "                                   verbose=False\n",
    "                                 )\n",
    "            print( \"  Best N trees = \", model.best_ntree_limit )\n",
    "            print( \"  Best gini = \", model.best_score )\n",
    "        else:\n",
    "            fit_model = model.fit( X_train, y_train )\n",
    "\n",
    "        # Generate validation predictions for this fold\n",
    "        pred = fit_model.predict_proba(X_valid)[:,1]\n",
    "        print( \"  Gini = \", eval_gini(y_valid, pred) )\n",
    "        gini+= eval_gini(y_valid, pred)\n",
    "        # Accumulate test set predictions\n",
    "\n",
    "    ###############################\n",
    "    \n",
    "    avg_gini = gini/5\n",
    "    print('The avg gini value is {}'.format(avg_gini))\n",
    "    \n",
    "    return avg_gini\n",
    "\n",
    "xgbBO = BayesianOptimization(xgb_evaluate, {'min_child_weight': (0, 20),\n",
    "                                            'colsample_bytree': (0.1, 1),\n",
    "                                            'subsample': (0.5, 1),\n",
    "                                            'gamma': (0, 10),\n",
    "                                            'reg_lambda': (0, 10),\n",
    "                                            'reg_alpha':(0, 10)\n",
    "                                            })\n",
    "\n",
    "\n",
    "xgbBO.explore({\n",
    "              'gamma':                [0.5, 8, 0.2, 9, 0.5, 8, 0.2, 9],\n",
    "              'min_child_weight':     [0.2, 0.2, 0.2, 0.2, 12, 12, 12, 12],\n",
    "              'subsample':            [0.6, 0.8, 0.6, 0.8, 0.6, 0.8, 0.6, 0.8],\n",
    "              'colsample_bytree':     [0.6, 0.8, 0.6, 0.8, 0.6, 0.8, 0.6, 0.8],\n",
    "              'reg_lambda':           [0.1, 0.5, 1, 1.5, 0.1, 0.5, 1, 1.5],\n",
    "              'reg_alpha':            [0.1, 0.5, 1, 1.5, 0.1, 0.5, 1, 1.5]\n",
    "              })\n",
    "\n",
    "\n",
    "\n",
    "xgbBO.maximize(init_points=init_points, n_iter=num_iter,acq='ei', xi=0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
