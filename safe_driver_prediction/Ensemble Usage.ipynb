{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization and package preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data load and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import data_util\n",
    "import data_preprocess\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "import numpy as np \n",
    "\n",
    "rso = np.random.RandomState(66)\n",
    "\n",
    "train_data = data_util.load_train_data()\n",
    "test_data= data_util.load_test_data()\n",
    "\n",
    "lv1_pre = data_preprocess.preprocess_cell()\n",
    "\n",
    "X_train = train_data.drop(['id','target'],axis=1)\n",
    "y_train = train_data['target']\n",
    "y_train\n",
    "\n",
    "X_train, X_train_test, y_train, y_train_test = train_test_split(X_train,y_train,test_size =0.1 ,random_state=rso)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train,y_train, test_size =0.2, random_state=rso)\n",
    "\n",
    "X_train, y_train = lv1_pre.process(X_train, y=y_train, rso =rso)\n",
    "X_val = lv1_pre.process(X_val,test=True,rso =rso)\n",
    "X_train_test = lv1_pre.process(X_train_test,test=True,rso =rso)\n",
    "X_dev, y_dev = X_train[:10000,:], y_train[:10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load back neural network and give it a try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('search_lr_wd.pkl', 'rb') as f:\n",
    "    neural_outcome = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.autograd import Variable\n",
    "dtype = torch.FloatTensor\n",
    "\n",
    "\n",
    "nn_model = neural_outcome['best_net']\n",
    "nn_model.net.eval()\n",
    "# X_train_test_tensor = torch.from_numpy(X_train_test.values)\n",
    "# X_train_test_tensor= X_train_test_tensor.type(dtype)\n",
    "# out = nn_model.test(X_train_test_tensor)\n",
    "# nn_model.check_auc(out,y_train_test)\n",
    "\n",
    "\n",
    "idx = test_data['id']\n",
    "test_data_mutated = test_data.drop(['id'], axis=1)\n",
    "test_data_mutated = naive_pre.dtype_change(test_data_mutated)\n",
    "test_data_mutated = naive_pre.scale(test_data_mutated, test=True)\n",
    "test_data_tensor = torch.from_numpy(test_data_mutated.values)\n",
    "test_data_tensor = test_data_tensor.type(dtype)\n",
    "out = nn_model.test(test_data_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(892816, 2)\n"
     ]
    }
   ],
   "source": [
    "result= np.hstack((idx.values.reshape(892816,1), out.data.numpy()[:,1].reshape(892816,1)))\n",
    "print(result.shape)\n",
    "result[:,0] = result[:,0].astype(np.int32)\n",
    "np.savetxt('submission.csv',result, fmt=['%d', '%.9f'] , delimiter= ',', header= 'id, target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ! cat submission.csv 10\n",
    "#! sudo pip3 install lightgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try to use Lightgbm here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: svn: not found\r\n"
     ]
    }
   ],
   "source": [
    "! svn export https://github.com/Microsoft/LightGBM/tree/master/examples/python-guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/sklearn/utils/deprecation.py:77: DeprecationWarning: Function _ratio_float is deprecated; Use a float for 'ratio' is deprecated from version 0.2. The support will be removed in 0.4. Use a dict, str, or a callable instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import data_util\n",
    "import data_preprocess\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "import numpy as np \n",
    "\n",
    "rso = np.random.RandomState(66)\n",
    "\n",
    "train_data = data_util.load_train_data()\n",
    "test_data= data_util.load_test_data()\n",
    "\n",
    "lv1_pre = data_preprocess.preprocess_cell()\n",
    "\n",
    "X_train = train_data.drop(['id','target'],axis=1)\n",
    "y_train = train_data['target']\n",
    "y_train\n",
    "\n",
    "X_train, X_train_test, y_train, y_train_test = train_test_split(X_train,y_train,test_size =0.1 ,random_state=rso)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train,y_train, test_size =0.2, random_state=rso)\n",
    "\n",
    "X_train, y_train, col = lv1_pre.process(X_train, y=y_train, rso =rso)\n",
    "X_val = lv1_pre.process(X_val,test=True,rso =rso)\n",
    "X_train_test = lv1_pre.process(X_train_test,test=True,rso =rso)\n",
    "X_dev, y_dev = X_train[:10000,:], y_train[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "train_data11 = data_util.load_train_data()\n",
    "train_data11.head(10)\n",
    "# for col in train_data11.columns:\n",
    "#     if 'bin' in col or 'cat' in col:\n",
    "#         train_data11[col] = train_data11[col].astype('category')\n",
    "# data = pd.get_dummies(train_data11)\n",
    "# data.columns\n",
    "\n",
    "uselessones = ['ps_ind_14',\n",
    "             'ps_ind_10_bin',\n",
    "             'ps_ind_11_bin',\n",
    "             'ps_ind_12_bin', \n",
    "             'ps_ind_13_bin',\n",
    "             'ps_ind_18_bin',\n",
    "             'ps_car_10_cat',\n",
    "             'ps_calc_15_bin',\n",
    "             'ps_calc_20_bin'\n",
    "        ]\n",
    "print(train_data11.columns)\n",
    "train_data11 = train_data11.drop(uselessones, axis =1)\n",
    "print('hhhhh')\n",
    "print(train_data11.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If increase data will boost my performance??\n",
    "Well, not really, at least insignificantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's binary_logloss: 0.693147\n",
      "Training until validation scores don't improve for 5 rounds.\n",
      "[2]\tvalid_0's binary_logloss: 0.693147\n",
      "[3]\tvalid_0's binary_logloss: 0.693147\n",
      "[4]\tvalid_0's binary_logloss: 0.693147\n",
      "[5]\tvalid_0's binary_logloss: 0.693147\n",
      "[6]\tvalid_0's binary_logloss: 0.693147\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.693147\n",
      "1: We reach 0.5 with lr 0.16928286101010034, wd 10000000 at the 1 round!\n"
     ]
    }
   ],
   "source": [
    "import data_util\n",
    "import data_preprocess\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "import numpy as np \n",
    "from sklearn import metrics\n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "\n",
    "rso = np.random.RandomState(66)\n",
    "\n",
    "train_data = data_util.load_train_data()\n",
    "lv1_pre = data_preprocess.preprocess_cell()\n",
    "X_train = train_data.drop(['id','target'],axis=1)\n",
    "y_train = train_data['target']\n",
    "\n",
    "X_train, X_train_test, y_train, y_train_test = train_test_split(X_train,y_train,test_size =0.1 ,random_state=rso)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train,y_train, test_size =0.2, random_state=rso)\n",
    "\n",
    "X_train, y_train, col = lv1_pre.process(X_train, y=y_train, rso =rso)\n",
    "X_val = lv1_pre.process(X_val,test=True,rso =rso)\n",
    "X_train_test = lv1_pre.process(X_train_test,test=True,rso =rso)\n",
    "\n",
    "auc_dict={}\n",
    "\n",
    "lgb_train = lgb.Dataset(X_train, y_train, free_raw_data=False)\n",
    "lgb_val = lgb.Dataset(X_val, y_val, reference=lgb_train, free_raw_data=False)\n",
    "\n",
    "#seems like lambda_l2 equals 0.0342\n",
    "lgb1_params = {\n",
    "    'boosting_type':'gbdt',\n",
    "    'objective':'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'lambda_l2':0.0342,\n",
    "    'learning_rate': 0.09, \n",
    "    'feature_fraction': 0.9,   #\n",
    "    'bagging_fraction': 0.8,   #\n",
    "    'bagging_freq': 10,\n",
    "    'verbose': 1,\n",
    "    'verbose_eval':False\n",
    "}\n",
    "for i in range(1):\n",
    "    \n",
    "    #seems like the best combination is learning rate:0.1, Weight decay:0.05\n",
    "    lr = 10**np.random.uniform(-2,0)\n",
    "    #weight_decay = 10** np.random.uniform(-4,2)\n",
    "    weight_decay = 10000000\n",
    "    lgb1_params['learning_rate'] = lr\n",
    "    lgb1_params['lambda_l2'] = weight_decay\n",
    "    lgb1 = lgb.train(lgb1_params,\n",
    "            lgb_train,\n",
    "            num_boost_round =2500,  #Number of trees it will generate  \n",
    "            valid_sets= lgb_val,\n",
    "        early_stopping_rounds = 5\n",
    "            )\n",
    "\n",
    "    y_pred1 = lgb1.predict(X_train_test, lgb1.best_iteration)\n",
    "    score = metrics.roc_auc_score(y_train_test,y_pred1)\n",
    "    describe = '{}: We reach {} with lr {}, wd {} at the {} round!'.format(i+1, score,lr,weight_decay, lgb1.best_iteration)\n",
    "    print(describe)\n",
    "    auc_dict[describe] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71: We reach 0.6425146075906395 with lr 0.10090459507003365, wd 0.03426226856475561 at the 102 round!\n",
      "148: We reach 0.6424326925153181 with lr 0.10745944670147414, wd 0.0003203867371549406 at the 83 round!\n",
      "65: We reach 0.6421573746817226 with lr 0.04258541643753987, wd 0.21953639465793823 at the 237 round!\n",
      "35: We reach 0.6421277053525621 with lr 0.013641124580520884, wd 0.6284206847642912 at the 760 round!\n",
      "93: We reach 0.6421208784246081 with lr 0.014135085650213666, wd 9.483519863230152 at the 640 round!\n",
      "31: We reach 0.6421177994559204 with lr 0.04477025632952884, wd 0.0010087862148837454 at the 218 round!\n",
      "116: We reach 0.6419487947034517 with lr 0.01697460315960794, wd 0.0005371775659281679 at the 560 round!\n",
      "36: We reach 0.6418749961764769 with lr 0.0140269328712022, wd 0.035941098081195236 at the 652 round!\n",
      "7: We reach 0.6416946508246797 with lr 0.012987316213487771, wd 0.7248514625482201 at the 640 round!\n",
      "42: We reach 0.6416197641804985 with lr 0.01567690910094347, wd 4.111732584722368 at the 615 round!\n",
      "171: We reach 0.6415362048391764 with lr 0.011898710504949865, wd 2.645520163540573 at the 845 round!\n",
      "125: We reach 0.641460302618936 with lr 0.013450456685286294, wd 3.6289993401212484 at the 641 round!\n",
      "163: We reach 0.6414233146940468 with lr 0.04095613136364058, wd 2.5842654189845358 at the 261 round!\n",
      "159: We reach 0.6413777991543105 with lr 0.03743630199730534, wd 0.0014344799080794578 at the 249 round!\n",
      "162: We reach 0.6413630007603037 with lr 0.012451536541343905, wd 26.190418486496775 at the 782 round!\n",
      "19: We reach 0.6413245942529824 with lr 0.011377182449757453, wd 23.714370825138534 at the 785 round!\n",
      "122: We reach 0.6412723323865667 with lr 0.03962195139607323, wd 0.8436555237576461 at the 247 round!\n",
      "68: We reach 0.6412450004943678 with lr 0.04188142909953747, wd 0.09339682077444994 at the 247 round!\n",
      "91: We reach 0.6411983887772978 with lr 0.027010106715961997, wd 0.5060015225844995 at the 380 round!\n",
      "99: We reach 0.641184493117566 with lr 0.015623337327866887, wd 51.14899579899544 at the 640 round!\n",
      "54: We reach 0.6411666076947442 with lr 0.017646727333381382, wd 13.420552626683177 at the 643 round!\n",
      "69: We reach 0.641158168741299 with lr 0.10403634927861201, wd 0.05635404763969719 at the 102 round!\n",
      "60: We reach 0.6411361162325833 with lr 0.04859422728301708, wd 3.1210590608861604 at the 220 round!\n",
      "115: We reach 0.6411103318848559 with lr 0.01717280936714019, wd 0.0002332318062766132 at the 560 round!\n",
      "66: We reach 0.6410982336335465 with lr 0.05888841727739124, wd 1.160650035966149 at the 186 round!\n",
      "153: We reach 0.6410394591841474 with lr 0.013150081577453242, wd 0.10840877378540305 at the 684 round!\n",
      "181: We reach 0.6410209128308743 with lr 0.015756012075732573, wd 1.9963144060529505 at the 640 round!\n",
      "28: We reach 0.6409236270924967 with lr 0.011905227453842437, wd 0.5010863849122363 at the 766 round!\n",
      "177: We reach 0.640845218172618 with lr 0.03681066836057119, wd 0.03834489190813672 at the 243 round!\n",
      "55: We reach 0.6408200625148321 with lr 0.01217790836671795, wd 1.5646331784026402 at the 767 round!\n",
      "156: We reach 0.6408181200241155 with lr 0.03529437058897336, wd 0.001088233538660814 at the 285 round!\n",
      "67: We reach 0.6408007907500881 with lr 0.05164420545736684, wd 0.00029849434405169334 at the 190 round!\n",
      "56: We reach 0.6407821154347757 with lr 0.047831085659529975, wd 0.3829028335817983 at the 224 round!\n",
      "107: We reach 0.6407639640277479 with lr 0.028689176126040376, wd 0.1553021043153997 at the 319 round!\n",
      "120: We reach 0.6407465380321911 with lr 0.01029469163709374, wd 0.00028477319389358504 at the 896 round!\n",
      "47: We reach 0.6407422258640028 with lr 0.02418885270568613, wd 0.0005418076514953516 at the 412 round!\n",
      "88: We reach 0.6407148536711668 with lr 0.011003829841419097, wd 0.6924495710293782 at the 898 round!\n",
      "126: We reach 0.6407136527121761 with lr 0.030905551917218402, wd 0.0011858873473334487 at the 352 round!\n",
      "95: We reach 0.640697056909747 with lr 0.01828937556301799, wd 0.0054682832146763 at the 480 round!\n",
      "15: We reach 0.6406914873616758 with lr 0.039552455404299025, wd 0.00025946641892183925 at the 238 round!\n",
      "168: We reach 0.6406626804661529 with lr 0.020631948130652455, wd 0.006968847684417463 at the 478 round!\n",
      "136: We reach 0.6406384920236616 with lr 0.011399455709764084, wd 0.9589786814922799 at the 784 round!\n",
      "39: We reach 0.6406214932148598 with lr 0.012678902700874078, wd 4.871260719768081 at the 641 round!\n",
      "1: We reach 0.6405812651187337 with lr 0.09254887356613015, wd 1.2425868552411716 at the 108 round!\n",
      "78: We reach 0.6404959486696263 with lr 0.026342728905955456, wd 0.0006742303254545117 at the 375 round!\n",
      "185: We reach 0.6404740412432046 with lr 0.044903561868621746, wd 0.0179690084458655 at the 223 round!\n",
      "150: We reach 0.6404448877622012 with lr 0.021116558561080436, wd 0.0002836791762266724 at the 412 round!\n",
      "21: We reach 0.6404257772000063 with lr 0.051626295904810784, wd 0.2121544891780462 at the 196 round!\n",
      "108: We reach 0.6403563392019848 with lr 0.030446017802564167, wd 0.11735255181878367 at the 304 round!\n",
      "87: We reach 0.640338510200055 with lr 0.025530655269288015, wd 0.04338937738292714 at the 380 round!\n",
      "187: We reach 0.640287610495179 with lr 0.06882760944227757, wd 13.822026267542928 at the 143 round!\n",
      "79: We reach 0.6402858695076489 with lr 0.020777827269242113, wd 0.001740554954564852 at the 411 round!\n",
      "70: We reach 0.640282822779471 with lr 0.13318842612910065, wd 0.7340048730264138 at the 74 round!\n",
      "73: We reach 0.6402815089786958 with lr 0.02381367150532219, wd 0.00014771350047881136 at the 412 round!\n",
      "142: We reach 0.6401597688136211 with lr 0.04971729063376209, wd 0.0009256841172678133 at the 191 round!\n",
      "37: We reach 0.6401504190657736 with lr 0.012456721239684955, wd 0.00012458353481047411 at the 768 round!\n",
      "74: We reach 0.6400952233129644 with lr 0.03034183325403628, wd 0.004043915667037326 at the 308 round!\n",
      "141: We reach 0.64000894770869 with lr 0.01823617921600184, wd 32.92031856023356 at the 640 round!\n",
      "134: We reach 0.640001201926206 with lr 0.03278324791672489, wd 29.06042028214775 at the 310 round!\n",
      "146: We reach 0.639971186011565 with lr 0.10234315502711074, wd 0.003903446307280836 at the 84 round!\n",
      "169: We reach 0.6399401464607373 with lr 0.08330752950144044, wd 0.00013703147766740267 at the 111 round!\n",
      "106: We reach 0.6399186339805607 with lr 0.024856866787759956, wd 0.0001266729831977916 at the 405 round!\n",
      "173: We reach 0.6398010850817624 with lr 0.16780754784333843, wd 0.00951319902684638 at the 61 round!\n",
      "57: We reach 0.6397957089767501 with lr 0.1177762092645177, wd 4.028210990486796 at the 86 round!\n",
      "137: We reach 0.6397941775525338 with lr 0.11278555519694052, wd 0.0018637521993408585 at the 92 round!\n",
      "138: We reach 0.6397208706933338 with lr 0.07523347921022644, wd 27.961828215673645 at the 140 round!\n",
      "20: We reach 0.6397189523829997 with lr 0.07140323704109185, wd 6.081956827722914 at the 142 round!\n",
      "89: We reach 0.6396475154733693 with lr 0.02983316810975885, wd 0.00016430783082156423 at the 320 round!\n",
      "22: We reach 0.6396357960480502 with lr 0.0794857966865625, wd 0.013073060963804629 at the 118 round!\n",
      "132: We reach 0.6395943025119137 with lr 0.05420641628775015, wd 38.660010024748175 at the 191 round!\n",
      "62: We reach 0.6395769893581411 with lr 0.022525540595731187, wd 0.0005248037595009447 at the 410 round!\n",
      "161: We reach 0.639547166886559 with lr 0.029051259127036204, wd 0.22790161413052756 at the 320 round!\n",
      "80: We reach 0.6395429433797727 with lr 0.02156625900981813, wd 0.27721423292157965 at the 459 round!\n",
      "164: We reach 0.6394356792036064 with lr 0.028887487233197076, wd 0.007839226982905867 at the 320 round!\n",
      "52: We reach 0.6394050023585142 with lr 0.029991576809968938, wd 7.612076917229362 at the 396 round!\n",
      "83: We reach 0.6392955700080626 with lr 0.03433819233719858, wd 3.983065916596818 at the 306 round!\n",
      "155: We reach 0.6392658361978825 with lr 0.08261343551861577, wd 28.305351251212556 at the 140 round!\n",
      "135: We reach 0.6392291545578364 with lr 0.12559607446933135, wd 0.23528749407237035 at the 73 round!\n",
      "6: We reach 0.6392093266442981 with lr 0.13291882897214458, wd 10.070537783627476 at the 77 round!\n",
      "186: We reach 0.6391994610482936 with lr 0.05414235789308697, wd 0.0003174672679017957 at the 177 round!\n",
      "27: We reach 0.639182768524335 with lr 0.11830978108272594, wd 0.3515042024249584 at the 81 round!\n",
      "124: We reach 0.6390726994238146 with lr 0.09968015494747678, wd 70.61204711749437 at the 121 round!\n",
      "10: We reach 0.6390061147109118 with lr 0.11053658394891228, wd 0.0029025394591224375 at the 102 round!\n",
      "147: We reach 0.6389788069990954 with lr 0.034240664793792376, wd 81.8538736087291 at the 333 round!\n",
      "26: We reach 0.6389345246588598 with lr 0.07183405174827226, wd 22.930818280403518 at the 153 round!\n",
      "43: We reach 0.6388442915320052 with lr 0.07049644121954998, wd 0.003035107733017324 at the 129 round!\n",
      "84: We reach 0.6388141063546875 with lr 0.07688090697153588, wd 0.00012089645630136974 at the 125 round!\n",
      "64: We reach 0.6387644559695671 with lr 0.05979695157235675, wd 0.0003192570682860412 at the 156 round!\n",
      "117: We reach 0.6387298619025318 with lr 0.11426800027688874, wd 0.019537746741047652 at the 82 round!\n",
      "157: We reach 0.6387278388105405 with lr 0.11156602341086032, wd 1.4954734096757791 at the 96 round!\n",
      "145: We reach 0.638649147786201 with lr 0.10691812776678097, wd 8.234481582174684 at the 96 round!\n",
      "128: We reach 0.6385599060550232 with lr 0.12205726301277109, wd 0.002054321197490208 at the 72 round!\n",
      "23: We reach 0.6385031385773603 with lr 0.06500718495225312, wd 0.7733644002505425 at the 145 round!\n",
      "29: We reach 0.6384443157671964 with lr 0.08336059104870927, wd 0.0017604897429893947 at the 108 round!\n",
      "172: We reach 0.6384329509874853 with lr 0.22667540165272246, wd 0.0010932926729426144 at the 34 round!\n",
      "131: We reach 0.6383339806824699 with lr 0.05363555005909568, wd 0.09807436895804791 at the 186 round!\n",
      "32: We reach 0.6383307485713605 with lr 0.060763422344818935, wd 0.05023651863310688 at the 157 round!\n",
      "38: We reach 0.6382491317207548 with lr 0.15054522440096144, wd 0.42676239373084246 at the 58 round!\n",
      "102: We reach 0.6382322941145022 with lr 0.08558583211773338, wd 60.30434912418219 at the 140 round!\n",
      "17: We reach 0.6382214048823109 with lr 0.06911582353810553, wd 0.494791482439334 at the 139 round!\n",
      "76: We reach 0.6381394898069896 with lr 0.07276322996864529, wd 0.042458672257427534 at the 133 round!\n",
      "130: We reach 0.6379062780792172 with lr 0.09526581208766086, wd 94.80505960146068 at the 120 round!\n",
      "45: We reach 0.6378713374267013 with lr 0.06575340406303977, wd 0.41043272680502196 at the 155 round!\n",
      "86: We reach 0.6378525815101144 with lr 0.14845787647330533, wd 0.002311016501072018 at the 64 round!\n",
      "90: We reach 0.6378336724511058 with lr 0.06326543032832639, wd 59.35698861714012 at the 187 round!\n",
      "24: We reach 0.6376037734357173 with lr 0.08939030860284972, wd 0.12026752167497906 at the 112 round!\n",
      "111: We reach 0.6375981877673913 with lr 0.1428443738853448, wd 3.328855646851317 at the 80 round!\n",
      "30: We reach 0.6375706301916239 with lr 0.06807810034067231, wd 47.25485563404989 at the 139 round!\n",
      "183: We reach 0.6374633821357125 with lr 0.07024226109521464, wd 0.3348021606734124 at the 141 round!\n",
      "49: We reach 0.6374209697450463 with lr 0.09778152022191278, wd 1.2474559383090127 at the 102 round!\n",
      "119: We reach 0.6373973213310943 with lr 0.13314371701109684, wd 0.005939221677219017 at the 64 round!\n",
      "114: We reach 0.6373654193466288 with lr 0.0697926650987405, wd 0.0005584520018822516 at the 138 round!\n",
      "11: We reach 0.6373246028611986 with lr 0.2382867241800027, wd 0.0028683902570766162 at the 36 round!\n",
      "98: We reach 0.6371802298582311 with lr 0.23822583031700578, wd 0.015156215112302565 at the 36 round!\n",
      "61: We reach 0.6368833995444594 with lr 0.08404358445402176, wd 78.75843617298362 at the 126 round!\n",
      "14: We reach 0.6366909559413524 with lr 0.23768733309109233, wd 0.000943584050570683 at the 32 round!\n",
      "5: We reach 0.6366761575473459 with lr 0.28788892627375995, wd 0.03795193859803569 at the 28 round!\n",
      "152: We reach 0.6366423050120362 with lr 0.1763059984561974, wd 0.0006139822492713215 at the 51 round!\n",
      "109: We reach 0.6364634185433081 with lr 0.16147011182978996, wd 23.20193502216448 at the 68 round!\n",
      "154: We reach 0.6364512638711066 with lr 0.2981538144386258, wd 0.00012368412476270274 at the 26 round!\n",
      "72: We reach 0.636407392597371 with lr 0.12653570822681404, wd 68.32173053952626 at the 81 round!\n",
      "133: We reach 0.636398864982524 with lr 0.17150447616907075, wd 0.007874087156986613 at the 46 round!\n",
      "103: We reach 0.6361909298144494 with lr 0.14471648421697716, wd 0.017926124181365497 at the 65 round!\n",
      "176: We reach 0.6361163575152391 with lr 0.1596342232732236, wd 0.0031896124322834383 at the 56 round!\n",
      "82: We reach 0.6360531822362496 with lr 0.2507244244161868, wd 0.6269808920116435 at the 39 round!\n",
      "2: We reach 0.635864889598782 with lr 0.1809323653440157, wd 14.150618769563255 at the 60 round!\n",
      "160: We reach 0.6358001506550663 with lr 0.16417797028874134, wd 0.00031236151185703157 at the 51 round!\n",
      "118: We reach 0.6357886004924238 with lr 0.11769333235233859, wd 0.0004205738263549772 at the 75 round!\n",
      "175: We reach 0.6356153964135512 with lr 0.23955171081436666, wd 0.002640142642788531 at the 33 round!\n",
      "184: We reach 0.6355224309034894 with lr 0.25123583726517495, wd 1.0374907877847632 at the 37 round!\n",
      "77: We reach 0.635365451861183 with lr 0.2870447901214608, wd 0.009971919146369874 at the 27 round!\n",
      "139: We reach 0.6352765808958681 with lr 0.2857579845528326, wd 17.88275652742067 at the 33 round!\n",
      "48: We reach 0.6351985024412151 with lr 0.2747239356873587, wd 0.11359897559426688 at the 31 round!\n",
      "59: We reach 0.6349276257378473 with lr 0.19821536318757282, wd 1.0491842246021446 at the 52 round!\n",
      "151: We reach 0.6349013174818352 with lr 0.1567153371105409, wd 0.16479296219506184 at the 56 round!\n",
      "18: We reach 0.6348581151986784 with lr 0.25543001857259295, wd 0.09134059173071452 at the 34 round!\n",
      "41: We reach 0.6346103952414878 with lr 0.25710158306655867, wd 0.04684087409459131 at the 28 round!\n",
      "149: We reach 0.6341042353575915 with lr 0.28685523119302037, wd 1.9257425172151157 at the 29 round!\n",
      "92: We reach 0.6340927013152039 with lr 0.1567580613481686, wd 1.346420668827874 at the 61 round!\n",
      "3: We reach 0.6340865111373188 with lr 0.20457624034069175, wd 73.8519292022082 at the 59 round!\n",
      "51: We reach 0.6340612909985133 with lr 0.19919189565055537, wd 4.760653149476448 at the 52 round!\n",
      "113: We reach 0.6338323269579096 with lr 0.22922379192964723, wd 5.6970650428728575 at the 42 round!\n",
      "40: We reach 0.6337298424373238 with lr 0.3238614658038854, wd 1.6270301096045112 at the 31 round!\n",
      "127: We reach 0.6335159588951876 with lr 0.1875256712902443, wd 0.002921123272880906 at the 41 round!\n",
      "170: We reach 0.6334148768367791 with lr 0.44512412456564887, wd 2.782483652186722 at the 20 round!\n",
      "33: We reach 0.6333272713114746 with lr 0.35544614409689473, wd 0.4512681822798303 at the 23 round!\n",
      "121: We reach 0.6331305558408168 with lr 0.1935287019157817, wd 0.002993969859849505 at the 46 round!\n",
      "178: We reach 0.6330095572074679 with lr 0.28392191312407894, wd 0.0003435184827039059 at the 26 round!\n",
      "96: We reach 0.6329909786136849 with lr 0.38288064511793635, wd 0.40914462776909 at the 23 round!\n",
      "123: We reach 0.6329605677527987 with lr 0.388334468568872, wd 0.03242988437494277 at the 19 round!\n",
      "180: We reach 0.6327122432860487 with lr 0.30675314138586884, wd 64.42576300518738 at the 36 round!\n",
      "16: We reach 0.632399083154054 with lr 0.2197976735283846, wd 0.09849812038712766 at the 40 round!\n",
      "144: We reach 0.6313676850645766 with lr 0.31950311955384003, wd 0.00010275644700043796 at the 23 round!\n",
      "97: We reach 0.6312600823630572 with lr 0.43600970112269666, wd 52.256835619824805 at the 21 round!\n",
      "188: We reach 0.6307170312758013 with lr 0.44841256463626805, wd 19.192484689294155 at the 21 round!\n",
      "100: We reach 0.6304625891723133 with lr 0.3238393164365016, wd 0.00015489752169474437 at the 23 round!\n",
      "46: We reach 0.6303936750825762 with lr 0.507247518443318, wd 4.991422387702773 at the 18 round!\n",
      "94: We reach 0.6296890829208744 with lr 0.36509566786624925, wd 50.13594885465059 at the 33 round!\n",
      "179: We reach 0.6292645721280937 with lr 0.6612432920606013, wd 0.04524972646304054 at the 10 round!\n",
      "101: We reach 0.6291725012921795 with lr 0.6651567646394752, wd 0.03773632605831472 at the 9 round!\n",
      "166: We reach 0.6290551135959301 with lr 0.4811013444208525, wd 0.4452477496798672 at the 16 round!\n",
      "112: We reach 0.628616658782653 with lr 0.48763830548897796, wd 0.14185007563301305 at the 15 round!\n",
      "63: We reach 0.6285701196067299 with lr 0.4824685415257807, wd 40.614494246607606 at the 18 round!\n",
      "44: We reach 0.627742300186438 with lr 0.6101617885270045, wd 0.006330886888419875 at the 11 round!\n",
      "140: We reach 0.6276701983162924 with lr 0.5181620409177823, wd 0.023370994474232548 at the 14 round!\n",
      "129: We reach 0.6274764892731755 with lr 0.44671974099369877, wd 0.005482936769915275 at the 16 round!\n",
      "105: We reach 0.6259293639285133 with lr 0.6865024574967113, wd 15.848832892576572 at the 10 round!\n",
      "53: We reach 0.6256687316471402 with lr 0.6493749130848167, wd 28.221056861741857 at the 14 round!\n",
      "104: We reach 0.6250067251688449 with lr 0.5707344616845085, wd 0.000167028336840966 at the 12 round!\n",
      "75: We reach 0.625001699679377 with lr 0.7214336331386367, wd 0.0014106918006264665 at the 8 round!\n",
      "12: We reach 0.6249404749512316 with lr 0.6593478827881252, wd 60.32189483818619 at the 16 round!\n",
      "81: We reach 0.6236609982288918 with lr 0.7030020991400782, wd 7.389109664543593 at the 10 round!\n",
      "158: We reach 0.623096793337131 with lr 0.6272948116313554, wd 0.20347412434183024 at the 13 round!\n",
      "8: We reach 0.6225986532800523 with lr 0.709773187528865, wd 4.383918072237094 at the 10 round!\n",
      "13: We reach 0.6224444509516539 with lr 0.8498220076226594, wd 0.0005844560831886742 at the 7 round!\n",
      "4: We reach 0.6221637005921542 with lr 0.6926555367457454, wd 0.0023647434092113847 at the 8 round!\n",
      "50: We reach 0.6217448077082095 with lr 0.6038529120502579, wd 0.014328862750490261 at the 11 round!\n",
      "34: We reach 0.6217106891886939 with lr 0.8978817352233633, wd 0.17472751288144667 at the 7 round!\n",
      "165: We reach 0.6215547378126379 with lr 0.8450701038538557, wd 69.95580953539348 at the 12 round!\n",
      "110: We reach 0.6214903212740206 with lr 0.5903587203296202, wd 0.7609678216780167 at the 13 round!\n",
      "9: We reach 0.6212519873352507 with lr 0.6889079239885432, wd 0.04736251887002789 at the 9 round!\n",
      "182: We reach 0.6196534625578103 with lr 0.7389848135869477, wd 0.053548971531495335 at the 10 round!\n",
      "25: We reach 0.6191270113030471 with lr 0.9449495389544806, wd 17.659231006054465 at the 9 round!\n",
      "143: We reach 0.6180764583206739 with lr 0.8919116458467189, wd 35.606890713404795 at the 8 round!\n",
      "58: We reach 0.6171095251305787 with lr 0.7221103628395317, wd 7.082966918881887 at the 10 round!\n",
      "85: We reach 0.6157296111600266 with lr 0.9565653066506804, wd 0.5962432157466733 at the 7 round!\n",
      "174: We reach 0.6144144200129353 with lr 0.8149066799602968, wd 0.005854778914000867 at the 6 round!\n",
      "167: We reach 0.6043759423346886 with lr 0.9559656502891358, wd 0.012168564846522857 at the 5 round!\n"
     ]
    }
   ],
   "source": [
    "for i in sorted(auc_dict, key = lambda x: auc_dict[x], reverse=True):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_util\n",
    "import data_preprocess\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from datetime import datetime\n",
    "import numpy as np \n",
    "\n",
    "rso = np.random.RandomState(66)\n",
    "\n",
    "train_data = data_util.load_train_data()\n",
    "test_data= data_util.load_test_data()\n",
    "\n",
    "lv1_pre = data_preprocess.preprocess_cell()\n",
    "\n",
    "X_train = train_data.drop(['id','target'],axis=1)\n",
    "y_train = train_data['target']\n",
    "X_test = test_data.drop(['id'], axis=1)\n",
    "\n",
    "# combs = [\n",
    "#     ('ps_reg_01', 'ps_car_02_cat'),  \n",
    "#     ('ps_reg_01', 'ps_car_04_cat'),\n",
    "# ]\n",
    "\n",
    "# #'ps_reg_01_plus_ps_car_02_cat'\n",
    "# #'ps_reg_01_plus_ps_car_04_cat'\n",
    "# for n_c, (f1, f2) in enumerate(combs):\n",
    "#     name1 = f1 + \"_plus_\" + f2\n",
    "\n",
    "#     X_train[name1] = X_train[f1].apply(lambda x: str(x)) + \"_\" + X_train[f2].apply(lambda x: str(x))\n",
    "#     X_test[name1] = X_test[f1].apply(lambda x: str(x)) + \"_\" + X_test[f2].apply(lambda x: str(x)) \n",
    "#     lbl = LabelEncoder()\n",
    "#     lbl.fit(list(X_train[name1].values) + list(X_test[name1].values))\n",
    "#     X_train[name1] = lbl.transform(list(X_train[name1].values))\n",
    "#     X_test[name1] = lbl.transform(list(X_test[name1].values))\n",
    "\n",
    "\n",
    "X_train, y_train, col = lv1_pre.process(X_train, y=y_train, rso =rso, oversample= None)\n",
    "X_test = lv1_pre.process(X_test,test=True,rso =rso)\n",
    "\n",
    "result = pd.DataFrame()\n",
    "result['id'] = test_data['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's binary_logloss: 0.651205\n",
      "Training until validation scores don't improve for 5 rounds.\n",
      "[2]\tvalid_0's binary_logloss: 0.613263\n",
      "[3]\tvalid_0's binary_logloss: 0.578807\n",
      "[4]\tvalid_0's binary_logloss: 0.547396\n",
      "[5]\tvalid_0's binary_logloss: 0.51868\n",
      "[6]\tvalid_0's binary_logloss: 0.492351\n",
      "[7]\tvalid_0's binary_logloss: 0.468154\n",
      "[8]\tvalid_0's binary_logloss: 0.445878\n",
      "[9]\tvalid_0's binary_logloss: 0.425329\n",
      "[10]\tvalid_0's binary_logloss: 0.406339\n",
      "[11]\tvalid_0's binary_logloss: 0.388766\n",
      "[12]\tvalid_0's binary_logloss: 0.372485\n",
      "[13]\tvalid_0's binary_logloss: 0.357377\n",
      "[14]\tvalid_0's binary_logloss: 0.343355\n",
      "[15]\tvalid_0's binary_logloss: 0.330322\n",
      "[16]\tvalid_0's binary_logloss: 0.318198\n",
      "[17]\tvalid_0's binary_logloss: 0.306918\n",
      "[18]\tvalid_0's binary_logloss: 0.296413\n",
      "[19]\tvalid_0's binary_logloss: 0.286623\n",
      "[20]\tvalid_0's binary_logloss: 0.277504\n",
      "[21]\tvalid_0's binary_logloss: 0.268994\n",
      "[22]\tvalid_0's binary_logloss: 0.261055\n",
      "[23]\tvalid_0's binary_logloss: 0.253646\n",
      "[24]\tvalid_0's binary_logloss: 0.246733\n",
      "[25]\tvalid_0's binary_logloss: 0.240281\n",
      "[26]\tvalid_0's binary_logloss: 0.234258\n",
      "[27]\tvalid_0's binary_logloss: 0.228632\n",
      "[28]\tvalid_0's binary_logloss: 0.223379\n",
      "[29]\tvalid_0's binary_logloss: 0.218473\n",
      "[30]\tvalid_0's binary_logloss: 0.213891\n",
      "[31]\tvalid_0's binary_logloss: 0.209611\n",
      "[32]\tvalid_0's binary_logloss: 0.205622\n",
      "[33]\tvalid_0's binary_logloss: 0.201895\n",
      "[34]\tvalid_0's binary_logloss: 0.198413\n",
      "[35]\tvalid_0's binary_logloss: 0.195156\n",
      "[36]\tvalid_0's binary_logloss: 0.192133\n",
      "[37]\tvalid_0's binary_logloss: 0.189311\n",
      "[38]\tvalid_0's binary_logloss: 0.186682\n",
      "[39]\tvalid_0's binary_logloss: 0.184224\n",
      "[40]\tvalid_0's binary_logloss: 0.181942\n",
      "[41]\tvalid_0's binary_logloss: 0.179817\n",
      "[42]\tvalid_0's binary_logloss: 0.177839\n",
      "[43]\tvalid_0's binary_logloss: 0.175996\n",
      "[44]\tvalid_0's binary_logloss: 0.174269\n",
      "[45]\tvalid_0's binary_logloss: 0.172676\n",
      "[46]\tvalid_0's binary_logloss: 0.171185\n",
      "[47]\tvalid_0's binary_logloss: 0.169807\n",
      "[48]\tvalid_0's binary_logloss: 0.168515\n",
      "[49]\tvalid_0's binary_logloss: 0.167321\n",
      "[50]\tvalid_0's binary_logloss: 0.16622\n",
      "[51]\tvalid_0's binary_logloss: 0.165195\n",
      "[52]\tvalid_0's binary_logloss: 0.164236\n",
      "[53]\tvalid_0's binary_logloss: 0.163347\n",
      "[54]\tvalid_0's binary_logloss: 0.162523\n",
      "[55]\tvalid_0's binary_logloss: 0.161753\n",
      "[56]\tvalid_0's binary_logloss: 0.161049\n",
      "[57]\tvalid_0's binary_logloss: 0.160393\n",
      "[58]\tvalid_0's binary_logloss: 0.159777\n",
      "[59]\tvalid_0's binary_logloss: 0.159212\n",
      "[60]\tvalid_0's binary_logloss: 0.158688\n",
      "[61]\tvalid_0's binary_logloss: 0.158195\n",
      "[62]\tvalid_0's binary_logloss: 0.157736\n",
      "[63]\tvalid_0's binary_logloss: 0.157315\n",
      "[64]\tvalid_0's binary_logloss: 0.156929\n",
      "[65]\tvalid_0's binary_logloss: 0.156568\n",
      "[66]\tvalid_0's binary_logloss: 0.156236\n",
      "[67]\tvalid_0's binary_logloss: 0.155928\n",
      "[68]\tvalid_0's binary_logloss: 0.155649\n",
      "[69]\tvalid_0's binary_logloss: 0.155394\n",
      "[70]\tvalid_0's binary_logloss: 0.155147\n",
      "[71]\tvalid_0's binary_logloss: 0.154917\n",
      "[72]\tvalid_0's binary_logloss: 0.154708\n",
      "[73]\tvalid_0's binary_logloss: 0.154515\n",
      "[74]\tvalid_0's binary_logloss: 0.154334\n",
      "[75]\tvalid_0's binary_logloss: 0.154178\n",
      "[76]\tvalid_0's binary_logloss: 0.154022\n",
      "[77]\tvalid_0's binary_logloss: 0.153878\n",
      "[78]\tvalid_0's binary_logloss: 0.153742\n",
      "[79]\tvalid_0's binary_logloss: 0.153621\n",
      "[80]\tvalid_0's binary_logloss: 0.153507\n",
      "[81]\tvalid_0's binary_logloss: 0.153401\n",
      "[82]\tvalid_0's binary_logloss: 0.153304\n",
      "[83]\tvalid_0's binary_logloss: 0.153215\n",
      "[84]\tvalid_0's binary_logloss: 0.153135\n",
      "[85]\tvalid_0's binary_logloss: 0.153064\n",
      "[86]\tvalid_0's binary_logloss: 0.153001\n",
      "[87]\tvalid_0's binary_logloss: 0.15293\n",
      "[88]\tvalid_0's binary_logloss: 0.152869\n",
      "[89]\tvalid_0's binary_logloss: 0.152813\n",
      "[90]\tvalid_0's binary_logloss: 0.152745\n",
      "[91]\tvalid_0's binary_logloss: 0.152696\n",
      "[92]\tvalid_0's binary_logloss: 0.15265\n",
      "[93]\tvalid_0's binary_logloss: 0.1526\n",
      "[94]\tvalid_0's binary_logloss: 0.152564\n",
      "[95]\tvalid_0's binary_logloss: 0.152529\n",
      "[96]\tvalid_0's binary_logloss: 0.152489\n",
      "[97]\tvalid_0's binary_logloss: 0.152456\n",
      "[98]\tvalid_0's binary_logloss: 0.152428\n",
      "[99]\tvalid_0's binary_logloss: 0.152401\n",
      "[100]\tvalid_0's binary_logloss: 0.152369\n",
      "[101]\tvalid_0's binary_logloss: 0.152338\n",
      "[102]\tvalid_0's binary_logloss: 0.152313\n",
      "[103]\tvalid_0's binary_logloss: 0.152287\n",
      "[104]\tvalid_0's binary_logloss: 0.152259\n",
      "[105]\tvalid_0's binary_logloss: 0.152232\n",
      "[106]\tvalid_0's binary_logloss: 0.152214\n",
      "[107]\tvalid_0's binary_logloss: 0.152193\n",
      "[108]\tvalid_0's binary_logloss: 0.152174\n",
      "[109]\tvalid_0's binary_logloss: 0.15216\n",
      "[110]\tvalid_0's binary_logloss: 0.152134\n",
      "[111]\tvalid_0's binary_logloss: 0.152121\n",
      "[112]\tvalid_0's binary_logloss: 0.152112\n",
      "[113]\tvalid_0's binary_logloss: 0.152102\n",
      "[114]\tvalid_0's binary_logloss: 0.152089\n",
      "[115]\tvalid_0's binary_logloss: 0.152071\n",
      "[116]\tvalid_0's binary_logloss: 0.152061\n",
      "[117]\tvalid_0's binary_logloss: 0.152044\n",
      "[118]\tvalid_0's binary_logloss: 0.152039\n",
      "[119]\tvalid_0's binary_logloss: 0.152032\n",
      "[120]\tvalid_0's binary_logloss: 0.15201\n",
      "[121]\tvalid_0's binary_logloss: 0.151996\n",
      "[122]\tvalid_0's binary_logloss: 0.151988\n",
      "[123]\tvalid_0's binary_logloss: 0.151983\n",
      "[124]\tvalid_0's binary_logloss: 0.151973\n",
      "[125]\tvalid_0's binary_logloss: 0.151965\n",
      "[126]\tvalid_0's binary_logloss: 0.151955\n",
      "[127]\tvalid_0's binary_logloss: 0.151953\n",
      "[128]\tvalid_0's binary_logloss: 0.151939\n",
      "[129]\tvalid_0's binary_logloss: 0.151933\n",
      "[130]\tvalid_0's binary_logloss: 0.151934\n",
      "[131]\tvalid_0's binary_logloss: 0.151923\n",
      "[132]\tvalid_0's binary_logloss: 0.151919\n",
      "[133]\tvalid_0's binary_logloss: 0.151912\n",
      "[134]\tvalid_0's binary_logloss: 0.151901\n",
      "[135]\tvalid_0's binary_logloss: 0.151893\n",
      "[136]\tvalid_0's binary_logloss: 0.151889\n",
      "[137]\tvalid_0's binary_logloss: 0.151885\n",
      "[138]\tvalid_0's binary_logloss: 0.151881\n",
      "[139]\tvalid_0's binary_logloss: 0.151875\n",
      "[140]\tvalid_0's binary_logloss: 0.151872\n",
      "[141]\tvalid_0's binary_logloss: 0.151871\n",
      "[142]\tvalid_0's binary_logloss: 0.151867\n",
      "[143]\tvalid_0's binary_logloss: 0.151869\n",
      "[144]\tvalid_0's binary_logloss: 0.151864\n",
      "[145]\tvalid_0's binary_logloss: 0.151861\n",
      "[146]\tvalid_0's binary_logloss: 0.15186\n",
      "[147]\tvalid_0's binary_logloss: 0.15186\n",
      "[148]\tvalid_0's binary_logloss: 0.151864\n",
      "[149]\tvalid_0's binary_logloss: 0.151866\n",
      "[150]\tvalid_0's binary_logloss: 0.151867\n",
      "[151]\tvalid_0's binary_logloss: 0.151858\n",
      "[152]\tvalid_0's binary_logloss: 0.151867\n",
      "[153]\tvalid_0's binary_logloss: 0.151861\n",
      "[154]\tvalid_0's binary_logloss: 0.151862\n",
      "[155]\tvalid_0's binary_logloss: 0.15186\n",
      "[156]\tvalid_0's binary_logloss: 0.151862\n",
      "Early stopping, best iteration is:\n",
      "[151]\tvalid_0's binary_logloss: 0.151858\n",
      "151\n",
      "[1]\tvalid_0's binary_logloss: 0.651195\n",
      "Training until validation scores don't improve for 5 rounds.\n",
      "[2]\tvalid_0's binary_logloss: 0.613247\n",
      "[3]\tvalid_0's binary_logloss: 0.578776\n",
      "[4]\tvalid_0's binary_logloss: 0.547361\n",
      "[5]\tvalid_0's binary_logloss: 0.518637\n",
      "[6]\tvalid_0's binary_logloss: 0.492301\n",
      "[7]\tvalid_0's binary_logloss: 0.468101\n",
      "[8]\tvalid_0's binary_logloss: 0.445821\n",
      "[9]\tvalid_0's binary_logloss: 0.425269\n",
      "[10]\tvalid_0's binary_logloss: 0.406276\n",
      "[11]\tvalid_0's binary_logloss: 0.3887\n",
      "[12]\tvalid_0's binary_logloss: 0.372415\n",
      "[13]\tvalid_0's binary_logloss: 0.357311\n",
      "[14]\tvalid_0's binary_logloss: 0.343291\n",
      "[15]\tvalid_0's binary_logloss: 0.330256\n",
      "[16]\tvalid_0's binary_logloss: 0.318135\n",
      "[17]\tvalid_0's binary_logloss: 0.306851\n",
      "[18]\tvalid_0's binary_logloss: 0.296348\n",
      "[19]\tvalid_0's binary_logloss: 0.286561\n",
      "[20]\tvalid_0's binary_logloss: 0.277438\n",
      "[21]\tvalid_0's binary_logloss: 0.268914\n",
      "[22]\tvalid_0's binary_logloss: 0.260963\n",
      "[23]\tvalid_0's binary_logloss: 0.253548\n",
      "[24]\tvalid_0's binary_logloss: 0.246629\n",
      "[25]\tvalid_0's binary_logloss: 0.240164\n",
      "[26]\tvalid_0's binary_logloss: 0.234139\n",
      "[27]\tvalid_0's binary_logloss: 0.228514\n",
      "[28]\tvalid_0's binary_logloss: 0.223254\n",
      "[29]\tvalid_0's binary_logloss: 0.218345\n",
      "[30]\tvalid_0's binary_logloss: 0.213763\n",
      "[31]\tvalid_0's binary_logloss: 0.209498\n",
      "[32]\tvalid_0's binary_logloss: 0.205512\n",
      "[33]\tvalid_0's binary_logloss: 0.201795\n",
      "[34]\tvalid_0's binary_logloss: 0.198326\n",
      "[35]\tvalid_0's binary_logloss: 0.19509\n",
      "[36]\tvalid_0's binary_logloss: 0.192069\n",
      "[37]\tvalid_0's binary_logloss: 0.189251\n",
      "[38]\tvalid_0's binary_logloss: 0.186629\n",
      "[39]\tvalid_0's binary_logloss: 0.18418\n",
      "[40]\tvalid_0's binary_logloss: 0.181898\n",
      "[41]\tvalid_0's binary_logloss: 0.179772\n",
      "[42]\tvalid_0's binary_logloss: 0.177787\n",
      "[43]\tvalid_0's binary_logloss: 0.175936\n",
      "[44]\tvalid_0's binary_logloss: 0.174218\n",
      "[45]\tvalid_0's binary_logloss: 0.172623\n",
      "[46]\tvalid_0's binary_logloss: 0.171134\n",
      "[47]\tvalid_0's binary_logloss: 0.169751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[48]\tvalid_0's binary_logloss: 0.168465\n",
      "[49]\tvalid_0's binary_logloss: 0.167275\n",
      "[50]\tvalid_0's binary_logloss: 0.16617\n",
      "[51]\tvalid_0's binary_logloss: 0.165141\n",
      "[52]\tvalid_0's binary_logloss: 0.164187\n",
      "[53]\tvalid_0's binary_logloss: 0.16331\n",
      "[54]\tvalid_0's binary_logloss: 0.162496\n",
      "[55]\tvalid_0's binary_logloss: 0.16173\n",
      "[56]\tvalid_0's binary_logloss: 0.161021\n",
      "[57]\tvalid_0's binary_logloss: 0.160373\n",
      "[58]\tvalid_0's binary_logloss: 0.159761\n",
      "[59]\tvalid_0's binary_logloss: 0.159195\n",
      "[60]\tvalid_0's binary_logloss: 0.158683\n",
      "[61]\tvalid_0's binary_logloss: 0.158208\n",
      "[62]\tvalid_0's binary_logloss: 0.157765\n",
      "[63]\tvalid_0's binary_logloss: 0.157352\n",
      "[64]\tvalid_0's binary_logloss: 0.156966\n",
      "[65]\tvalid_0's binary_logloss: 0.156611\n",
      "[66]\tvalid_0's binary_logloss: 0.156286\n",
      "[67]\tvalid_0's binary_logloss: 0.155982\n",
      "[68]\tvalid_0's binary_logloss: 0.155704\n",
      "[69]\tvalid_0's binary_logloss: 0.155441\n",
      "[70]\tvalid_0's binary_logloss: 0.155206\n",
      "[71]\tvalid_0's binary_logloss: 0.154989\n",
      "[72]\tvalid_0's binary_logloss: 0.154794\n",
      "[73]\tvalid_0's binary_logloss: 0.154612\n",
      "[74]\tvalid_0's binary_logloss: 0.154431\n",
      "[75]\tvalid_0's binary_logloss: 0.154279\n",
      "[76]\tvalid_0's binary_logloss: 0.154141\n",
      "[77]\tvalid_0's binary_logloss: 0.153998\n",
      "[78]\tvalid_0's binary_logloss: 0.153875\n",
      "[79]\tvalid_0's binary_logloss: 0.153751\n",
      "[80]\tvalid_0's binary_logloss: 0.153649\n",
      "[81]\tvalid_0's binary_logloss: 0.153545\n",
      "[82]\tvalid_0's binary_logloss: 0.153454\n",
      "[83]\tvalid_0's binary_logloss: 0.153364\n",
      "[84]\tvalid_0's binary_logloss: 0.153281\n",
      "[85]\tvalid_0's binary_logloss: 0.153205\n",
      "[86]\tvalid_0's binary_logloss: 0.153138\n",
      "[87]\tvalid_0's binary_logloss: 0.153073\n",
      "[88]\tvalid_0's binary_logloss: 0.153003\n",
      "[89]\tvalid_0's binary_logloss: 0.152949\n",
      "[90]\tvalid_0's binary_logloss: 0.152903\n",
      "[91]\tvalid_0's binary_logloss: 0.152846\n",
      "[92]\tvalid_0's binary_logloss: 0.152793\n",
      "[93]\tvalid_0's binary_logloss: 0.152749\n",
      "[94]\tvalid_0's binary_logloss: 0.15271\n",
      "[95]\tvalid_0's binary_logloss: 0.152674\n",
      "[96]\tvalid_0's binary_logloss: 0.15265\n",
      "[97]\tvalid_0's binary_logloss: 0.152625\n",
      "[98]\tvalid_0's binary_logloss: 0.152596\n",
      "[99]\tvalid_0's binary_logloss: 0.152564\n",
      "[100]\tvalid_0's binary_logloss: 0.15254\n",
      "[101]\tvalid_0's binary_logloss: 0.152522\n",
      "[102]\tvalid_0's binary_logloss: 0.152493\n",
      "[103]\tvalid_0's binary_logloss: 0.152478\n",
      "[104]\tvalid_0's binary_logloss: 0.152457\n",
      "[105]\tvalid_0's binary_logloss: 0.152447\n",
      "[106]\tvalid_0's binary_logloss: 0.152436\n",
      "[107]\tvalid_0's binary_logloss: 0.152415\n",
      "[108]\tvalid_0's binary_logloss: 0.152397\n",
      "[109]\tvalid_0's binary_logloss: 0.152382\n",
      "[110]\tvalid_0's binary_logloss: 0.152368\n",
      "[111]\tvalid_0's binary_logloss: 0.152354\n",
      "[112]\tvalid_0's binary_logloss: 0.152344\n",
      "[113]\tvalid_0's binary_logloss: 0.152337\n",
      "[114]\tvalid_0's binary_logloss: 0.152319\n",
      "[115]\tvalid_0's binary_logloss: 0.1523\n",
      "[116]\tvalid_0's binary_logloss: 0.152295\n",
      "[117]\tvalid_0's binary_logloss: 0.152295\n",
      "[118]\tvalid_0's binary_logloss: 0.152283\n",
      "[119]\tvalid_0's binary_logloss: 0.152273\n",
      "[120]\tvalid_0's binary_logloss: 0.152257\n",
      "[121]\tvalid_0's binary_logloss: 0.152248\n",
      "[122]\tvalid_0's binary_logloss: 0.152235\n",
      "[123]\tvalid_0's binary_logloss: 0.152225\n",
      "[124]\tvalid_0's binary_logloss: 0.152219\n",
      "[125]\tvalid_0's binary_logloss: 0.152211\n",
      "[126]\tvalid_0's binary_logloss: 0.152205\n",
      "[127]\tvalid_0's binary_logloss: 0.152209\n",
      "[128]\tvalid_0's binary_logloss: 0.152202\n",
      "[129]\tvalid_0's binary_logloss: 0.152203\n",
      "[130]\tvalid_0's binary_logloss: 0.152202\n",
      "[131]\tvalid_0's binary_logloss: 0.152191\n",
      "[132]\tvalid_0's binary_logloss: 0.152191\n",
      "[133]\tvalid_0's binary_logloss: 0.152192\n",
      "[134]\tvalid_0's binary_logloss: 0.152182\n",
      "[135]\tvalid_0's binary_logloss: 0.152185\n",
      "[136]\tvalid_0's binary_logloss: 0.152184\n",
      "[137]\tvalid_0's binary_logloss: 0.15218\n",
      "[138]\tvalid_0's binary_logloss: 0.152182\n",
      "[139]\tvalid_0's binary_logloss: 0.152186\n",
      "[140]\tvalid_0's binary_logloss: 0.152179\n",
      "[141]\tvalid_0's binary_logloss: 0.152177\n",
      "[142]\tvalid_0's binary_logloss: 0.152176\n",
      "[143]\tvalid_0's binary_logloss: 0.152174\n",
      "[144]\tvalid_0's binary_logloss: 0.152174\n",
      "[145]\tvalid_0's binary_logloss: 0.152171\n",
      "[146]\tvalid_0's binary_logloss: 0.152171\n",
      "[147]\tvalid_0's binary_logloss: 0.152172\n",
      "[148]\tvalid_0's binary_logloss: 0.152178\n",
      "[149]\tvalid_0's binary_logloss: 0.152174\n",
      "[150]\tvalid_0's binary_logloss: 0.152166\n",
      "[151]\tvalid_0's binary_logloss: 0.152169\n",
      "[152]\tvalid_0's binary_logloss: 0.15217\n",
      "[153]\tvalid_0's binary_logloss: 0.152173\n",
      "[154]\tvalid_0's binary_logloss: 0.152163\n",
      "[155]\tvalid_0's binary_logloss: 0.152164\n",
      "[156]\tvalid_0's binary_logloss: 0.15216\n",
      "[157]\tvalid_0's binary_logloss: 0.152157\n",
      "[158]\tvalid_0's binary_logloss: 0.152151\n",
      "[159]\tvalid_0's binary_logloss: 0.152149\n",
      "[160]\tvalid_0's binary_logloss: 0.152152\n",
      "[161]\tvalid_0's binary_logloss: 0.15215\n",
      "[162]\tvalid_0's binary_logloss: 0.15215\n",
      "[163]\tvalid_0's binary_logloss: 0.152152\n",
      "[164]\tvalid_0's binary_logloss: 0.152146\n",
      "[165]\tvalid_0's binary_logloss: 0.152144\n",
      "[166]\tvalid_0's binary_logloss: 0.152148\n",
      "[167]\tvalid_0's binary_logloss: 0.152147\n",
      "[168]\tvalid_0's binary_logloss: 0.15215\n",
      "[169]\tvalid_0's binary_logloss: 0.152144\n",
      "[170]\tvalid_0's binary_logloss: 0.152149\n",
      "[171]\tvalid_0's binary_logloss: 0.152157\n",
      "[172]\tvalid_0's binary_logloss: 0.152161\n",
      "[173]\tvalid_0's binary_logloss: 0.152166\n",
      "[174]\tvalid_0's binary_logloss: 0.152169\n",
      "Early stopping, best iteration is:\n",
      "[169]\tvalid_0's binary_logloss: 0.152144\n",
      "169\n",
      "[1]\tvalid_0's binary_logloss: 0.651178\n",
      "Training until validation scores don't improve for 5 rounds.\n",
      "[2]\tvalid_0's binary_logloss: 0.613218\n",
      "[3]\tvalid_0's binary_logloss: 0.578738\n",
      "[4]\tvalid_0's binary_logloss: 0.54731\n",
      "[5]\tvalid_0's binary_logloss: 0.518575\n",
      "[6]\tvalid_0's binary_logloss: 0.492232\n",
      "[7]\tvalid_0's binary_logloss: 0.468024\n",
      "[8]\tvalid_0's binary_logloss: 0.445735\n",
      "[9]\tvalid_0's binary_logloss: 0.425177\n",
      "[10]\tvalid_0's binary_logloss: 0.40618\n",
      "[11]\tvalid_0's binary_logloss: 0.388605\n",
      "[12]\tvalid_0's binary_logloss: 0.372324\n",
      "[13]\tvalid_0's binary_logloss: 0.357221\n",
      "[14]\tvalid_0's binary_logloss: 0.3432\n",
      "[15]\tvalid_0's binary_logloss: 0.330169\n",
      "[16]\tvalid_0's binary_logloss: 0.31805\n",
      "[17]\tvalid_0's binary_logloss: 0.306772\n",
      "[18]\tvalid_0's binary_logloss: 0.296269\n",
      "[19]\tvalid_0's binary_logloss: 0.286486\n",
      "[20]\tvalid_0's binary_logloss: 0.277363\n",
      "[21]\tvalid_0's binary_logloss: 0.268859\n",
      "[22]\tvalid_0's binary_logloss: 0.260926\n",
      "[23]\tvalid_0's binary_logloss: 0.253526\n",
      "[24]\tvalid_0's binary_logloss: 0.246622\n",
      "[25]\tvalid_0's binary_logloss: 0.240176\n",
      "[26]\tvalid_0's binary_logloss: 0.234162\n",
      "[27]\tvalid_0's binary_logloss: 0.228543\n",
      "[28]\tvalid_0's binary_logloss: 0.223297\n",
      "[29]\tvalid_0's binary_logloss: 0.218402\n",
      "[30]\tvalid_0's binary_logloss: 0.213827\n",
      "[31]\tvalid_0's binary_logloss: 0.209563\n",
      "[32]\tvalid_0's binary_logloss: 0.20558\n",
      "[33]\tvalid_0's binary_logloss: 0.201863\n",
      "[34]\tvalid_0's binary_logloss: 0.198393\n",
      "[35]\tvalid_0's binary_logloss: 0.195157\n",
      "[36]\tvalid_0's binary_logloss: 0.192137\n",
      "[37]\tvalid_0's binary_logloss: 0.189325\n",
      "[38]\tvalid_0's binary_logloss: 0.1867\n",
      "[39]\tvalid_0's binary_logloss: 0.184253\n",
      "[40]\tvalid_0's binary_logloss: 0.181972\n",
      "[41]\tvalid_0's binary_logloss: 0.179828\n",
      "[42]\tvalid_0's binary_logloss: 0.177843\n",
      "[43]\tvalid_0's binary_logloss: 0.175992\n",
      "[44]\tvalid_0's binary_logloss: 0.174266\n",
      "[45]\tvalid_0's binary_logloss: 0.172663\n",
      "[46]\tvalid_0's binary_logloss: 0.17117\n",
      "[47]\tvalid_0's binary_logloss: 0.169782\n",
      "[48]\tvalid_0's binary_logloss: 0.168493\n",
      "[49]\tvalid_0's binary_logloss: 0.167297\n",
      "[50]\tvalid_0's binary_logloss: 0.166188\n",
      "[51]\tvalid_0's binary_logloss: 0.165162\n",
      "[52]\tvalid_0's binary_logloss: 0.164218\n",
      "[53]\tvalid_0's binary_logloss: 0.16333\n",
      "[54]\tvalid_0's binary_logloss: 0.162502\n",
      "[55]\tvalid_0's binary_logloss: 0.161741\n",
      "[56]\tvalid_0's binary_logloss: 0.161039\n",
      "[57]\tvalid_0's binary_logloss: 0.160388\n",
      "[58]\tvalid_0's binary_logloss: 0.159786\n",
      "[59]\tvalid_0's binary_logloss: 0.159231\n",
      "[60]\tvalid_0's binary_logloss: 0.158719\n",
      "[61]\tvalid_0's binary_logloss: 0.158234\n",
      "[62]\tvalid_0's binary_logloss: 0.157788\n",
      "[63]\tvalid_0's binary_logloss: 0.157379\n",
      "[64]\tvalid_0's binary_logloss: 0.157005\n",
      "[65]\tvalid_0's binary_logloss: 0.156646\n",
      "[66]\tvalid_0's binary_logloss: 0.156323\n",
      "[67]\tvalid_0's binary_logloss: 0.156025\n",
      "[68]\tvalid_0's binary_logloss: 0.155747\n",
      "[69]\tvalid_0's binary_logloss: 0.155486\n",
      "[70]\tvalid_0's binary_logloss: 0.155248\n",
      "[71]\tvalid_0's binary_logloss: 0.155028\n",
      "[72]\tvalid_0's binary_logloss: 0.154818\n",
      "[73]\tvalid_0's binary_logloss: 0.154631\n",
      "[74]\tvalid_0's binary_logloss: 0.154453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[75]\tvalid_0's binary_logloss: 0.154297\n",
      "[76]\tvalid_0's binary_logloss: 0.154153\n",
      "[77]\tvalid_0's binary_logloss: 0.154015\n",
      "[78]\tvalid_0's binary_logloss: 0.153885\n",
      "[79]\tvalid_0's binary_logloss: 0.153766\n",
      "[80]\tvalid_0's binary_logloss: 0.15366\n",
      "[81]\tvalid_0's binary_logloss: 0.153559\n",
      "[82]\tvalid_0's binary_logloss: 0.153468\n",
      "[83]\tvalid_0's binary_logloss: 0.153384\n",
      "[84]\tvalid_0's binary_logloss: 0.15331\n",
      "[85]\tvalid_0's binary_logloss: 0.153239\n",
      "[86]\tvalid_0's binary_logloss: 0.153167\n",
      "[87]\tvalid_0's binary_logloss: 0.153096\n",
      "[88]\tvalid_0's binary_logloss: 0.153041\n",
      "[89]\tvalid_0's binary_logloss: 0.152986\n",
      "[90]\tvalid_0's binary_logloss: 0.15294\n",
      "[91]\tvalid_0's binary_logloss: 0.152883\n",
      "[92]\tvalid_0's binary_logloss: 0.152841\n",
      "[93]\tvalid_0's binary_logloss: 0.152794\n",
      "[94]\tvalid_0's binary_logloss: 0.152747\n",
      "[95]\tvalid_0's binary_logloss: 0.152707\n",
      "[96]\tvalid_0's binary_logloss: 0.152674\n",
      "[97]\tvalid_0's binary_logloss: 0.152643\n",
      "[98]\tvalid_0's binary_logloss: 0.152605\n",
      "[99]\tvalid_0's binary_logloss: 0.152567\n",
      "[100]\tvalid_0's binary_logloss: 0.152528\n",
      "[101]\tvalid_0's binary_logloss: 0.152507\n",
      "[102]\tvalid_0's binary_logloss: 0.152484\n",
      "[103]\tvalid_0's binary_logloss: 0.152467\n",
      "[104]\tvalid_0's binary_logloss: 0.15246\n",
      "[105]\tvalid_0's binary_logloss: 0.152442\n",
      "[106]\tvalid_0's binary_logloss: 0.15242\n",
      "[107]\tvalid_0's binary_logloss: 0.152403\n",
      "[108]\tvalid_0's binary_logloss: 0.152388\n",
      "[109]\tvalid_0's binary_logloss: 0.152379\n",
      "[110]\tvalid_0's binary_logloss: 0.152362\n",
      "[111]\tvalid_0's binary_logloss: 0.152353\n",
      "[112]\tvalid_0's binary_logloss: 0.152335\n",
      "[113]\tvalid_0's binary_logloss: 0.152326\n",
      "[114]\tvalid_0's binary_logloss: 0.152312\n",
      "[115]\tvalid_0's binary_logloss: 0.152303\n",
      "[116]\tvalid_0's binary_logloss: 0.152292\n",
      "[117]\tvalid_0's binary_logloss: 0.152277\n",
      "[118]\tvalid_0's binary_logloss: 0.152258\n",
      "[119]\tvalid_0's binary_logloss: 0.152251\n",
      "[120]\tvalid_0's binary_logloss: 0.152237\n",
      "[121]\tvalid_0's binary_logloss: 0.152224\n",
      "[122]\tvalid_0's binary_logloss: 0.152213\n",
      "[123]\tvalid_0's binary_logloss: 0.152201\n",
      "[124]\tvalid_0's binary_logloss: 0.152196\n",
      "[125]\tvalid_0's binary_logloss: 0.152186\n",
      "[126]\tvalid_0's binary_logloss: 0.152177\n",
      "[127]\tvalid_0's binary_logloss: 0.152173\n",
      "[128]\tvalid_0's binary_logloss: 0.152167\n",
      "[129]\tvalid_0's binary_logloss: 0.152158\n",
      "[130]\tvalid_0's binary_logloss: 0.15216\n",
      "[131]\tvalid_0's binary_logloss: 0.152153\n",
      "[132]\tvalid_0's binary_logloss: 0.152155\n",
      "[133]\tvalid_0's binary_logloss: 0.152157\n",
      "[134]\tvalid_0's binary_logloss: 0.152153\n",
      "[135]\tvalid_0's binary_logloss: 0.152152\n",
      "[136]\tvalid_0's binary_logloss: 0.152152\n",
      "[137]\tvalid_0's binary_logloss: 0.152147\n",
      "[138]\tvalid_0's binary_logloss: 0.152151\n",
      "[139]\tvalid_0's binary_logloss: 0.15215\n",
      "[140]\tvalid_0's binary_logloss: 0.152143\n",
      "[141]\tvalid_0's binary_logloss: 0.152139\n",
      "[142]\tvalid_0's binary_logloss: 0.152133\n",
      "[143]\tvalid_0's binary_logloss: 0.152128\n",
      "[144]\tvalid_0's binary_logloss: 0.15214\n",
      "[145]\tvalid_0's binary_logloss: 0.152143\n",
      "[146]\tvalid_0's binary_logloss: 0.152142\n",
      "[147]\tvalid_0's binary_logloss: 0.152133\n",
      "[148]\tvalid_0's binary_logloss: 0.152132\n",
      "Early stopping, best iteration is:\n",
      "[143]\tvalid_0's binary_logloss: 0.152128\n",
      "143\n",
      "[1]\tvalid_0's binary_logloss: 0.651201\n",
      "Training until validation scores don't improve for 5 rounds.\n",
      "[2]\tvalid_0's binary_logloss: 0.613257\n",
      "[3]\tvalid_0's binary_logloss: 0.578797\n",
      "[4]\tvalid_0's binary_logloss: 0.547386\n",
      "[5]\tvalid_0's binary_logloss: 0.518667\n",
      "[6]\tvalid_0's binary_logloss: 0.492337\n",
      "[7]\tvalid_0's binary_logloss: 0.468142\n",
      "[8]\tvalid_0's binary_logloss: 0.445865\n",
      "[9]\tvalid_0's binary_logloss: 0.425314\n",
      "[10]\tvalid_0's binary_logloss: 0.406322\n",
      "[11]\tvalid_0's binary_logloss: 0.388731\n",
      "[12]\tvalid_0's binary_logloss: 0.372433\n",
      "[13]\tvalid_0's binary_logloss: 0.357316\n",
      "[14]\tvalid_0's binary_logloss: 0.34328\n",
      "[15]\tvalid_0's binary_logloss: 0.330233\n",
      "[16]\tvalid_0's binary_logloss: 0.318101\n",
      "[17]\tvalid_0's binary_logloss: 0.30681\n",
      "[18]\tvalid_0's binary_logloss: 0.296296\n",
      "[19]\tvalid_0's binary_logloss: 0.286502\n",
      "[20]\tvalid_0's binary_logloss: 0.277367\n",
      "[21]\tvalid_0's binary_logloss: 0.268872\n",
      "[22]\tvalid_0's binary_logloss: 0.260949\n",
      "[23]\tvalid_0's binary_logloss: 0.253556\n",
      "[24]\tvalid_0's binary_logloss: 0.246654\n",
      "[25]\tvalid_0's binary_logloss: 0.240205\n",
      "[26]\tvalid_0's binary_logloss: 0.234193\n",
      "[27]\tvalid_0's binary_logloss: 0.228581\n",
      "[28]\tvalid_0's binary_logloss: 0.223336\n",
      "[29]\tvalid_0's binary_logloss: 0.21844\n",
      "[30]\tvalid_0's binary_logloss: 0.213862\n",
      "[31]\tvalid_0's binary_logloss: 0.209583\n",
      "[32]\tvalid_0's binary_logloss: 0.20559\n",
      "[33]\tvalid_0's binary_logloss: 0.201864\n",
      "[34]\tvalid_0's binary_logloss: 0.198385\n",
      "[35]\tvalid_0's binary_logloss: 0.195137\n",
      "[36]\tvalid_0's binary_logloss: 0.192112\n",
      "[37]\tvalid_0's binary_logloss: 0.189286\n",
      "[38]\tvalid_0's binary_logloss: 0.186652\n",
      "[39]\tvalid_0's binary_logloss: 0.184192\n",
      "[40]\tvalid_0's binary_logloss: 0.181905\n",
      "[41]\tvalid_0's binary_logloss: 0.179771\n",
      "[42]\tvalid_0's binary_logloss: 0.177781\n",
      "[43]\tvalid_0's binary_logloss: 0.175928\n",
      "[44]\tvalid_0's binary_logloss: 0.174209\n",
      "[45]\tvalid_0's binary_logloss: 0.172604\n",
      "[46]\tvalid_0's binary_logloss: 0.17111\n",
      "[47]\tvalid_0's binary_logloss: 0.169717\n",
      "[48]\tvalid_0's binary_logloss: 0.168422\n",
      "[49]\tvalid_0's binary_logloss: 0.167221\n",
      "[50]\tvalid_0's binary_logloss: 0.16611\n",
      "[51]\tvalid_0's binary_logloss: 0.165079\n",
      "[52]\tvalid_0's binary_logloss: 0.164122\n",
      "[53]\tvalid_0's binary_logloss: 0.163243\n",
      "[54]\tvalid_0's binary_logloss: 0.162419\n",
      "[55]\tvalid_0's binary_logloss: 0.161657\n",
      "[56]\tvalid_0's binary_logloss: 0.160948\n",
      "[57]\tvalid_0's binary_logloss: 0.160296\n",
      "[58]\tvalid_0's binary_logloss: 0.159696\n",
      "[59]\tvalid_0's binary_logloss: 0.159141\n",
      "[60]\tvalid_0's binary_logloss: 0.158623\n",
      "[61]\tvalid_0's binary_logloss: 0.158138\n",
      "[62]\tvalid_0's binary_logloss: 0.157681\n",
      "[63]\tvalid_0's binary_logloss: 0.15726\n",
      "[64]\tvalid_0's binary_logloss: 0.156873\n",
      "[65]\tvalid_0's binary_logloss: 0.15652\n",
      "[66]\tvalid_0's binary_logloss: 0.156189\n",
      "[67]\tvalid_0's binary_logloss: 0.155876\n",
      "[68]\tvalid_0's binary_logloss: 0.155601\n",
      "[69]\tvalid_0's binary_logloss: 0.155338\n",
      "[70]\tvalid_0's binary_logloss: 0.155103\n",
      "[71]\tvalid_0's binary_logloss: 0.154869\n",
      "[72]\tvalid_0's binary_logloss: 0.15466\n",
      "[73]\tvalid_0's binary_logloss: 0.154465\n",
      "[74]\tvalid_0's binary_logloss: 0.154285\n",
      "[75]\tvalid_0's binary_logloss: 0.154115\n",
      "[76]\tvalid_0's binary_logloss: 0.153967\n",
      "[77]\tvalid_0's binary_logloss: 0.153818\n",
      "[78]\tvalid_0's binary_logloss: 0.153682\n",
      "[79]\tvalid_0's binary_logloss: 0.153557\n",
      "[80]\tvalid_0's binary_logloss: 0.153445\n",
      "[81]\tvalid_0's binary_logloss: 0.15334\n",
      "[82]\tvalid_0's binary_logloss: 0.153245\n",
      "[83]\tvalid_0's binary_logloss: 0.153154\n",
      "[84]\tvalid_0's binary_logloss: 0.153074\n",
      "[85]\tvalid_0's binary_logloss: 0.153\n",
      "[86]\tvalid_0's binary_logloss: 0.152926\n",
      "[87]\tvalid_0's binary_logloss: 0.152859\n",
      "[88]\tvalid_0's binary_logloss: 0.152788\n",
      "[89]\tvalid_0's binary_logloss: 0.152729\n",
      "[90]\tvalid_0's binary_logloss: 0.152672\n",
      "[91]\tvalid_0's binary_logloss: 0.152626\n",
      "[92]\tvalid_0's binary_logloss: 0.152576\n",
      "[93]\tvalid_0's binary_logloss: 0.152528\n",
      "[94]\tvalid_0's binary_logloss: 0.152486\n",
      "[95]\tvalid_0's binary_logloss: 0.152455\n",
      "[96]\tvalid_0's binary_logloss: 0.152422\n",
      "[97]\tvalid_0's binary_logloss: 0.152384\n",
      "[98]\tvalid_0's binary_logloss: 0.152363\n",
      "[99]\tvalid_0's binary_logloss: 0.152333\n",
      "[100]\tvalid_0's binary_logloss: 0.152294\n",
      "[101]\tvalid_0's binary_logloss: 0.152265\n",
      "[102]\tvalid_0's binary_logloss: 0.152243\n",
      "[103]\tvalid_0's binary_logloss: 0.152217\n",
      "[104]\tvalid_0's binary_logloss: 0.152189\n",
      "[105]\tvalid_0's binary_logloss: 0.15216\n",
      "[106]\tvalid_0's binary_logloss: 0.152142\n",
      "[107]\tvalid_0's binary_logloss: 0.152118\n",
      "[108]\tvalid_0's binary_logloss: 0.152099\n",
      "[109]\tvalid_0's binary_logloss: 0.152082\n",
      "[110]\tvalid_0's binary_logloss: 0.152064\n",
      "[111]\tvalid_0's binary_logloss: 0.152049\n",
      "[112]\tvalid_0's binary_logloss: 0.15204\n",
      "[113]\tvalid_0's binary_logloss: 0.152022\n",
      "[114]\tvalid_0's binary_logloss: 0.152008\n",
      "[115]\tvalid_0's binary_logloss: 0.152003\n",
      "[116]\tvalid_0's binary_logloss: 0.151995\n",
      "[117]\tvalid_0's binary_logloss: 0.151978\n",
      "[118]\tvalid_0's binary_logloss: 0.151971\n",
      "[119]\tvalid_0's binary_logloss: 0.151956\n",
      "[120]\tvalid_0's binary_logloss: 0.151945\n",
      "[121]\tvalid_0's binary_logloss: 0.151945\n",
      "[122]\tvalid_0's binary_logloss: 0.151939\n",
      "[123]\tvalid_0's binary_logloss: 0.151931\n",
      "[124]\tvalid_0's binary_logloss: 0.151933\n",
      "[125]\tvalid_0's binary_logloss: 0.151924\n",
      "[126]\tvalid_0's binary_logloss: 0.151923\n",
      "[127]\tvalid_0's binary_logloss: 0.151918\n",
      "[128]\tvalid_0's binary_logloss: 0.151914\n",
      "[129]\tvalid_0's binary_logloss: 0.151912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[130]\tvalid_0's binary_logloss: 0.151903\n",
      "[131]\tvalid_0's binary_logloss: 0.151902\n",
      "[132]\tvalid_0's binary_logloss: 0.151893\n",
      "[133]\tvalid_0's binary_logloss: 0.151884\n",
      "[134]\tvalid_0's binary_logloss: 0.151879\n",
      "[135]\tvalid_0's binary_logloss: 0.151874\n",
      "[136]\tvalid_0's binary_logloss: 0.151871\n",
      "[137]\tvalid_0's binary_logloss: 0.151856\n",
      "[138]\tvalid_0's binary_logloss: 0.151855\n",
      "[139]\tvalid_0's binary_logloss: 0.151857\n",
      "[140]\tvalid_0's binary_logloss: 0.151855\n",
      "[141]\tvalid_0's binary_logloss: 0.151839\n",
      "[142]\tvalid_0's binary_logloss: 0.151822\n",
      "[143]\tvalid_0's binary_logloss: 0.151818\n",
      "[144]\tvalid_0's binary_logloss: 0.151805\n",
      "[145]\tvalid_0's binary_logloss: 0.151797\n",
      "[146]\tvalid_0's binary_logloss: 0.151792\n",
      "[147]\tvalid_0's binary_logloss: 0.151781\n",
      "[148]\tvalid_0's binary_logloss: 0.151779\n",
      "[149]\tvalid_0's binary_logloss: 0.151776\n",
      "[150]\tvalid_0's binary_logloss: 0.151781\n",
      "[151]\tvalid_0's binary_logloss: 0.151779\n",
      "[152]\tvalid_0's binary_logloss: 0.15177\n",
      "[153]\tvalid_0's binary_logloss: 0.151763\n",
      "[154]\tvalid_0's binary_logloss: 0.151748\n",
      "[155]\tvalid_0's binary_logloss: 0.151749\n",
      "[156]\tvalid_0's binary_logloss: 0.15175\n",
      "[157]\tvalid_0's binary_logloss: 0.151742\n",
      "[158]\tvalid_0's binary_logloss: 0.151743\n",
      "[159]\tvalid_0's binary_logloss: 0.151744\n",
      "[160]\tvalid_0's binary_logloss: 0.151742\n",
      "[161]\tvalid_0's binary_logloss: 0.15174\n",
      "[162]\tvalid_0's binary_logloss: 0.151737\n",
      "[163]\tvalid_0's binary_logloss: 0.151736\n",
      "[164]\tvalid_0's binary_logloss: 0.151734\n",
      "[165]\tvalid_0's binary_logloss: 0.15173\n",
      "[166]\tvalid_0's binary_logloss: 0.151731\n",
      "[167]\tvalid_0's binary_logloss: 0.151733\n",
      "[168]\tvalid_0's binary_logloss: 0.151733\n",
      "[169]\tvalid_0's binary_logloss: 0.151733\n",
      "[170]\tvalid_0's binary_logloss: 0.151731\n",
      "Early stopping, best iteration is:\n",
      "[165]\tvalid_0's binary_logloss: 0.15173\n",
      "165\n",
      "[1]\tvalid_0's binary_logloss: 0.651168\n",
      "Training until validation scores don't improve for 5 rounds.\n",
      "[2]\tvalid_0's binary_logloss: 0.613196\n",
      "[3]\tvalid_0's binary_logloss: 0.578713\n",
      "[4]\tvalid_0's binary_logloss: 0.547279\n",
      "[5]\tvalid_0's binary_logloss: 0.518539\n",
      "[6]\tvalid_0's binary_logloss: 0.492196\n",
      "[7]\tvalid_0's binary_logloss: 0.467989\n",
      "[8]\tvalid_0's binary_logloss: 0.445696\n",
      "[9]\tvalid_0's binary_logloss: 0.425126\n",
      "[10]\tvalid_0's binary_logloss: 0.406124\n",
      "[11]\tvalid_0's binary_logloss: 0.388565\n",
      "[12]\tvalid_0's binary_logloss: 0.372295\n",
      "[13]\tvalid_0's binary_logloss: 0.357203\n",
      "[14]\tvalid_0's binary_logloss: 0.343191\n",
      "[15]\tvalid_0's binary_logloss: 0.33017\n",
      "[16]\tvalid_0's binary_logloss: 0.31806\n",
      "[17]\tvalid_0's binary_logloss: 0.306789\n",
      "[18]\tvalid_0's binary_logloss: 0.296293\n",
      "[19]\tvalid_0's binary_logloss: 0.286515\n",
      "[20]\tvalid_0's binary_logloss: 0.277394\n",
      "[21]\tvalid_0's binary_logloss: 0.26889\n",
      "[22]\tvalid_0's binary_logloss: 0.260958\n",
      "[23]\tvalid_0's binary_logloss: 0.253558\n",
      "[24]\tvalid_0's binary_logloss: 0.246649\n",
      "[25]\tvalid_0's binary_logloss: 0.240201\n",
      "[26]\tvalid_0's binary_logloss: 0.234183\n",
      "[27]\tvalid_0's binary_logloss: 0.228566\n",
      "[28]\tvalid_0's binary_logloss: 0.223319\n",
      "[29]\tvalid_0's binary_logloss: 0.218421\n",
      "[30]\tvalid_0's binary_logloss: 0.213851\n",
      "[31]\tvalid_0's binary_logloss: 0.209582\n",
      "[32]\tvalid_0's binary_logloss: 0.205596\n",
      "[33]\tvalid_0's binary_logloss: 0.20188\n",
      "[34]\tvalid_0's binary_logloss: 0.198404\n",
      "[35]\tvalid_0's binary_logloss: 0.195163\n",
      "[36]\tvalid_0's binary_logloss: 0.19214\n",
      "[37]\tvalid_0's binary_logloss: 0.189322\n",
      "[38]\tvalid_0's binary_logloss: 0.186694\n",
      "[39]\tvalid_0's binary_logloss: 0.184245\n",
      "[40]\tvalid_0's binary_logloss: 0.181964\n",
      "[41]\tvalid_0's binary_logloss: 0.179838\n",
      "[42]\tvalid_0's binary_logloss: 0.17786\n",
      "[43]\tvalid_0's binary_logloss: 0.176014\n",
      "[44]\tvalid_0's binary_logloss: 0.174298\n",
      "[45]\tvalid_0's binary_logloss: 0.172702\n",
      "[46]\tvalid_0's binary_logloss: 0.171223\n",
      "[47]\tvalid_0's binary_logloss: 0.169844\n",
      "[48]\tvalid_0's binary_logloss: 0.168563\n",
      "[49]\tvalid_0's binary_logloss: 0.167363\n",
      "[50]\tvalid_0's binary_logloss: 0.166265\n",
      "[51]\tvalid_0's binary_logloss: 0.165237\n",
      "[52]\tvalid_0's binary_logloss: 0.164285\n",
      "[53]\tvalid_0's binary_logloss: 0.163401\n",
      "[54]\tvalid_0's binary_logloss: 0.162587\n",
      "[55]\tvalid_0's binary_logloss: 0.161827\n",
      "[56]\tvalid_0's binary_logloss: 0.161125\n",
      "[57]\tvalid_0's binary_logloss: 0.160477\n",
      "[58]\tvalid_0's binary_logloss: 0.159858\n",
      "[59]\tvalid_0's binary_logloss: 0.159301\n",
      "[60]\tvalid_0's binary_logloss: 0.158777\n",
      "[61]\tvalid_0's binary_logloss: 0.158292\n",
      "[62]\tvalid_0's binary_logloss: 0.157838\n",
      "[63]\tvalid_0's binary_logloss: 0.157418\n",
      "[64]\tvalid_0's binary_logloss: 0.15703\n",
      "[65]\tvalid_0's binary_logloss: 0.156672\n",
      "[66]\tvalid_0's binary_logloss: 0.156339\n",
      "[67]\tvalid_0's binary_logloss: 0.156027\n",
      "[68]\tvalid_0's binary_logloss: 0.155737\n",
      "[69]\tvalid_0's binary_logloss: 0.155478\n",
      "[70]\tvalid_0's binary_logloss: 0.155232\n",
      "[71]\tvalid_0's binary_logloss: 0.155004\n",
      "[72]\tvalid_0's binary_logloss: 0.154806\n",
      "[73]\tvalid_0's binary_logloss: 0.154611\n",
      "[74]\tvalid_0's binary_logloss: 0.154441\n",
      "[75]\tvalid_0's binary_logloss: 0.15427\n",
      "[76]\tvalid_0's binary_logloss: 0.154114\n",
      "[77]\tvalid_0's binary_logloss: 0.153985\n",
      "[78]\tvalid_0's binary_logloss: 0.153851\n",
      "[79]\tvalid_0's binary_logloss: 0.153736\n",
      "[80]\tvalid_0's binary_logloss: 0.153628\n",
      "[81]\tvalid_0's binary_logloss: 0.153518\n",
      "[82]\tvalid_0's binary_logloss: 0.153417\n",
      "[83]\tvalid_0's binary_logloss: 0.153329\n",
      "[84]\tvalid_0's binary_logloss: 0.153249\n",
      "[85]\tvalid_0's binary_logloss: 0.153178\n",
      "[86]\tvalid_0's binary_logloss: 0.153106\n",
      "[87]\tvalid_0's binary_logloss: 0.153037\n",
      "[88]\tvalid_0's binary_logloss: 0.152976\n",
      "[89]\tvalid_0's binary_logloss: 0.152922\n",
      "[90]\tvalid_0's binary_logloss: 0.15287\n",
      "[91]\tvalid_0's binary_logloss: 0.152832\n",
      "[92]\tvalid_0's binary_logloss: 0.152788\n",
      "[93]\tvalid_0's binary_logloss: 0.152752\n",
      "[94]\tvalid_0's binary_logloss: 0.152718\n",
      "[95]\tvalid_0's binary_logloss: 0.152681\n",
      "[96]\tvalid_0's binary_logloss: 0.152643\n",
      "[97]\tvalid_0's binary_logloss: 0.152614\n",
      "[98]\tvalid_0's binary_logloss: 0.152571\n",
      "[99]\tvalid_0's binary_logloss: 0.152547\n",
      "[100]\tvalid_0's binary_logloss: 0.152523\n",
      "[101]\tvalid_0's binary_logloss: 0.152501\n",
      "[102]\tvalid_0's binary_logloss: 0.152467\n",
      "[103]\tvalid_0's binary_logloss: 0.152443\n",
      "[104]\tvalid_0's binary_logloss: 0.152424\n",
      "[105]\tvalid_0's binary_logloss: 0.152404\n",
      "[106]\tvalid_0's binary_logloss: 0.15238\n",
      "[107]\tvalid_0's binary_logloss: 0.152362\n",
      "[108]\tvalid_0's binary_logloss: 0.152343\n",
      "[109]\tvalid_0's binary_logloss: 0.152324\n",
      "[110]\tvalid_0's binary_logloss: 0.152308\n",
      "[111]\tvalid_0's binary_logloss: 0.152287\n",
      "[112]\tvalid_0's binary_logloss: 0.152283\n",
      "[113]\tvalid_0's binary_logloss: 0.152269\n",
      "[114]\tvalid_0's binary_logloss: 0.152261\n",
      "[115]\tvalid_0's binary_logloss: 0.152249\n",
      "[116]\tvalid_0's binary_logloss: 0.152239\n",
      "[117]\tvalid_0's binary_logloss: 0.152229\n",
      "[118]\tvalid_0's binary_logloss: 0.152217\n",
      "[119]\tvalid_0's binary_logloss: 0.152201\n",
      "[120]\tvalid_0's binary_logloss: 0.152193\n",
      "[121]\tvalid_0's binary_logloss: 0.152188\n",
      "[122]\tvalid_0's binary_logloss: 0.152191\n",
      "[123]\tvalid_0's binary_logloss: 0.152184\n",
      "[124]\tvalid_0's binary_logloss: 0.152171\n",
      "[125]\tvalid_0's binary_logloss: 0.152163\n",
      "[126]\tvalid_0's binary_logloss: 0.152157\n",
      "[127]\tvalid_0's binary_logloss: 0.152154\n",
      "[128]\tvalid_0's binary_logloss: 0.152144\n",
      "[129]\tvalid_0's binary_logloss: 0.152138\n",
      "[130]\tvalid_0's binary_logloss: 0.152127\n",
      "[131]\tvalid_0's binary_logloss: 0.152113\n",
      "[132]\tvalid_0's binary_logloss: 0.152094\n",
      "[133]\tvalid_0's binary_logloss: 0.152084\n",
      "[134]\tvalid_0's binary_logloss: 0.152074\n",
      "[135]\tvalid_0's binary_logloss: 0.152066\n",
      "[136]\tvalid_0's binary_logloss: 0.152064\n",
      "[137]\tvalid_0's binary_logloss: 0.152058\n",
      "[138]\tvalid_0's binary_logloss: 0.152054\n",
      "[139]\tvalid_0's binary_logloss: 0.152054\n",
      "[140]\tvalid_0's binary_logloss: 0.152052\n",
      "[141]\tvalid_0's binary_logloss: 0.152045\n",
      "[142]\tvalid_0's binary_logloss: 0.15204\n",
      "[143]\tvalid_0's binary_logloss: 0.15203\n",
      "[144]\tvalid_0's binary_logloss: 0.152024\n",
      "[145]\tvalid_0's binary_logloss: 0.152015\n",
      "[146]\tvalid_0's binary_logloss: 0.152015\n",
      "[147]\tvalid_0's binary_logloss: 0.15201\n",
      "[148]\tvalid_0's binary_logloss: 0.152002\n",
      "[149]\tvalid_0's binary_logloss: 0.152\n",
      "[150]\tvalid_0's binary_logloss: 0.152004\n",
      "[151]\tvalid_0's binary_logloss: 0.151995\n",
      "[152]\tvalid_0's binary_logloss: 0.151989\n",
      "[153]\tvalid_0's binary_logloss: 0.151983\n",
      "[154]\tvalid_0's binary_logloss: 0.151985\n",
      "[155]\tvalid_0's binary_logloss: 0.151977\n",
      "[156]\tvalid_0's binary_logloss: 0.151971\n",
      "[157]\tvalid_0's binary_logloss: 0.151973\n",
      "[158]\tvalid_0's binary_logloss: 0.151966\n",
      "[159]\tvalid_0's binary_logloss: 0.151964\n",
      "[160]\tvalid_0's binary_logloss: 0.151958\n",
      "[161]\tvalid_0's binary_logloss: 0.151957\n",
      "[162]\tvalid_0's binary_logloss: 0.151953\n",
      "[163]\tvalid_0's binary_logloss: 0.151947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[164]\tvalid_0's binary_logloss: 0.151943\n",
      "[165]\tvalid_0's binary_logloss: 0.151937\n",
      "[166]\tvalid_0's binary_logloss: 0.151939\n",
      "[167]\tvalid_0's binary_logloss: 0.151933\n",
      "[168]\tvalid_0's binary_logloss: 0.151932\n",
      "[169]\tvalid_0's binary_logloss: 0.151928\n",
      "[170]\tvalid_0's binary_logloss: 0.151921\n",
      "[171]\tvalid_0's binary_logloss: 0.151917\n",
      "[172]\tvalid_0's binary_logloss: 0.151914\n",
      "[173]\tvalid_0's binary_logloss: 0.151911\n",
      "[174]\tvalid_0's binary_logloss: 0.151902\n",
      "[175]\tvalid_0's binary_logloss: 0.151898\n",
      "[176]\tvalid_0's binary_logloss: 0.151896\n",
      "[177]\tvalid_0's binary_logloss: 0.151892\n",
      "[178]\tvalid_0's binary_logloss: 0.15189\n",
      "[179]\tvalid_0's binary_logloss: 0.151892\n",
      "[180]\tvalid_0's binary_logloss: 0.151887\n",
      "[181]\tvalid_0's binary_logloss: 0.151885\n",
      "[182]\tvalid_0's binary_logloss: 0.151891\n",
      "[183]\tvalid_0's binary_logloss: 0.15189\n",
      "[184]\tvalid_0's binary_logloss: 0.151884\n",
      "[185]\tvalid_0's binary_logloss: 0.151883\n",
      "[186]\tvalid_0's binary_logloss: 0.151883\n",
      "[187]\tvalid_0's binary_logloss: 0.151884\n",
      "[188]\tvalid_0's binary_logloss: 0.151888\n",
      "[189]\tvalid_0's binary_logloss: 0.151889\n",
      "[190]\tvalid_0's binary_logloss: 0.151889\n",
      "Early stopping, best iteration is:\n",
      "[185]\tvalid_0's binary_logloss: 0.151883\n",
      "185\n",
      "[1]\tvalid_0's binary_logloss: 0.611407\n",
      "Training until validation scores don't improve for 5 rounds.\n",
      "[2]\tvalid_0's binary_logloss: 0.544576\n",
      "[3]\tvalid_0's binary_logloss: 0.489051\n",
      "[4]\tvalid_0's binary_logloss: 0.442397\n",
      "[5]\tvalid_0's binary_logloss: 0.402844\n",
      "[6]\tvalid_0's binary_logloss: 0.36909\n",
      "[7]\tvalid_0's binary_logloss: 0.340109\n",
      "[8]\tvalid_0's binary_logloss: 0.315146\n",
      "[9]\tvalid_0's binary_logloss: 0.293565\n",
      "[10]\tvalid_0's binary_logloss: 0.274857\n",
      "[11]\tvalid_0's binary_logloss: 0.25861\n",
      "[12]\tvalid_0's binary_logloss: 0.244493\n",
      "[13]\tvalid_0's binary_logloss: 0.232208\n",
      "[14]\tvalid_0's binary_logloss: 0.221503\n",
      "[15]\tvalid_0's binary_logloss: 0.212185\n",
      "[16]\tvalid_0's binary_logloss: 0.204073\n",
      "[17]\tvalid_0's binary_logloss: 0.197018\n",
      "[18]\tvalid_0's binary_logloss: 0.190876\n",
      "[19]\tvalid_0's binary_logloss: 0.185555\n",
      "[20]\tvalid_0's binary_logloss: 0.180929\n",
      "[21]\tvalid_0's binary_logloss: 0.176912\n",
      "[22]\tvalid_0's binary_logloss: 0.173434\n",
      "[23]\tvalid_0's binary_logloss: 0.170432\n",
      "[24]\tvalid_0's binary_logloss: 0.16782\n",
      "[25]\tvalid_0's binary_logloss: 0.165597\n",
      "[26]\tvalid_0's binary_logloss: 0.163678\n",
      "[27]\tvalid_0's binary_logloss: 0.16202\n",
      "[28]\tvalid_0's binary_logloss: 0.160578\n",
      "[29]\tvalid_0's binary_logloss: 0.159372\n",
      "[30]\tvalid_0's binary_logloss: 0.158331\n",
      "[31]\tvalid_0's binary_logloss: 0.15742\n",
      "[32]\tvalid_0's binary_logloss: 0.156661\n",
      "[33]\tvalid_0's binary_logloss: 0.15599\n",
      "[34]\tvalid_0's binary_logloss: 0.155425\n",
      "[35]\tvalid_0's binary_logloss: 0.15495\n",
      "[36]\tvalid_0's binary_logloss: 0.154553\n",
      "[37]\tvalid_0's binary_logloss: 0.154207\n",
      "[38]\tvalid_0's binary_logloss: 0.153912\n",
      "[39]\tvalid_0's binary_logloss: 0.153638\n",
      "[40]\tvalid_0's binary_logloss: 0.153406\n",
      "[41]\tvalid_0's binary_logloss: 0.153211\n",
      "[42]\tvalid_0's binary_logloss: 0.153047\n",
      "[43]\tvalid_0's binary_logloss: 0.152902\n",
      "[44]\tvalid_0's binary_logloss: 0.152783\n",
      "[45]\tvalid_0's binary_logloss: 0.152697\n",
      "[46]\tvalid_0's binary_logloss: 0.152594\n",
      "[47]\tvalid_0's binary_logloss: 0.152497\n",
      "[48]\tvalid_0's binary_logloss: 0.152422\n",
      "[49]\tvalid_0's binary_logloss: 0.15235\n",
      "[50]\tvalid_0's binary_logloss: 0.152283\n",
      "[51]\tvalid_0's binary_logloss: 0.15224\n",
      "[52]\tvalid_0's binary_logloss: 0.152188\n",
      "[53]\tvalid_0's binary_logloss: 0.152137\n",
      "[54]\tvalid_0's binary_logloss: 0.152095\n",
      "[55]\tvalid_0's binary_logloss: 0.152063\n",
      "[56]\tvalid_0's binary_logloss: 0.152034\n",
      "[57]\tvalid_0's binary_logloss: 0.152014\n",
      "[58]\tvalid_0's binary_logloss: 0.151984\n",
      "[59]\tvalid_0's binary_logloss: 0.151949\n",
      "[60]\tvalid_0's binary_logloss: 0.151931\n",
      "[61]\tvalid_0's binary_logloss: 0.151901\n",
      "[62]\tvalid_0's binary_logloss: 0.151893\n",
      "[63]\tvalid_0's binary_logloss: 0.151886\n",
      "[64]\tvalid_0's binary_logloss: 0.151858\n",
      "[65]\tvalid_0's binary_logloss: 0.151849\n",
      "[66]\tvalid_0's binary_logloss: 0.151833\n",
      "[67]\tvalid_0's binary_logloss: 0.15182\n",
      "[68]\tvalid_0's binary_logloss: 0.151809\n",
      "[69]\tvalid_0's binary_logloss: 0.151813\n",
      "[70]\tvalid_0's binary_logloss: 0.151799\n",
      "[71]\tvalid_0's binary_logloss: 0.151772\n",
      "[72]\tvalid_0's binary_logloss: 0.151771\n",
      "[73]\tvalid_0's binary_logloss: 0.151769\n",
      "[74]\tvalid_0's binary_logloss: 0.151768\n",
      "[75]\tvalid_0's binary_logloss: 0.151757\n",
      "[76]\tvalid_0's binary_logloss: 0.151759\n",
      "[77]\tvalid_0's binary_logloss: 0.151756\n",
      "[78]\tvalid_0's binary_logloss: 0.151769\n",
      "[79]\tvalid_0's binary_logloss: 0.15176\n",
      "[80]\tvalid_0's binary_logloss: 0.151757\n",
      "[81]\tvalid_0's binary_logloss: 0.151761\n",
      "[82]\tvalid_0's binary_logloss: 0.151767\n",
      "Early stopping, best iteration is:\n",
      "[77]\tvalid_0's binary_logloss: 0.151756\n",
      "77\n",
      "[1]\tvalid_0's binary_logloss: 0.611396\n",
      "Training until validation scores don't improve for 5 rounds.\n",
      "[2]\tvalid_0's binary_logloss: 0.544539\n",
      "[3]\tvalid_0's binary_logloss: 0.489003\n",
      "[4]\tvalid_0's binary_logloss: 0.442344\n",
      "[5]\tvalid_0's binary_logloss: 0.402789\n",
      "[6]\tvalid_0's binary_logloss: 0.369023\n",
      "[7]\tvalid_0's binary_logloss: 0.340046\n",
      "[8]\tvalid_0's binary_logloss: 0.315076\n",
      "[9]\tvalid_0's binary_logloss: 0.293493\n",
      "[10]\tvalid_0's binary_logloss: 0.274787\n",
      "[11]\tvalid_0's binary_logloss: 0.258545\n",
      "[12]\tvalid_0's binary_logloss: 0.244425\n",
      "[13]\tvalid_0's binary_logloss: 0.232144\n",
      "[14]\tvalid_0's binary_logloss: 0.221442\n",
      "[15]\tvalid_0's binary_logloss: 0.212132\n",
      "[16]\tvalid_0's binary_logloss: 0.204022\n",
      "[17]\tvalid_0's binary_logloss: 0.196968\n",
      "[18]\tvalid_0's binary_logloss: 0.190833\n",
      "[19]\tvalid_0's binary_logloss: 0.185521\n",
      "[20]\tvalid_0's binary_logloss: 0.180896\n",
      "[21]\tvalid_0's binary_logloss: 0.176886\n",
      "[22]\tvalid_0's binary_logloss: 0.173421\n",
      "[23]\tvalid_0's binary_logloss: 0.170424\n",
      "[24]\tvalid_0's binary_logloss: 0.167825\n",
      "[25]\tvalid_0's binary_logloss: 0.165588\n",
      "[26]\tvalid_0's binary_logloss: 0.16368\n",
      "[27]\tvalid_0's binary_logloss: 0.162024\n",
      "[28]\tvalid_0's binary_logloss: 0.160613\n",
      "[29]\tvalid_0's binary_logloss: 0.159401\n",
      "[30]\tvalid_0's binary_logloss: 0.158378\n",
      "[31]\tvalid_0's binary_logloss: 0.157512\n",
      "[32]\tvalid_0's binary_logloss: 0.156734\n",
      "[33]\tvalid_0's binary_logloss: 0.156079\n",
      "[34]\tvalid_0's binary_logloss: 0.155549\n",
      "[35]\tvalid_0's binary_logloss: 0.155101\n",
      "[36]\tvalid_0's binary_logloss: 0.154687\n",
      "[37]\tvalid_0's binary_logloss: 0.154351\n",
      "[38]\tvalid_0's binary_logloss: 0.154057\n",
      "[39]\tvalid_0's binary_logloss: 0.153808\n",
      "[40]\tvalid_0's binary_logloss: 0.153626\n",
      "[41]\tvalid_0's binary_logloss: 0.153445\n",
      "[42]\tvalid_0's binary_logloss: 0.153304\n",
      "[43]\tvalid_0's binary_logloss: 0.153186\n",
      "[44]\tvalid_0's binary_logloss: 0.153081\n",
      "[45]\tvalid_0's binary_logloss: 0.152986\n",
      "[46]\tvalid_0's binary_logloss: 0.152886\n",
      "[47]\tvalid_0's binary_logloss: 0.152798\n",
      "[48]\tvalid_0's binary_logloss: 0.152725\n",
      "[49]\tvalid_0's binary_logloss: 0.15266\n",
      "[50]\tvalid_0's binary_logloss: 0.152611\n",
      "[51]\tvalid_0's binary_logloss: 0.152577\n",
      "[52]\tvalid_0's binary_logloss: 0.152561\n",
      "[53]\tvalid_0's binary_logloss: 0.152528\n",
      "[54]\tvalid_0's binary_logloss: 0.152492\n",
      "[55]\tvalid_0's binary_logloss: 0.152478\n",
      "[56]\tvalid_0's binary_logloss: 0.152444\n",
      "[57]\tvalid_0's binary_logloss: 0.152434\n",
      "[58]\tvalid_0's binary_logloss: 0.152417\n",
      "[59]\tvalid_0's binary_logloss: 0.152394\n",
      "[60]\tvalid_0's binary_logloss: 0.152385\n",
      "[61]\tvalid_0's binary_logloss: 0.152379\n",
      "[62]\tvalid_0's binary_logloss: 0.152373\n",
      "[63]\tvalid_0's binary_logloss: 0.152349\n",
      "[64]\tvalid_0's binary_logloss: 0.15234\n",
      "[65]\tvalid_0's binary_logloss: 0.152352\n",
      "[66]\tvalid_0's binary_logloss: 0.152342\n",
      "[67]\tvalid_0's binary_logloss: 0.152324\n",
      "[68]\tvalid_0's binary_logloss: 0.152321\n",
      "[69]\tvalid_0's binary_logloss: 0.152327\n",
      "[70]\tvalid_0's binary_logloss: 0.152329\n",
      "[71]\tvalid_0's binary_logloss: 0.15232\n",
      "[72]\tvalid_0's binary_logloss: 0.152314\n",
      "[73]\tvalid_0's binary_logloss: 0.152297\n",
      "[74]\tvalid_0's binary_logloss: 0.152295\n",
      "[75]\tvalid_0's binary_logloss: 0.152294\n",
      "[76]\tvalid_0's binary_logloss: 0.152304\n",
      "[77]\tvalid_0's binary_logloss: 0.15231\n",
      "[78]\tvalid_0's binary_logloss: 0.152309\n",
      "[79]\tvalid_0's binary_logloss: 0.152296\n",
      "[80]\tvalid_0's binary_logloss: 0.152295\n",
      "Early stopping, best iteration is:\n",
      "[75]\tvalid_0's binary_logloss: 0.152294\n",
      "75\n",
      "[1]\tvalid_0's binary_logloss: 0.611362\n",
      "Training until validation scores don't improve for 5 rounds.\n",
      "[2]\tvalid_0's binary_logloss: 0.544482\n",
      "[3]\tvalid_0's binary_logloss: 0.488944\n",
      "[4]\tvalid_0's binary_logloss: 0.442276\n",
      "[5]\tvalid_0's binary_logloss: 0.402711\n",
      "[6]\tvalid_0's binary_logloss: 0.368938\n",
      "[7]\tvalid_0's binary_logloss: 0.339962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8]\tvalid_0's binary_logloss: 0.31499\n",
      "[9]\tvalid_0's binary_logloss: 0.293408\n",
      "[10]\tvalid_0's binary_logloss: 0.274701\n",
      "[11]\tvalid_0's binary_logloss: 0.258467\n",
      "[12]\tvalid_0's binary_logloss: 0.244356\n",
      "[13]\tvalid_0's binary_logloss: 0.232079\n",
      "[14]\tvalid_0's binary_logloss: 0.221392\n",
      "[15]\tvalid_0's binary_logloss: 0.21209\n",
      "[16]\tvalid_0's binary_logloss: 0.203992\n",
      "[17]\tvalid_0's binary_logloss: 0.196951\n",
      "[18]\tvalid_0's binary_logloss: 0.190828\n",
      "[19]\tvalid_0's binary_logloss: 0.185517\n",
      "[20]\tvalid_0's binary_logloss: 0.180907\n",
      "[21]\tvalid_0's binary_logloss: 0.176919\n",
      "[22]\tvalid_0's binary_logloss: 0.173461\n",
      "[23]\tvalid_0's binary_logloss: 0.170481\n",
      "[24]\tvalid_0's binary_logloss: 0.167912\n",
      "[25]\tvalid_0's binary_logloss: 0.165675\n",
      "[26]\tvalid_0's binary_logloss: 0.163781\n",
      "[27]\tvalid_0's binary_logloss: 0.162153\n",
      "[28]\tvalid_0's binary_logloss: 0.160728\n",
      "[29]\tvalid_0's binary_logloss: 0.159521\n",
      "[30]\tvalid_0's binary_logloss: 0.158474\n",
      "[31]\tvalid_0's binary_logloss: 0.157598\n",
      "[32]\tvalid_0's binary_logloss: 0.156812\n",
      "[33]\tvalid_0's binary_logloss: 0.156188\n",
      "[34]\tvalid_0's binary_logloss: 0.155629\n",
      "[35]\tvalid_0's binary_logloss: 0.155146\n",
      "[36]\tvalid_0's binary_logloss: 0.154739\n",
      "[37]\tvalid_0's binary_logloss: 0.154383\n",
      "[38]\tvalid_0's binary_logloss: 0.154094\n",
      "[39]\tvalid_0's binary_logloss: 0.153839\n",
      "[40]\tvalid_0's binary_logloss: 0.153647\n",
      "[41]\tvalid_0's binary_logloss: 0.153455\n",
      "[42]\tvalid_0's binary_logloss: 0.153286\n",
      "[43]\tvalid_0's binary_logloss: 0.153149\n",
      "[44]\tvalid_0's binary_logloss: 0.153015\n",
      "[45]\tvalid_0's binary_logloss: 0.152913\n",
      "[46]\tvalid_0's binary_logloss: 0.152822\n",
      "[47]\tvalid_0's binary_logloss: 0.152753\n",
      "[48]\tvalid_0's binary_logloss: 0.152693\n",
      "[49]\tvalid_0's binary_logloss: 0.152634\n",
      "[50]\tvalid_0's binary_logloss: 0.152574\n",
      "[51]\tvalid_0's binary_logloss: 0.152511\n",
      "[52]\tvalid_0's binary_logloss: 0.152467\n",
      "[53]\tvalid_0's binary_logloss: 0.15244\n",
      "[54]\tvalid_0's binary_logloss: 0.152383\n",
      "[55]\tvalid_0's binary_logloss: 0.15235\n",
      "[56]\tvalid_0's binary_logloss: 0.152318\n",
      "[57]\tvalid_0's binary_logloss: 0.152296\n",
      "[58]\tvalid_0's binary_logloss: 0.152281\n",
      "[59]\tvalid_0's binary_logloss: 0.152284\n",
      "[60]\tvalid_0's binary_logloss: 0.15227\n",
      "[61]\tvalid_0's binary_logloss: 0.152254\n",
      "[62]\tvalid_0's binary_logloss: 0.15223\n",
      "[63]\tvalid_0's binary_logloss: 0.15221\n",
      "[64]\tvalid_0's binary_logloss: 0.152197\n",
      "[65]\tvalid_0's binary_logloss: 0.152182\n",
      "[66]\tvalid_0's binary_logloss: 0.152169\n",
      "[67]\tvalid_0's binary_logloss: 0.152158\n",
      "[68]\tvalid_0's binary_logloss: 0.152147\n",
      "[69]\tvalid_0's binary_logloss: 0.152145\n",
      "[70]\tvalid_0's binary_logloss: 0.152151\n",
      "[71]\tvalid_0's binary_logloss: 0.15215\n",
      "[72]\tvalid_0's binary_logloss: 0.15215\n",
      "[73]\tvalid_0's binary_logloss: 0.152146\n",
      "[74]\tvalid_0's binary_logloss: 0.152149\n",
      "Early stopping, best iteration is:\n",
      "[69]\tvalid_0's binary_logloss: 0.152145\n",
      "69\n",
      "[1]\tvalid_0's binary_logloss: 0.61141\n",
      "Training until validation scores don't improve for 5 rounds.\n",
      "[2]\tvalid_0's binary_logloss: 0.544567\n",
      "[3]\tvalid_0's binary_logloss: 0.489039\n",
      "[4]\tvalid_0's binary_logloss: 0.442383\n",
      "[5]\tvalid_0's binary_logloss: 0.402822\n",
      "[6]\tvalid_0's binary_logloss: 0.369062\n",
      "[7]\tvalid_0's binary_logloss: 0.340084\n",
      "[8]\tvalid_0's binary_logloss: 0.315123\n",
      "[9]\tvalid_0's binary_logloss: 0.293541\n",
      "[10]\tvalid_0's binary_logloss: 0.274833\n",
      "[11]\tvalid_0's binary_logloss: 0.258564\n",
      "[12]\tvalid_0's binary_logloss: 0.244411\n",
      "[13]\tvalid_0's binary_logloss: 0.232107\n",
      "[14]\tvalid_0's binary_logloss: 0.221404\n",
      "[15]\tvalid_0's binary_logloss: 0.212072\n",
      "[16]\tvalid_0's binary_logloss: 0.203948\n",
      "[17]\tvalid_0's binary_logloss: 0.196877\n",
      "[18]\tvalid_0's binary_logloss: 0.190733\n",
      "[19]\tvalid_0's binary_logloss: 0.185412\n",
      "[20]\tvalid_0's binary_logloss: 0.180785\n",
      "[21]\tvalid_0's binary_logloss: 0.176798\n",
      "[22]\tvalid_0's binary_logloss: 0.173324\n",
      "[23]\tvalid_0's binary_logloss: 0.170332\n",
      "[24]\tvalid_0's binary_logloss: 0.16773\n",
      "[25]\tvalid_0's binary_logloss: 0.165501\n",
      "[26]\tvalid_0's binary_logloss: 0.163576\n",
      "[27]\tvalid_0's binary_logloss: 0.161937\n",
      "[28]\tvalid_0's binary_logloss: 0.160516\n",
      "[29]\tvalid_0's binary_logloss: 0.159299\n",
      "[30]\tvalid_0's binary_logloss: 0.158261\n",
      "[31]\tvalid_0's binary_logloss: 0.157373\n",
      "[32]\tvalid_0's binary_logloss: 0.156606\n",
      "[33]\tvalid_0's binary_logloss: 0.15595\n",
      "[34]\tvalid_0's binary_logloss: 0.155379\n",
      "[35]\tvalid_0's binary_logloss: 0.15489\n",
      "[36]\tvalid_0's binary_logloss: 0.154473\n",
      "[37]\tvalid_0's binary_logloss: 0.154131\n",
      "[38]\tvalid_0's binary_logloss: 0.153834\n",
      "[39]\tvalid_0's binary_logloss: 0.15358\n",
      "[40]\tvalid_0's binary_logloss: 0.153365\n",
      "[41]\tvalid_0's binary_logloss: 0.153172\n",
      "[42]\tvalid_0's binary_logloss: 0.153004\n",
      "[43]\tvalid_0's binary_logloss: 0.152863\n",
      "[44]\tvalid_0's binary_logloss: 0.15274\n",
      "[45]\tvalid_0's binary_logloss: 0.152649\n",
      "[46]\tvalid_0's binary_logloss: 0.152553\n",
      "[47]\tvalid_0's binary_logloss: 0.152443\n",
      "[48]\tvalid_0's binary_logloss: 0.152368\n",
      "[49]\tvalid_0's binary_logloss: 0.152293\n",
      "[50]\tvalid_0's binary_logloss: 0.152228\n",
      "[51]\tvalid_0's binary_logloss: 0.152182\n",
      "[52]\tvalid_0's binary_logloss: 0.152142\n",
      "[53]\tvalid_0's binary_logloss: 0.152087\n",
      "[54]\tvalid_0's binary_logloss: 0.152064\n",
      "[55]\tvalid_0's binary_logloss: 0.152\n",
      "[56]\tvalid_0's binary_logloss: 0.151971\n",
      "[57]\tvalid_0's binary_logloss: 0.151961\n",
      "[58]\tvalid_0's binary_logloss: 0.151932\n",
      "[59]\tvalid_0's binary_logloss: 0.15192\n",
      "[60]\tvalid_0's binary_logloss: 0.151904\n",
      "[61]\tvalid_0's binary_logloss: 0.151893\n",
      "[62]\tvalid_0's binary_logloss: 0.151879\n",
      "[63]\tvalid_0's binary_logloss: 0.15186\n",
      "[64]\tvalid_0's binary_logloss: 0.15184\n",
      "[65]\tvalid_0's binary_logloss: 0.151832\n",
      "[66]\tvalid_0's binary_logloss: 0.151826\n",
      "[67]\tvalid_0's binary_logloss: 0.151789\n",
      "[68]\tvalid_0's binary_logloss: 0.151771\n",
      "[69]\tvalid_0's binary_logloss: 0.151761\n",
      "[70]\tvalid_0's binary_logloss: 0.151755\n",
      "[71]\tvalid_0's binary_logloss: 0.151734\n",
      "[72]\tvalid_0's binary_logloss: 0.151721\n",
      "[73]\tvalid_0's binary_logloss: 0.15172\n",
      "[74]\tvalid_0's binary_logloss: 0.151716\n",
      "[75]\tvalid_0's binary_logloss: 0.151711\n",
      "[76]\tvalid_0's binary_logloss: 0.151711\n",
      "[77]\tvalid_0's binary_logloss: 0.151716\n",
      "[78]\tvalid_0's binary_logloss: 0.151707\n",
      "[79]\tvalid_0's binary_logloss: 0.151693\n",
      "[80]\tvalid_0's binary_logloss: 0.151704\n",
      "[81]\tvalid_0's binary_logloss: 0.151706\n",
      "[82]\tvalid_0's binary_logloss: 0.1517\n",
      "[83]\tvalid_0's binary_logloss: 0.151692\n",
      "[84]\tvalid_0's binary_logloss: 0.151688\n",
      "[85]\tvalid_0's binary_logloss: 0.151692\n",
      "[86]\tvalid_0's binary_logloss: 0.151702\n",
      "[87]\tvalid_0's binary_logloss: 0.151703\n",
      "[88]\tvalid_0's binary_logloss: 0.151707\n",
      "[89]\tvalid_0's binary_logloss: 0.151715\n",
      "Early stopping, best iteration is:\n",
      "[84]\tvalid_0's binary_logloss: 0.151688\n",
      "84\n",
      "[1]\tvalid_0's binary_logloss: 0.611344\n",
      "Training until validation scores don't improve for 5 rounds.\n",
      "[2]\tvalid_0's binary_logloss: 0.544451\n",
      "[3]\tvalid_0's binary_logloss: 0.488887\n",
      "[4]\tvalid_0's binary_logloss: 0.442207\n",
      "[5]\tvalid_0's binary_logloss: 0.402633\n",
      "[6]\tvalid_0's binary_logloss: 0.368854\n",
      "[7]\tvalid_0's binary_logloss: 0.339869\n",
      "[8]\tvalid_0's binary_logloss: 0.314889\n",
      "[9]\tvalid_0's binary_logloss: 0.293295\n",
      "[10]\tvalid_0's binary_logloss: 0.274581\n",
      "[11]\tvalid_0's binary_logloss: 0.258366\n",
      "[12]\tvalid_0's binary_logloss: 0.244271\n",
      "[13]\tvalid_0's binary_logloss: 0.232009\n",
      "[14]\tvalid_0's binary_logloss: 0.221325\n",
      "[15]\tvalid_0's binary_logloss: 0.212038\n",
      "[16]\tvalid_0's binary_logloss: 0.203947\n",
      "[17]\tvalid_0's binary_logloss: 0.196912\n",
      "[18]\tvalid_0's binary_logloss: 0.190787\n",
      "[19]\tvalid_0's binary_logloss: 0.18548\n",
      "[20]\tvalid_0's binary_logloss: 0.18088\n",
      "[21]\tvalid_0's binary_logloss: 0.176875\n",
      "[22]\tvalid_0's binary_logloss: 0.17341\n",
      "[23]\tvalid_0's binary_logloss: 0.170418\n",
      "[24]\tvalid_0's binary_logloss: 0.167838\n",
      "[25]\tvalid_0's binary_logloss: 0.16562\n",
      "[26]\tvalid_0's binary_logloss: 0.163727\n",
      "[27]\tvalid_0's binary_logloss: 0.162097\n",
      "[28]\tvalid_0's binary_logloss: 0.160666\n",
      "[29]\tvalid_0's binary_logloss: 0.15945\n",
      "[30]\tvalid_0's binary_logloss: 0.158405\n",
      "[31]\tvalid_0's binary_logloss: 0.157524\n",
      "[32]\tvalid_0's binary_logloss: 0.15677\n",
      "[33]\tvalid_0's binary_logloss: 0.156116\n",
      "[34]\tvalid_0's binary_logloss: 0.15556\n",
      "[35]\tvalid_0's binary_logloss: 0.155082\n",
      "[36]\tvalid_0's binary_logloss: 0.15469\n",
      "[37]\tvalid_0's binary_logloss: 0.154353\n",
      "[38]\tvalid_0's binary_logloss: 0.154067\n",
      "[39]\tvalid_0's binary_logloss: 0.153807\n",
      "[40]\tvalid_0's binary_logloss: 0.153599\n",
      "[41]\tvalid_0's binary_logloss: 0.153413\n",
      "[42]\tvalid_0's binary_logloss: 0.153248\n",
      "[43]\tvalid_0's binary_logloss: 0.153121\n",
      "[44]\tvalid_0's binary_logloss: 0.153009\n",
      "[45]\tvalid_0's binary_logloss: 0.152905\n",
      "[46]\tvalid_0's binary_logloss: 0.152813\n",
      "[47]\tvalid_0's binary_logloss: 0.152728\n",
      "[48]\tvalid_0's binary_logloss: 0.15266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[49]\tvalid_0's binary_logloss: 0.152584\n",
      "[50]\tvalid_0's binary_logloss: 0.152536\n",
      "[51]\tvalid_0's binary_logloss: 0.152491\n",
      "[52]\tvalid_0's binary_logloss: 0.152459\n",
      "[53]\tvalid_0's binary_logloss: 0.152415\n",
      "[54]\tvalid_0's binary_logloss: 0.152379\n",
      "[55]\tvalid_0's binary_logloss: 0.152341\n",
      "[56]\tvalid_0's binary_logloss: 0.152303\n",
      "[57]\tvalid_0's binary_logloss: 0.152284\n",
      "[58]\tvalid_0's binary_logloss: 0.152257\n",
      "[59]\tvalid_0's binary_logloss: 0.152239\n",
      "[60]\tvalid_0's binary_logloss: 0.152211\n",
      "[61]\tvalid_0's binary_logloss: 0.152194\n",
      "[62]\tvalid_0's binary_logloss: 0.152155\n",
      "[63]\tvalid_0's binary_logloss: 0.152128\n",
      "[64]\tvalid_0's binary_logloss: 0.152098\n",
      "[65]\tvalid_0's binary_logloss: 0.152073\n",
      "[66]\tvalid_0's binary_logloss: 0.152046\n",
      "[67]\tvalid_0's binary_logloss: 0.152046\n",
      "[68]\tvalid_0's binary_logloss: 0.152042\n",
      "[69]\tvalid_0's binary_logloss: 0.152049\n",
      "[70]\tvalid_0's binary_logloss: 0.152029\n",
      "[71]\tvalid_0's binary_logloss: 0.152021\n",
      "[72]\tvalid_0's binary_logloss: 0.151999\n",
      "[73]\tvalid_0's binary_logloss: 0.151995\n",
      "[74]\tvalid_0's binary_logloss: 0.151998\n",
      "[75]\tvalid_0's binary_logloss: 0.151988\n",
      "[76]\tvalid_0's binary_logloss: 0.151985\n",
      "[77]\tvalid_0's binary_logloss: 0.151988\n",
      "[78]\tvalid_0's binary_logloss: 0.151978\n",
      "[79]\tvalid_0's binary_logloss: 0.15196\n",
      "[80]\tvalid_0's binary_logloss: 0.151959\n",
      "[81]\tvalid_0's binary_logloss: 0.151936\n",
      "[82]\tvalid_0's binary_logloss: 0.151933\n",
      "[83]\tvalid_0's binary_logloss: 0.151942\n",
      "[84]\tvalid_0's binary_logloss: 0.151943\n",
      "[85]\tvalid_0's binary_logloss: 0.151929\n",
      "[86]\tvalid_0's binary_logloss: 0.151929\n",
      "[87]\tvalid_0's binary_logloss: 0.15193\n",
      "[88]\tvalid_0's binary_logloss: 0.151933\n",
      "[89]\tvalid_0's binary_logloss: 0.151938\n",
      "[90]\tvalid_0's binary_logloss: 0.151922\n",
      "[91]\tvalid_0's binary_logloss: 0.151923\n",
      "[92]\tvalid_0's binary_logloss: 0.15193\n",
      "[93]\tvalid_0's binary_logloss: 0.151937\n",
      "[94]\tvalid_0's binary_logloss: 0.15194\n",
      "[95]\tvalid_0's binary_logloss: 0.151936\n",
      "Early stopping, best iteration is:\n",
      "[90]\tvalid_0's binary_logloss: 0.151922\n",
      "90\n",
      "[1]\tvalid_0's binary_logloss: 0.667723\n",
      "Training until validation scores don't improve for 5 rounds.\n",
      "[2]\tvalid_0's binary_logloss: 0.643791\n",
      "[3]\tvalid_0's binary_logloss: 0.621215\n",
      "[4]\tvalid_0's binary_logloss: 0.599887\n",
      "[5]\tvalid_0's binary_logloss: 0.579723\n",
      "[6]\tvalid_0's binary_logloss: 0.560631\n",
      "[7]\tvalid_0's binary_logloss: 0.542537\n",
      "[8]\tvalid_0's binary_logloss: 0.525376\n",
      "[9]\tvalid_0's binary_logloss: 0.509085\n",
      "[10]\tvalid_0's binary_logloss: 0.493604\n",
      "[11]\tvalid_0's binary_logloss: 0.478875\n",
      "[12]\tvalid_0's binary_logloss: 0.464857\n",
      "[13]\tvalid_0's binary_logloss: 0.45151\n",
      "[14]\tvalid_0's binary_logloss: 0.438787\n",
      "[15]\tvalid_0's binary_logloss: 0.426661\n",
      "[16]\tvalid_0's binary_logloss: 0.415088\n",
      "[17]\tvalid_0's binary_logloss: 0.404046\n",
      "[18]\tvalid_0's binary_logloss: 0.393497\n",
      "[19]\tvalid_0's binary_logloss: 0.383423\n",
      "[20]\tvalid_0's binary_logloss: 0.373795\n",
      "[21]\tvalid_0's binary_logloss: 0.364591\n",
      "[22]\tvalid_0's binary_logloss: 0.355786\n",
      "[23]\tvalid_0's binary_logloss: 0.347364\n",
      "[24]\tvalid_0's binary_logloss: 0.339299\n",
      "[25]\tvalid_0's binary_logloss: 0.33157\n",
      "[26]\tvalid_0's binary_logloss: 0.324172\n",
      "[27]\tvalid_0's binary_logloss: 0.317084\n",
      "[28]\tvalid_0's binary_logloss: 0.310285\n",
      "[29]\tvalid_0's binary_logloss: 0.303769\n",
      "[30]\tvalid_0's binary_logloss: 0.297521\n",
      "[31]\tvalid_0's binary_logloss: 0.291544\n",
      "[32]\tvalid_0's binary_logloss: 0.285804\n",
      "[33]\tvalid_0's binary_logloss: 0.280302\n",
      "[34]\tvalid_0's binary_logloss: 0.275033\n",
      "[35]\tvalid_0's binary_logloss: 0.269978\n",
      "[36]\tvalid_0's binary_logloss: 0.265123\n",
      "[37]\tvalid_0's binary_logloss: 0.260463\n",
      "[38]\tvalid_0's binary_logloss: 0.255992\n",
      "[39]\tvalid_0's binary_logloss: 0.251704\n",
      "[40]\tvalid_0's binary_logloss: 0.247592\n",
      "[41]\tvalid_0's binary_logloss: 0.243646\n",
      "[42]\tvalid_0's binary_logloss: 0.239865\n",
      "[43]\tvalid_0's binary_logloss: 0.236225\n",
      "[44]\tvalid_0's binary_logloss: 0.232733\n",
      "[45]\tvalid_0's binary_logloss: 0.229382\n",
      "[46]\tvalid_0's binary_logloss: 0.226168\n",
      "[47]\tvalid_0's binary_logloss: 0.223083\n",
      "[48]\tvalid_0's binary_logloss: 0.220123\n",
      "[49]\tvalid_0's binary_logloss: 0.21728\n",
      "[50]\tvalid_0's binary_logloss: 0.214553\n",
      "[51]\tvalid_0's binary_logloss: 0.211934\n",
      "[52]\tvalid_0's binary_logloss: 0.209422\n",
      "[53]\tvalid_0's binary_logloss: 0.207014\n",
      "[54]\tvalid_0's binary_logloss: 0.204704\n",
      "[55]\tvalid_0's binary_logloss: 0.20248\n",
      "[56]\tvalid_0's binary_logloss: 0.200348\n",
      "[57]\tvalid_0's binary_logloss: 0.198301\n",
      "[58]\tvalid_0's binary_logloss: 0.196339\n",
      "[59]\tvalid_0's binary_logloss: 0.194458\n",
      "[60]\tvalid_0's binary_logloss: 0.192652\n",
      "[61]\tvalid_0's binary_logloss: 0.190924\n",
      "[62]\tvalid_0's binary_logloss: 0.189265\n",
      "[63]\tvalid_0's binary_logloss: 0.187668\n",
      "[64]\tvalid_0's binary_logloss: 0.186147\n",
      "[65]\tvalid_0's binary_logloss: 0.184681\n",
      "[66]\tvalid_0's binary_logloss: 0.183273\n",
      "[67]\tvalid_0's binary_logloss: 0.181926\n",
      "[68]\tvalid_0's binary_logloss: 0.180635\n",
      "[69]\tvalid_0's binary_logloss: 0.179401\n",
      "[70]\tvalid_0's binary_logloss: 0.178212\n",
      "[71]\tvalid_0's binary_logloss: 0.177072\n",
      "[72]\tvalid_0's binary_logloss: 0.175985\n",
      "[73]\tvalid_0's binary_logloss: 0.174941\n",
      "[74]\tvalid_0's binary_logloss: 0.173941\n",
      "[75]\tvalid_0's binary_logloss: 0.172978\n",
      "[76]\tvalid_0's binary_logloss: 0.172056\n",
      "[77]\tvalid_0's binary_logloss: 0.171174\n",
      "[78]\tvalid_0's binary_logloss: 0.170328\n",
      "[79]\tvalid_0's binary_logloss: 0.16952\n",
      "[80]\tvalid_0's binary_logloss: 0.168752\n",
      "[81]\tvalid_0's binary_logloss: 0.168017\n",
      "[82]\tvalid_0's binary_logloss: 0.167317\n",
      "[83]\tvalid_0's binary_logloss: 0.166645\n",
      "[84]\tvalid_0's binary_logloss: 0.166003\n",
      "[85]\tvalid_0's binary_logloss: 0.165381\n",
      "[86]\tvalid_0's binary_logloss: 0.164786\n",
      "[87]\tvalid_0's binary_logloss: 0.164223\n",
      "[88]\tvalid_0's binary_logloss: 0.16368\n",
      "[89]\tvalid_0's binary_logloss: 0.163161\n",
      "[90]\tvalid_0's binary_logloss: 0.162662\n",
      "[91]\tvalid_0's binary_logloss: 0.162186\n",
      "[92]\tvalid_0's binary_logloss: 0.161736\n",
      "[93]\tvalid_0's binary_logloss: 0.161298\n",
      "[94]\tvalid_0's binary_logloss: 0.160884\n",
      "[95]\tvalid_0's binary_logloss: 0.160483\n",
      "[96]\tvalid_0's binary_logloss: 0.160103\n",
      "[97]\tvalid_0's binary_logloss: 0.159736\n",
      "[98]\tvalid_0's binary_logloss: 0.159389\n",
      "[99]\tvalid_0's binary_logloss: 0.159059\n",
      "[100]\tvalid_0's binary_logloss: 0.158749\n",
      "[101]\tvalid_0's binary_logloss: 0.15845\n",
      "[102]\tvalid_0's binary_logloss: 0.158159\n",
      "[103]\tvalid_0's binary_logloss: 0.157885\n",
      "[104]\tvalid_0's binary_logloss: 0.157629\n",
      "[105]\tvalid_0's binary_logloss: 0.157378\n",
      "[106]\tvalid_0's binary_logloss: 0.157134\n",
      "[107]\tvalid_0's binary_logloss: 0.156907\n",
      "[108]\tvalid_0's binary_logloss: 0.156685\n",
      "[109]\tvalid_0's binary_logloss: 0.156475\n",
      "[110]\tvalid_0's binary_logloss: 0.156272\n",
      "[111]\tvalid_0's binary_logloss: 0.156078\n",
      "[112]\tvalid_0's binary_logloss: 0.155899\n",
      "[113]\tvalid_0's binary_logloss: 0.155724\n",
      "[114]\tvalid_0's binary_logloss: 0.155556\n",
      "[115]\tvalid_0's binary_logloss: 0.155398\n",
      "[116]\tvalid_0's binary_logloss: 0.155245\n",
      "[117]\tvalid_0's binary_logloss: 0.155105\n",
      "[118]\tvalid_0's binary_logloss: 0.154966\n",
      "[119]\tvalid_0's binary_logloss: 0.154835\n",
      "[120]\tvalid_0's binary_logloss: 0.154709\n",
      "[121]\tvalid_0's binary_logloss: 0.154587\n",
      "[122]\tvalid_0's binary_logloss: 0.154476\n",
      "[123]\tvalid_0's binary_logloss: 0.15437\n",
      "[124]\tvalid_0's binary_logloss: 0.154265\n",
      "[125]\tvalid_0's binary_logloss: 0.154169\n",
      "[126]\tvalid_0's binary_logloss: 0.15407\n",
      "[127]\tvalid_0's binary_logloss: 0.15398\n",
      "[128]\tvalid_0's binary_logloss: 0.153894\n",
      "[129]\tvalid_0's binary_logloss: 0.153814\n",
      "[130]\tvalid_0's binary_logloss: 0.153733\n",
      "[131]\tvalid_0's binary_logloss: 0.15365\n",
      "[132]\tvalid_0's binary_logloss: 0.153575\n",
      "[133]\tvalid_0's binary_logloss: 0.153502\n",
      "[134]\tvalid_0's binary_logloss: 0.153437\n",
      "[135]\tvalid_0's binary_logloss: 0.153379\n",
      "[136]\tvalid_0's binary_logloss: 0.153315\n",
      "[137]\tvalid_0's binary_logloss: 0.153259\n",
      "[138]\tvalid_0's binary_logloss: 0.153201\n",
      "[139]\tvalid_0's binary_logloss: 0.153151\n",
      "[140]\tvalid_0's binary_logloss: 0.153101\n",
      "[141]\tvalid_0's binary_logloss: 0.153051\n",
      "[142]\tvalid_0's binary_logloss: 0.153006\n",
      "[143]\tvalid_0's binary_logloss: 0.152958\n",
      "[144]\tvalid_0's binary_logloss: 0.152917\n",
      "[145]\tvalid_0's binary_logloss: 0.152869\n",
      "[146]\tvalid_0's binary_logloss: 0.152829\n",
      "[147]\tvalid_0's binary_logloss: 0.15279\n",
      "[148]\tvalid_0's binary_logloss: 0.15275\n",
      "[149]\tvalid_0's binary_logloss: 0.152716\n",
      "[150]\tvalid_0's binary_logloss: 0.152684\n",
      "[151]\tvalid_0's binary_logloss: 0.152647\n",
      "[152]\tvalid_0's binary_logloss: 0.152615\n",
      "[153]\tvalid_0's binary_logloss: 0.152587\n",
      "[154]\tvalid_0's binary_logloss: 0.152563\n",
      "[155]\tvalid_0's binary_logloss: 0.152541\n",
      "[156]\tvalid_0's binary_logloss: 0.152511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[157]\tvalid_0's binary_logloss: 0.15248\n",
      "[158]\tvalid_0's binary_logloss: 0.15245\n",
      "[159]\tvalid_0's binary_logloss: 0.152425\n",
      "[160]\tvalid_0's binary_logloss: 0.152406\n",
      "[161]\tvalid_0's binary_logloss: 0.152384\n",
      "[162]\tvalid_0's binary_logloss: 0.152367\n",
      "[163]\tvalid_0's binary_logloss: 0.152345\n",
      "[164]\tvalid_0's binary_logloss: 0.152322\n",
      "[165]\tvalid_0's binary_logloss: 0.152305\n",
      "[166]\tvalid_0's binary_logloss: 0.152286\n",
      "[167]\tvalid_0's binary_logloss: 0.152265\n",
      "[168]\tvalid_0's binary_logloss: 0.152248\n",
      "[169]\tvalid_0's binary_logloss: 0.152232\n",
      "[170]\tvalid_0's binary_logloss: 0.152217\n",
      "[171]\tvalid_0's binary_logloss: 0.152203\n",
      "[172]\tvalid_0's binary_logloss: 0.152191\n",
      "[173]\tvalid_0's binary_logloss: 0.152174\n",
      "[174]\tvalid_0's binary_logloss: 0.152159\n",
      "[175]\tvalid_0's binary_logloss: 0.152144\n",
      "[176]\tvalid_0's binary_logloss: 0.152129\n",
      "[177]\tvalid_0's binary_logloss: 0.152115\n",
      "[178]\tvalid_0's binary_logloss: 0.152103\n",
      "[179]\tvalid_0's binary_logloss: 0.152086\n",
      "[180]\tvalid_0's binary_logloss: 0.15208\n",
      "[181]\tvalid_0's binary_logloss: 0.15207\n",
      "[182]\tvalid_0's binary_logloss: 0.152057\n",
      "[183]\tvalid_0's binary_logloss: 0.152045\n",
      "[184]\tvalid_0's binary_logloss: 0.152031\n",
      "[185]\tvalid_0's binary_logloss: 0.152019\n",
      "[186]\tvalid_0's binary_logloss: 0.152004\n",
      "[187]\tvalid_0's binary_logloss: 0.151993\n",
      "[188]\tvalid_0's binary_logloss: 0.151977\n",
      "[189]\tvalid_0's binary_logloss: 0.151968\n",
      "[190]\tvalid_0's binary_logloss: 0.151963\n",
      "[191]\tvalid_0's binary_logloss: 0.151958\n",
      "[192]\tvalid_0's binary_logloss: 0.151948\n",
      "[193]\tvalid_0's binary_logloss: 0.151938\n",
      "[194]\tvalid_0's binary_logloss: 0.151927\n",
      "[195]\tvalid_0's binary_logloss: 0.151923\n",
      "[196]\tvalid_0's binary_logloss: 0.151914\n",
      "[197]\tvalid_0's binary_logloss: 0.151906\n",
      "[198]\tvalid_0's binary_logloss: 0.151897\n",
      "[199]\tvalid_0's binary_logloss: 0.151891\n",
      "[200]\tvalid_0's binary_logloss: 0.151886\n",
      "[201]\tvalid_0's binary_logloss: 0.151877\n",
      "[202]\tvalid_0's binary_logloss: 0.151866\n",
      "[203]\tvalid_0's binary_logloss: 0.151863\n",
      "[204]\tvalid_0's binary_logloss: 0.151853\n",
      "[205]\tvalid_0's binary_logloss: 0.151849\n",
      "[206]\tvalid_0's binary_logloss: 0.151846\n",
      "[207]\tvalid_0's binary_logloss: 0.151844\n",
      "[208]\tvalid_0's binary_logloss: 0.151841\n",
      "[209]\tvalid_0's binary_logloss: 0.151835\n",
      "[210]\tvalid_0's binary_logloss: 0.151832\n",
      "[211]\tvalid_0's binary_logloss: 0.151828\n",
      "[212]\tvalid_0's binary_logloss: 0.151824\n",
      "[213]\tvalid_0's binary_logloss: 0.151817\n",
      "[214]\tvalid_0's binary_logloss: 0.151809\n",
      "[215]\tvalid_0's binary_logloss: 0.151804\n",
      "[216]\tvalid_0's binary_logloss: 0.151801\n",
      "[217]\tvalid_0's binary_logloss: 0.151796\n",
      "[218]\tvalid_0's binary_logloss: 0.151794\n",
      "[219]\tvalid_0's binary_logloss: 0.151789\n",
      "[220]\tvalid_0's binary_logloss: 0.151783\n",
      "[221]\tvalid_0's binary_logloss: 0.151777\n",
      "[222]\tvalid_0's binary_logloss: 0.151772\n",
      "[223]\tvalid_0's binary_logloss: 0.151769\n",
      "[224]\tvalid_0's binary_logloss: 0.151764\n",
      "[225]\tvalid_0's binary_logloss: 0.151762\n",
      "[226]\tvalid_0's binary_logloss: 0.151762\n",
      "[227]\tvalid_0's binary_logloss: 0.151763\n",
      "[228]\tvalid_0's binary_logloss: 0.151758\n",
      "[229]\tvalid_0's binary_logloss: 0.15175\n",
      "[230]\tvalid_0's binary_logloss: 0.151744\n",
      "[231]\tvalid_0's binary_logloss: 0.151741\n",
      "[232]\tvalid_0's binary_logloss: 0.151735\n",
      "[233]\tvalid_0's binary_logloss: 0.15173\n",
      "[234]\tvalid_0's binary_logloss: 0.151725\n",
      "[235]\tvalid_0's binary_logloss: 0.151724\n",
      "[236]\tvalid_0's binary_logloss: 0.151718\n",
      "[237]\tvalid_0's binary_logloss: 0.151716\n",
      "[238]\tvalid_0's binary_logloss: 0.151716\n",
      "[239]\tvalid_0's binary_logloss: 0.15171\n",
      "[240]\tvalid_0's binary_logloss: 0.151708\n",
      "[241]\tvalid_0's binary_logloss: 0.151703\n",
      "[242]\tvalid_0's binary_logloss: 0.151699\n",
      "[243]\tvalid_0's binary_logloss: 0.151693\n",
      "[244]\tvalid_0's binary_logloss: 0.151691\n",
      "[245]\tvalid_0's binary_logloss: 0.151686\n",
      "[246]\tvalid_0's binary_logloss: 0.151677\n",
      "[247]\tvalid_0's binary_logloss: 0.151674\n",
      "[248]\tvalid_0's binary_logloss: 0.151676\n",
      "[249]\tvalid_0's binary_logloss: 0.151671\n",
      "[250]\tvalid_0's binary_logloss: 0.15167\n",
      "[251]\tvalid_0's binary_logloss: 0.151668\n",
      "[252]\tvalid_0's binary_logloss: 0.151667\n",
      "[253]\tvalid_0's binary_logloss: 0.151665\n",
      "[254]\tvalid_0's binary_logloss: 0.151663\n",
      "[255]\tvalid_0's binary_logloss: 0.151661\n",
      "[256]\tvalid_0's binary_logloss: 0.15166\n",
      "[257]\tvalid_0's binary_logloss: 0.151656\n",
      "[258]\tvalid_0's binary_logloss: 0.15165\n",
      "[259]\tvalid_0's binary_logloss: 0.151649\n",
      "[260]\tvalid_0's binary_logloss: 0.151647\n",
      "[261]\tvalid_0's binary_logloss: 0.151644\n",
      "[262]\tvalid_0's binary_logloss: 0.151644\n",
      "[263]\tvalid_0's binary_logloss: 0.151639\n",
      "[264]\tvalid_0's binary_logloss: 0.151634\n",
      "[265]\tvalid_0's binary_logloss: 0.151628\n",
      "[266]\tvalid_0's binary_logloss: 0.151627\n",
      "[267]\tvalid_0's binary_logloss: 0.151626\n",
      "[268]\tvalid_0's binary_logloss: 0.151623\n",
      "[269]\tvalid_0's binary_logloss: 0.151621\n",
      "[270]\tvalid_0's binary_logloss: 0.151619\n",
      "[271]\tvalid_0's binary_logloss: 0.151619\n",
      "[272]\tvalid_0's binary_logloss: 0.15162\n",
      "[273]\tvalid_0's binary_logloss: 0.151618\n",
      "[274]\tvalid_0's binary_logloss: 0.151617\n",
      "[275]\tvalid_0's binary_logloss: 0.151615\n",
      "[276]\tvalid_0's binary_logloss: 0.151616\n",
      "[277]\tvalid_0's binary_logloss: 0.151617\n",
      "[278]\tvalid_0's binary_logloss: 0.151614\n",
      "[279]\tvalid_0's binary_logloss: 0.151612\n",
      "[280]\tvalid_0's binary_logloss: 0.151607\n",
      "[281]\tvalid_0's binary_logloss: 0.151607\n",
      "[282]\tvalid_0's binary_logloss: 0.151605\n",
      "[283]\tvalid_0's binary_logloss: 0.151601\n",
      "[284]\tvalid_0's binary_logloss: 0.151602\n",
      "[285]\tvalid_0's binary_logloss: 0.151601\n",
      "[286]\tvalid_0's binary_logloss: 0.151603\n",
      "[287]\tvalid_0's binary_logloss: 0.151603\n",
      "[288]\tvalid_0's binary_logloss: 0.151598\n",
      "[289]\tvalid_0's binary_logloss: 0.151599\n",
      "[290]\tvalid_0's binary_logloss: 0.151595\n",
      "[291]\tvalid_0's binary_logloss: 0.151593\n",
      "[292]\tvalid_0's binary_logloss: 0.151591\n",
      "[293]\tvalid_0's binary_logloss: 0.151591\n",
      "[294]\tvalid_0's binary_logloss: 0.151588\n",
      "[295]\tvalid_0's binary_logloss: 0.151587\n",
      "[296]\tvalid_0's binary_logloss: 0.151587\n",
      "[297]\tvalid_0's binary_logloss: 0.151588\n",
      "[298]\tvalid_0's binary_logloss: 0.151589\n",
      "[299]\tvalid_0's binary_logloss: 0.151585\n",
      "[300]\tvalid_0's binary_logloss: 0.151586\n",
      "[301]\tvalid_0's binary_logloss: 0.151583\n",
      "[302]\tvalid_0's binary_logloss: 0.151581\n",
      "[303]\tvalid_0's binary_logloss: 0.151578\n",
      "[304]\tvalid_0's binary_logloss: 0.151578\n",
      "[305]\tvalid_0's binary_logloss: 0.151579\n",
      "[306]\tvalid_0's binary_logloss: 0.15158\n",
      "[307]\tvalid_0's binary_logloss: 0.15158\n",
      "[308]\tvalid_0's binary_logloss: 0.151578\n",
      "[309]\tvalid_0's binary_logloss: 0.151574\n",
      "[310]\tvalid_0's binary_logloss: 0.15157\n",
      "[311]\tvalid_0's binary_logloss: 0.151566\n",
      "[312]\tvalid_0's binary_logloss: 0.151562\n",
      "[313]\tvalid_0's binary_logloss: 0.151561\n",
      "[314]\tvalid_0's binary_logloss: 0.151563\n",
      "[315]\tvalid_0's binary_logloss: 0.151563\n",
      "[316]\tvalid_0's binary_logloss: 0.15156\n",
      "[317]\tvalid_0's binary_logloss: 0.151562\n",
      "[318]\tvalid_0's binary_logloss: 0.151562\n",
      "[319]\tvalid_0's binary_logloss: 0.151565\n",
      "[320]\tvalid_0's binary_logloss: 0.151566\n",
      "[321]\tvalid_0's binary_logloss: 0.151567\n",
      "Early stopping, best iteration is:\n",
      "[316]\tvalid_0's binary_logloss: 0.15156\n",
      "316\n",
      "[1]\tvalid_0's binary_logloss: 0.667724\n",
      "Training until validation scores don't improve for 5 rounds.\n",
      "[2]\tvalid_0's binary_logloss: 0.643783\n",
      "[3]\tvalid_0's binary_logloss: 0.621198\n",
      "[4]\tvalid_0's binary_logloss: 0.599868\n",
      "[5]\tvalid_0's binary_logloss: 0.579703\n",
      "[6]\tvalid_0's binary_logloss: 0.560603\n",
      "[7]\tvalid_0's binary_logloss: 0.542519\n",
      "[8]\tvalid_0's binary_logloss: 0.525364\n",
      "[9]\tvalid_0's binary_logloss: 0.509082\n",
      "[10]\tvalid_0's binary_logloss: 0.493604\n",
      "[11]\tvalid_0's binary_logloss: 0.478884\n",
      "[12]\tvalid_0's binary_logloss: 0.464874\n",
      "[13]\tvalid_0's binary_logloss: 0.451534\n",
      "[14]\tvalid_0's binary_logloss: 0.43882\n",
      "[15]\tvalid_0's binary_logloss: 0.426698\n",
      "[16]\tvalid_0's binary_logloss: 0.415121\n",
      "[17]\tvalid_0's binary_logloss: 0.404072\n",
      "[18]\tvalid_0's binary_logloss: 0.393519\n",
      "[19]\tvalid_0's binary_logloss: 0.383438\n",
      "[20]\tvalid_0's binary_logloss: 0.373802\n",
      "[21]\tvalid_0's binary_logloss: 0.364594\n",
      "[22]\tvalid_0's binary_logloss: 0.355778\n",
      "[23]\tvalid_0's binary_logloss: 0.34735\n",
      "[24]\tvalid_0's binary_logloss: 0.339278\n",
      "[25]\tvalid_0's binary_logloss: 0.331553\n",
      "[26]\tvalid_0's binary_logloss: 0.324159\n",
      "[27]\tvalid_0's binary_logloss: 0.317076\n",
      "[28]\tvalid_0's binary_logloss: 0.310287\n",
      "[29]\tvalid_0's binary_logloss: 0.303779\n",
      "[30]\tvalid_0's binary_logloss: 0.297543\n",
      "[31]\tvalid_0's binary_logloss: 0.291574\n",
      "[32]\tvalid_0's binary_logloss: 0.285842\n",
      "[33]\tvalid_0's binary_logloss: 0.280346\n",
      "[34]\tvalid_0's binary_logloss: 0.275078\n",
      "[35]\tvalid_0's binary_logloss: 0.270032\n",
      "[36]\tvalid_0's binary_logloss: 0.265186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[37]\tvalid_0's binary_logloss: 0.260522\n",
      "[38]\tvalid_0's binary_logloss: 0.256046\n",
      "[39]\tvalid_0's binary_logloss: 0.251757\n",
      "[40]\tvalid_0's binary_logloss: 0.247648\n",
      "[41]\tvalid_0's binary_logloss: 0.243704\n",
      "[42]\tvalid_0's binary_logloss: 0.239921\n",
      "[43]\tvalid_0's binary_logloss: 0.236282\n",
      "[44]\tvalid_0's binary_logloss: 0.232791\n",
      "[45]\tvalid_0's binary_logloss: 0.229443\n",
      "[46]\tvalid_0's binary_logloss: 0.226228\n",
      "[47]\tvalid_0's binary_logloss: 0.223142\n",
      "[48]\tvalid_0's binary_logloss: 0.22018\n",
      "[49]\tvalid_0's binary_logloss: 0.217342\n",
      "[50]\tvalid_0's binary_logloss: 0.214615\n",
      "[51]\tvalid_0's binary_logloss: 0.212002\n",
      "[52]\tvalid_0's binary_logloss: 0.209497\n",
      "[53]\tvalid_0's binary_logloss: 0.207092\n",
      "[54]\tvalid_0's binary_logloss: 0.204788\n",
      "[55]\tvalid_0's binary_logloss: 0.202564\n",
      "[56]\tvalid_0's binary_logloss: 0.200431\n",
      "[57]\tvalid_0's binary_logloss: 0.198382\n",
      "[58]\tvalid_0's binary_logloss: 0.196419\n",
      "[59]\tvalid_0's binary_logloss: 0.194535\n",
      "[60]\tvalid_0's binary_logloss: 0.192724\n",
      "[61]\tvalid_0's binary_logloss: 0.191002\n",
      "[62]\tvalid_0's binary_logloss: 0.189344\n",
      "[63]\tvalid_0's binary_logloss: 0.187755\n",
      "[64]\tvalid_0's binary_logloss: 0.186232\n",
      "[65]\tvalid_0's binary_logloss: 0.184763\n",
      "[66]\tvalid_0's binary_logloss: 0.183354\n",
      "[67]\tvalid_0's binary_logloss: 0.18201\n",
      "[68]\tvalid_0's binary_logloss: 0.180721\n",
      "[69]\tvalid_0's binary_logloss: 0.179485\n",
      "[70]\tvalid_0's binary_logloss: 0.1783\n",
      "[71]\tvalid_0's binary_logloss: 0.177165\n",
      "[72]\tvalid_0's binary_logloss: 0.176078\n",
      "[73]\tvalid_0's binary_logloss: 0.175033\n",
      "[74]\tvalid_0's binary_logloss: 0.174038\n",
      "[75]\tvalid_0's binary_logloss: 0.173081\n",
      "[76]\tvalid_0's binary_logloss: 0.172167\n",
      "[77]\tvalid_0's binary_logloss: 0.171291\n",
      "[78]\tvalid_0's binary_logloss: 0.170453\n",
      "[79]\tvalid_0's binary_logloss: 0.169648\n",
      "[80]\tvalid_0's binary_logloss: 0.168891\n",
      "[81]\tvalid_0's binary_logloss: 0.168155\n",
      "[82]\tvalid_0's binary_logloss: 0.167456\n",
      "[83]\tvalid_0's binary_logloss: 0.166779\n",
      "[84]\tvalid_0's binary_logloss: 0.166133\n",
      "[85]\tvalid_0's binary_logloss: 0.165513\n",
      "[86]\tvalid_0's binary_logloss: 0.164922\n",
      "[87]\tvalid_0's binary_logloss: 0.164353\n",
      "[88]\tvalid_0's binary_logloss: 0.16381\n",
      "[89]\tvalid_0's binary_logloss: 0.163298\n",
      "[90]\tvalid_0's binary_logloss: 0.162805\n",
      "[91]\tvalid_0's binary_logloss: 0.162332\n",
      "[92]\tvalid_0's binary_logloss: 0.161885\n",
      "[93]\tvalid_0's binary_logloss: 0.161455\n",
      "[94]\tvalid_0's binary_logloss: 0.161044\n",
      "[95]\tvalid_0's binary_logloss: 0.160653\n",
      "[96]\tvalid_0's binary_logloss: 0.160279\n",
      "[97]\tvalid_0's binary_logloss: 0.159918\n",
      "[98]\tvalid_0's binary_logloss: 0.159572\n",
      "[99]\tvalid_0's binary_logloss: 0.159246\n",
      "[100]\tvalid_0's binary_logloss: 0.15893\n",
      "[101]\tvalid_0's binary_logloss: 0.158627\n",
      "[102]\tvalid_0's binary_logloss: 0.158341\n",
      "[103]\tvalid_0's binary_logloss: 0.158071\n",
      "[104]\tvalid_0's binary_logloss: 0.157816\n",
      "[105]\tvalid_0's binary_logloss: 0.157571\n",
      "[106]\tvalid_0's binary_logloss: 0.157328\n",
      "[107]\tvalid_0's binary_logloss: 0.157098\n",
      "[108]\tvalid_0's binary_logloss: 0.156882\n",
      "[109]\tvalid_0's binary_logloss: 0.156678\n",
      "[110]\tvalid_0's binary_logloss: 0.156478\n",
      "[111]\tvalid_0's binary_logloss: 0.156287\n",
      "[112]\tvalid_0's binary_logloss: 0.156112\n",
      "[113]\tvalid_0's binary_logloss: 0.155936\n",
      "[114]\tvalid_0's binary_logloss: 0.155774\n",
      "[115]\tvalid_0's binary_logloss: 0.155613\n",
      "[116]\tvalid_0's binary_logloss: 0.155465\n",
      "[117]\tvalid_0's binary_logloss: 0.155321\n",
      "[118]\tvalid_0's binary_logloss: 0.155187\n",
      "[119]\tvalid_0's binary_logloss: 0.15506\n",
      "[120]\tvalid_0's binary_logloss: 0.154936\n",
      "[121]\tvalid_0's binary_logloss: 0.154817\n",
      "[122]\tvalid_0's binary_logloss: 0.154707\n",
      "[123]\tvalid_0's binary_logloss: 0.154605\n",
      "[124]\tvalid_0's binary_logloss: 0.154508\n",
      "[125]\tvalid_0's binary_logloss: 0.15441\n",
      "[126]\tvalid_0's binary_logloss: 0.154311\n",
      "[127]\tvalid_0's binary_logloss: 0.154219\n",
      "[128]\tvalid_0's binary_logloss: 0.154138\n",
      "[129]\tvalid_0's binary_logloss: 0.154057\n",
      "[130]\tvalid_0's binary_logloss: 0.153978\n",
      "[131]\tvalid_0's binary_logloss: 0.153903\n",
      "[132]\tvalid_0's binary_logloss: 0.15383\n",
      "[133]\tvalid_0's binary_logloss: 0.15376\n",
      "[134]\tvalid_0's binary_logloss: 0.153698\n",
      "[135]\tvalid_0's binary_logloss: 0.153631\n",
      "[136]\tvalid_0's binary_logloss: 0.153575\n",
      "[137]\tvalid_0's binary_logloss: 0.153521\n",
      "[138]\tvalid_0's binary_logloss: 0.153468\n",
      "[139]\tvalid_0's binary_logloss: 0.15341\n",
      "[140]\tvalid_0's binary_logloss: 0.153368\n",
      "[141]\tvalid_0's binary_logloss: 0.153327\n",
      "[142]\tvalid_0's binary_logloss: 0.153284\n",
      "[143]\tvalid_0's binary_logloss: 0.153244\n",
      "[144]\tvalid_0's binary_logloss: 0.153198\n",
      "[145]\tvalid_0's binary_logloss: 0.153162\n",
      "[146]\tvalid_0's binary_logloss: 0.153124\n",
      "[147]\tvalid_0's binary_logloss: 0.153085\n",
      "[148]\tvalid_0's binary_logloss: 0.15305\n",
      "[149]\tvalid_0's binary_logloss: 0.153022\n",
      "[150]\tvalid_0's binary_logloss: 0.152996\n",
      "[151]\tvalid_0's binary_logloss: 0.152965\n",
      "[152]\tvalid_0's binary_logloss: 0.152931\n",
      "[153]\tvalid_0's binary_logloss: 0.152906\n",
      "[154]\tvalid_0's binary_logloss: 0.152881\n",
      "[155]\tvalid_0's binary_logloss: 0.152862\n",
      "[156]\tvalid_0's binary_logloss: 0.152836\n",
      "[157]\tvalid_0's binary_logloss: 0.152808\n",
      "[158]\tvalid_0's binary_logloss: 0.152783\n",
      "[159]\tvalid_0's binary_logloss: 0.152762\n",
      "[160]\tvalid_0's binary_logloss: 0.152742\n",
      "[161]\tvalid_0's binary_logloss: 0.152721\n",
      "[162]\tvalid_0's binary_logloss: 0.152709\n",
      "[163]\tvalid_0's binary_logloss: 0.152694\n",
      "[164]\tvalid_0's binary_logloss: 0.152679\n",
      "[165]\tvalid_0's binary_logloss: 0.152663\n",
      "[166]\tvalid_0's binary_logloss: 0.152641\n",
      "[167]\tvalid_0's binary_logloss: 0.152625\n",
      "[168]\tvalid_0's binary_logloss: 0.152611\n",
      "[169]\tvalid_0's binary_logloss: 0.152596\n",
      "[170]\tvalid_0's binary_logloss: 0.152584\n",
      "[171]\tvalid_0's binary_logloss: 0.152574\n",
      "[172]\tvalid_0's binary_logloss: 0.152565\n",
      "[173]\tvalid_0's binary_logloss: 0.152555\n",
      "[174]\tvalid_0's binary_logloss: 0.152544\n",
      "[175]\tvalid_0's binary_logloss: 0.152536\n",
      "[176]\tvalid_0's binary_logloss: 0.152523\n",
      "[177]\tvalid_0's binary_logloss: 0.152516\n",
      "[178]\tvalid_0's binary_logloss: 0.152512\n",
      "[179]\tvalid_0's binary_logloss: 0.152498\n",
      "[180]\tvalid_0's binary_logloss: 0.152489\n",
      "[181]\tvalid_0's binary_logloss: 0.152482\n",
      "[182]\tvalid_0's binary_logloss: 0.152472\n",
      "[183]\tvalid_0's binary_logloss: 0.152456\n",
      "[184]\tvalid_0's binary_logloss: 0.152444\n",
      "[185]\tvalid_0's binary_logloss: 0.152436\n",
      "[186]\tvalid_0's binary_logloss: 0.152426\n",
      "[187]\tvalid_0's binary_logloss: 0.15242\n",
      "[188]\tvalid_0's binary_logloss: 0.152412\n",
      "[189]\tvalid_0's binary_logloss: 0.152406\n",
      "[190]\tvalid_0's binary_logloss: 0.152399\n",
      "[191]\tvalid_0's binary_logloss: 0.152397\n",
      "[192]\tvalid_0's binary_logloss: 0.152393\n",
      "[193]\tvalid_0's binary_logloss: 0.152386\n",
      "[194]\tvalid_0's binary_logloss: 0.152383\n",
      "[195]\tvalid_0's binary_logloss: 0.152379\n",
      "[196]\tvalid_0's binary_logloss: 0.152373\n",
      "[197]\tvalid_0's binary_logloss: 0.152364\n",
      "[198]\tvalid_0's binary_logloss: 0.152351\n",
      "[199]\tvalid_0's binary_logloss: 0.152346\n",
      "[200]\tvalid_0's binary_logloss: 0.152337\n",
      "[201]\tvalid_0's binary_logloss: 0.152325\n",
      "[202]\tvalid_0's binary_logloss: 0.152319\n",
      "[203]\tvalid_0's binary_logloss: 0.152319\n",
      "[204]\tvalid_0's binary_logloss: 0.15232\n",
      "[205]\tvalid_0's binary_logloss: 0.152316\n",
      "[206]\tvalid_0's binary_logloss: 0.152313\n",
      "[207]\tvalid_0's binary_logloss: 0.152302\n",
      "[208]\tvalid_0's binary_logloss: 0.1523\n",
      "[209]\tvalid_0's binary_logloss: 0.152294\n",
      "[210]\tvalid_0's binary_logloss: 0.152294\n",
      "[211]\tvalid_0's binary_logloss: 0.152288\n",
      "[212]\tvalid_0's binary_logloss: 0.152285\n",
      "[213]\tvalid_0's binary_logloss: 0.152281\n",
      "[214]\tvalid_0's binary_logloss: 0.152278\n",
      "[215]\tvalid_0's binary_logloss: 0.152275\n",
      "[216]\tvalid_0's binary_logloss: 0.152277\n",
      "[217]\tvalid_0's binary_logloss: 0.152277\n",
      "[218]\tvalid_0's binary_logloss: 0.152276\n",
      "[219]\tvalid_0's binary_logloss: 0.152271\n",
      "[220]\tvalid_0's binary_logloss: 0.152268\n",
      "[221]\tvalid_0's binary_logloss: 0.152267\n",
      "[222]\tvalid_0's binary_logloss: 0.152264\n",
      "[223]\tvalid_0's binary_logloss: 0.152256\n",
      "[224]\tvalid_0's binary_logloss: 0.152252\n",
      "[225]\tvalid_0's binary_logloss: 0.152251\n",
      "[226]\tvalid_0's binary_logloss: 0.15225\n",
      "[227]\tvalid_0's binary_logloss: 0.152249\n",
      "[228]\tvalid_0's binary_logloss: 0.152249\n",
      "[229]\tvalid_0's binary_logloss: 0.152241\n",
      "[230]\tvalid_0's binary_logloss: 0.152238\n",
      "[231]\tvalid_0's binary_logloss: 0.152234\n",
      "[232]\tvalid_0's binary_logloss: 0.152234\n",
      "[233]\tvalid_0's binary_logloss: 0.152229\n",
      "[234]\tvalid_0's binary_logloss: 0.15223\n",
      "[235]\tvalid_0's binary_logloss: 0.152228\n",
      "[236]\tvalid_0's binary_logloss: 0.152226\n",
      "[237]\tvalid_0's binary_logloss: 0.152224\n",
      "[238]\tvalid_0's binary_logloss: 0.152222\n",
      "[239]\tvalid_0's binary_logloss: 0.152223\n",
      "[240]\tvalid_0's binary_logloss: 0.152223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[241]\tvalid_0's binary_logloss: 0.152223\n",
      "[242]\tvalid_0's binary_logloss: 0.152221\n",
      "[243]\tvalid_0's binary_logloss: 0.152215\n",
      "[244]\tvalid_0's binary_logloss: 0.152215\n",
      "[245]\tvalid_0's binary_logloss: 0.152212\n",
      "[246]\tvalid_0's binary_logloss: 0.152211\n",
      "[247]\tvalid_0's binary_logloss: 0.152209\n",
      "[248]\tvalid_0's binary_logloss: 0.152207\n",
      "[249]\tvalid_0's binary_logloss: 0.152202\n",
      "[250]\tvalid_0's binary_logloss: 0.152202\n",
      "[251]\tvalid_0's binary_logloss: 0.1522\n",
      "[252]\tvalid_0's binary_logloss: 0.152198\n",
      "[253]\tvalid_0's binary_logloss: 0.152192\n",
      "[254]\tvalid_0's binary_logloss: 0.152192\n",
      "[255]\tvalid_0's binary_logloss: 0.152191\n",
      "[256]\tvalid_0's binary_logloss: 0.152192\n",
      "[257]\tvalid_0's binary_logloss: 0.152189\n",
      "[258]\tvalid_0's binary_logloss: 0.152185\n",
      "[259]\tvalid_0's binary_logloss: 0.152182\n",
      "[260]\tvalid_0's binary_logloss: 0.152185\n",
      "[261]\tvalid_0's binary_logloss: 0.152183\n",
      "[262]\tvalid_0's binary_logloss: 0.152183\n",
      "[263]\tvalid_0's binary_logloss: 0.152181\n",
      "[264]\tvalid_0's binary_logloss: 0.152178\n",
      "[265]\tvalid_0's binary_logloss: 0.152179\n",
      "[266]\tvalid_0's binary_logloss: 0.152176\n",
      "[267]\tvalid_0's binary_logloss: 0.152176\n",
      "[268]\tvalid_0's binary_logloss: 0.152176\n",
      "[269]\tvalid_0's binary_logloss: 0.152176\n",
      "[270]\tvalid_0's binary_logloss: 0.152177\n",
      "[271]\tvalid_0's binary_logloss: 0.152175\n",
      "[272]\tvalid_0's binary_logloss: 0.152174\n",
      "[273]\tvalid_0's binary_logloss: 0.152176\n",
      "[274]\tvalid_0's binary_logloss: 0.152179\n",
      "[275]\tvalid_0's binary_logloss: 0.152177\n",
      "[276]\tvalid_0's binary_logloss: 0.152176\n",
      "[277]\tvalid_0's binary_logloss: 0.152171\n",
      "[278]\tvalid_0's binary_logloss: 0.152166\n",
      "[279]\tvalid_0's binary_logloss: 0.152162\n",
      "[280]\tvalid_0's binary_logloss: 0.152162\n",
      "[281]\tvalid_0's binary_logloss: 0.152161\n",
      "[282]\tvalid_0's binary_logloss: 0.152159\n",
      "[283]\tvalid_0's binary_logloss: 0.152154\n",
      "[284]\tvalid_0's binary_logloss: 0.15216\n",
      "[285]\tvalid_0's binary_logloss: 0.152159\n",
      "[286]\tvalid_0's binary_logloss: 0.152155\n",
      "[287]\tvalid_0's binary_logloss: 0.152159\n",
      "[288]\tvalid_0's binary_logloss: 0.15216\n",
      "Early stopping, best iteration is:\n",
      "[283]\tvalid_0's binary_logloss: 0.152154\n",
      "283\n",
      "[1]\tvalid_0's binary_logloss: 0.667708\n",
      "Training until validation scores don't improve for 5 rounds.\n",
      "[2]\tvalid_0's binary_logloss: 0.64376\n",
      "[3]\tvalid_0's binary_logloss: 0.62117\n",
      "[4]\tvalid_0's binary_logloss: 0.599841\n",
      "[5]\tvalid_0's binary_logloss: 0.579673\n",
      "[6]\tvalid_0's binary_logloss: 0.560579\n",
      "[7]\tvalid_0's binary_logloss: 0.542478\n",
      "[8]\tvalid_0's binary_logloss: 0.525308\n",
      "[9]\tvalid_0's binary_logloss: 0.509011\n",
      "[10]\tvalid_0's binary_logloss: 0.493528\n",
      "[11]\tvalid_0's binary_logloss: 0.478803\n",
      "[12]\tvalid_0's binary_logloss: 0.464788\n",
      "[13]\tvalid_0's binary_logloss: 0.451434\n",
      "[14]\tvalid_0's binary_logloss: 0.438707\n",
      "[15]\tvalid_0's binary_logloss: 0.42657\n",
      "[16]\tvalid_0's binary_logloss: 0.414996\n",
      "[17]\tvalid_0's binary_logloss: 0.403946\n",
      "[18]\tvalid_0's binary_logloss: 0.393392\n",
      "[19]\tvalid_0's binary_logloss: 0.383316\n",
      "[20]\tvalid_0's binary_logloss: 0.373683\n",
      "[21]\tvalid_0's binary_logloss: 0.364475\n",
      "[22]\tvalid_0's binary_logloss: 0.355668\n",
      "[23]\tvalid_0's binary_logloss: 0.347244\n",
      "[24]\tvalid_0's binary_logloss: 0.339176\n",
      "[25]\tvalid_0's binary_logloss: 0.331466\n",
      "[26]\tvalid_0's binary_logloss: 0.324084\n",
      "[27]\tvalid_0's binary_logloss: 0.317012\n",
      "[28]\tvalid_0's binary_logloss: 0.31022\n",
      "[29]\tvalid_0's binary_logloss: 0.303712\n",
      "[30]\tvalid_0's binary_logloss: 0.297471\n",
      "[31]\tvalid_0's binary_logloss: 0.291503\n",
      "[32]\tvalid_0's binary_logloss: 0.285775\n",
      "[33]\tvalid_0's binary_logloss: 0.280283\n",
      "[34]\tvalid_0's binary_logloss: 0.275015\n",
      "[35]\tvalid_0's binary_logloss: 0.269965\n",
      "[36]\tvalid_0's binary_logloss: 0.265114\n",
      "[37]\tvalid_0's binary_logloss: 0.260455\n",
      "[38]\tvalid_0's binary_logloss: 0.255983\n",
      "[39]\tvalid_0's binary_logloss: 0.251695\n",
      "[40]\tvalid_0's binary_logloss: 0.247577\n",
      "[41]\tvalid_0's binary_logloss: 0.243624\n",
      "[42]\tvalid_0's binary_logloss: 0.239834\n",
      "[43]\tvalid_0's binary_logloss: 0.236198\n",
      "[44]\tvalid_0's binary_logloss: 0.232709\n",
      "[45]\tvalid_0's binary_logloss: 0.229364\n",
      "[46]\tvalid_0's binary_logloss: 0.226151\n",
      "[47]\tvalid_0's binary_logloss: 0.22307\n",
      "[48]\tvalid_0's binary_logloss: 0.220111\n",
      "[49]\tvalid_0's binary_logloss: 0.217267\n",
      "[50]\tvalid_0's binary_logloss: 0.214536\n",
      "[51]\tvalid_0's binary_logloss: 0.211919\n",
      "[52]\tvalid_0's binary_logloss: 0.209415\n",
      "[53]\tvalid_0's binary_logloss: 0.207012\n",
      "[54]\tvalid_0's binary_logloss: 0.204706\n",
      "[55]\tvalid_0's binary_logloss: 0.202487\n",
      "[56]\tvalid_0's binary_logloss: 0.20036\n",
      "[57]\tvalid_0's binary_logloss: 0.198317\n",
      "[58]\tvalid_0's binary_logloss: 0.196359\n",
      "[59]\tvalid_0's binary_logloss: 0.19448\n",
      "[60]\tvalid_0's binary_logloss: 0.192674\n",
      "[61]\tvalid_0's binary_logloss: 0.190951\n",
      "[62]\tvalid_0's binary_logloss: 0.189294\n",
      "[63]\tvalid_0's binary_logloss: 0.187701\n",
      "[64]\tvalid_0's binary_logloss: 0.186181\n",
      "[65]\tvalid_0's binary_logloss: 0.184723\n",
      "[66]\tvalid_0's binary_logloss: 0.18332\n",
      "[67]\tvalid_0's binary_logloss: 0.181972\n",
      "[68]\tvalid_0's binary_logloss: 0.180679\n",
      "[69]\tvalid_0's binary_logloss: 0.179443\n",
      "[70]\tvalid_0's binary_logloss: 0.178255\n",
      "[71]\tvalid_0's binary_logloss: 0.177116\n",
      "[72]\tvalid_0's binary_logloss: 0.17603\n",
      "[73]\tvalid_0's binary_logloss: 0.174982\n",
      "[74]\tvalid_0's binary_logloss: 0.173987\n",
      "[75]\tvalid_0's binary_logloss: 0.17303\n",
      "[76]\tvalid_0's binary_logloss: 0.172119\n",
      "[77]\tvalid_0's binary_logloss: 0.17125\n",
      "[78]\tvalid_0's binary_logloss: 0.170414\n",
      "[79]\tvalid_0's binary_logloss: 0.169607\n",
      "[80]\tvalid_0's binary_logloss: 0.168844\n",
      "[81]\tvalid_0's binary_logloss: 0.168106\n",
      "[82]\tvalid_0's binary_logloss: 0.167404\n",
      "[83]\tvalid_0's binary_logloss: 0.166728\n",
      "[84]\tvalid_0's binary_logloss: 0.166081\n",
      "[85]\tvalid_0's binary_logloss: 0.165457\n",
      "[86]\tvalid_0's binary_logloss: 0.164863\n",
      "[87]\tvalid_0's binary_logloss: 0.164297\n",
      "[88]\tvalid_0's binary_logloss: 0.163756\n",
      "[89]\tvalid_0's binary_logloss: 0.16324\n",
      "[90]\tvalid_0's binary_logloss: 0.162746\n",
      "[91]\tvalid_0's binary_logloss: 0.162272\n",
      "[92]\tvalid_0's binary_logloss: 0.161826\n",
      "[93]\tvalid_0's binary_logloss: 0.161392\n",
      "[94]\tvalid_0's binary_logloss: 0.160981\n",
      "[95]\tvalid_0's binary_logloss: 0.16059\n",
      "[96]\tvalid_0's binary_logloss: 0.160218\n",
      "[97]\tvalid_0's binary_logloss: 0.159856\n",
      "[98]\tvalid_0's binary_logloss: 0.159513\n",
      "[99]\tvalid_0's binary_logloss: 0.159183\n",
      "[100]\tvalid_0's binary_logloss: 0.158871\n",
      "[101]\tvalid_0's binary_logloss: 0.15857\n",
      "[102]\tvalid_0's binary_logloss: 0.158285\n",
      "[103]\tvalid_0's binary_logloss: 0.158014\n",
      "[104]\tvalid_0's binary_logloss: 0.157756\n",
      "[105]\tvalid_0's binary_logloss: 0.157505\n",
      "[106]\tvalid_0's binary_logloss: 0.157266\n",
      "[107]\tvalid_0's binary_logloss: 0.157039\n",
      "[108]\tvalid_0's binary_logloss: 0.156822\n",
      "[109]\tvalid_0's binary_logloss: 0.156621\n",
      "[110]\tvalid_0's binary_logloss: 0.156427\n",
      "[111]\tvalid_0's binary_logloss: 0.156234\n",
      "[112]\tvalid_0's binary_logloss: 0.156051\n",
      "[113]\tvalid_0's binary_logloss: 0.15588\n",
      "[114]\tvalid_0's binary_logloss: 0.15571\n",
      "[115]\tvalid_0's binary_logloss: 0.155558\n",
      "[116]\tvalid_0's binary_logloss: 0.155406\n",
      "[117]\tvalid_0's binary_logloss: 0.155265\n",
      "[118]\tvalid_0's binary_logloss: 0.155129\n",
      "[119]\tvalid_0's binary_logloss: 0.154997\n",
      "[120]\tvalid_0's binary_logloss: 0.154871\n",
      "[121]\tvalid_0's binary_logloss: 0.154754\n",
      "[122]\tvalid_0's binary_logloss: 0.154638\n",
      "[123]\tvalid_0's binary_logloss: 0.154533\n",
      "[124]\tvalid_0's binary_logloss: 0.154436\n",
      "[125]\tvalid_0's binary_logloss: 0.154339\n",
      "[126]\tvalid_0's binary_logloss: 0.154248\n",
      "[127]\tvalid_0's binary_logloss: 0.15416\n",
      "[128]\tvalid_0's binary_logloss: 0.154077\n",
      "[129]\tvalid_0's binary_logloss: 0.154001\n",
      "[130]\tvalid_0's binary_logloss: 0.15392\n",
      "[131]\tvalid_0's binary_logloss: 0.153841\n",
      "[132]\tvalid_0's binary_logloss: 0.153766\n",
      "[133]\tvalid_0's binary_logloss: 0.153696\n",
      "[134]\tvalid_0's binary_logloss: 0.153636\n",
      "[135]\tvalid_0's binary_logloss: 0.153575\n",
      "[136]\tvalid_0's binary_logloss: 0.153514\n",
      "[137]\tvalid_0's binary_logloss: 0.153456\n",
      "[138]\tvalid_0's binary_logloss: 0.153407\n",
      "[139]\tvalid_0's binary_logloss: 0.153355\n",
      "[140]\tvalid_0's binary_logloss: 0.153309\n",
      "[141]\tvalid_0's binary_logloss: 0.153262\n",
      "[142]\tvalid_0's binary_logloss: 0.153221\n",
      "[143]\tvalid_0's binary_logloss: 0.153179\n",
      "[144]\tvalid_0's binary_logloss: 0.153141\n",
      "[145]\tvalid_0's binary_logloss: 0.153102\n",
      "[146]\tvalid_0's binary_logloss: 0.153063\n",
      "[147]\tvalid_0's binary_logloss: 0.153022\n",
      "[148]\tvalid_0's binary_logloss: 0.152986\n",
      "[149]\tvalid_0's binary_logloss: 0.152952\n",
      "[150]\tvalid_0's binary_logloss: 0.15292\n",
      "[151]\tvalid_0's binary_logloss: 0.152892\n",
      "[152]\tvalid_0's binary_logloss: 0.152862\n",
      "[153]\tvalid_0's binary_logloss: 0.152838\n",
      "[154]\tvalid_0's binary_logloss: 0.15281\n",
      "[155]\tvalid_0's binary_logloss: 0.152788\n",
      "[156]\tvalid_0's binary_logloss: 0.152763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[157]\tvalid_0's binary_logloss: 0.152737\n",
      "[158]\tvalid_0's binary_logloss: 0.152717\n",
      "[159]\tvalid_0's binary_logloss: 0.152697\n",
      "[160]\tvalid_0's binary_logloss: 0.152676\n",
      "[161]\tvalid_0's binary_logloss: 0.152657\n",
      "[162]\tvalid_0's binary_logloss: 0.152644\n",
      "[163]\tvalid_0's binary_logloss: 0.152621\n",
      "[164]\tvalid_0's binary_logloss: 0.152602\n",
      "[165]\tvalid_0's binary_logloss: 0.152582\n",
      "[166]\tvalid_0's binary_logloss: 0.152562\n",
      "[167]\tvalid_0's binary_logloss: 0.152545\n",
      "[168]\tvalid_0's binary_logloss: 0.152528\n",
      "[169]\tvalid_0's binary_logloss: 0.152512\n",
      "[170]\tvalid_0's binary_logloss: 0.152489\n",
      "[171]\tvalid_0's binary_logloss: 0.152472\n",
      "[172]\tvalid_0's binary_logloss: 0.152455\n",
      "[173]\tvalid_0's binary_logloss: 0.152439\n",
      "[174]\tvalid_0's binary_logloss: 0.152429\n",
      "[175]\tvalid_0's binary_logloss: 0.152423\n",
      "[176]\tvalid_0's binary_logloss: 0.152414\n",
      "[177]\tvalid_0's binary_logloss: 0.152404\n",
      "[178]\tvalid_0's binary_logloss: 0.152396\n",
      "[179]\tvalid_0's binary_logloss: 0.152382\n",
      "[180]\tvalid_0's binary_logloss: 0.152375\n",
      "[181]\tvalid_0's binary_logloss: 0.152365\n",
      "[182]\tvalid_0's binary_logloss: 0.152357\n",
      "[183]\tvalid_0's binary_logloss: 0.152346\n",
      "[184]\tvalid_0's binary_logloss: 0.152335\n",
      "[185]\tvalid_0's binary_logloss: 0.152325\n",
      "[186]\tvalid_0's binary_logloss: 0.152316\n",
      "[187]\tvalid_0's binary_logloss: 0.152309\n",
      "[188]\tvalid_0's binary_logloss: 0.152295\n",
      "[189]\tvalid_0's binary_logloss: 0.152288\n",
      "[190]\tvalid_0's binary_logloss: 0.152284\n",
      "[191]\tvalid_0's binary_logloss: 0.152278\n",
      "[192]\tvalid_0's binary_logloss: 0.152272\n",
      "[193]\tvalid_0's binary_logloss: 0.152264\n",
      "[194]\tvalid_0's binary_logloss: 0.152255\n",
      "[195]\tvalid_0's binary_logloss: 0.152248\n",
      "[196]\tvalid_0's binary_logloss: 0.152244\n",
      "[197]\tvalid_0's binary_logloss: 0.152235\n",
      "[198]\tvalid_0's binary_logloss: 0.152227\n",
      "[199]\tvalid_0's binary_logloss: 0.152225\n",
      "[200]\tvalid_0's binary_logloss: 0.15222\n",
      "[201]\tvalid_0's binary_logloss: 0.152212\n",
      "[202]\tvalid_0's binary_logloss: 0.152205\n",
      "[203]\tvalid_0's binary_logloss: 0.1522\n",
      "[204]\tvalid_0's binary_logloss: 0.152194\n",
      "[205]\tvalid_0's binary_logloss: 0.152194\n",
      "[206]\tvalid_0's binary_logloss: 0.152193\n",
      "[207]\tvalid_0's binary_logloss: 0.152181\n",
      "[208]\tvalid_0's binary_logloss: 0.152177\n",
      "[209]\tvalid_0's binary_logloss: 0.152172\n",
      "[210]\tvalid_0's binary_logloss: 0.152165\n",
      "[211]\tvalid_0's binary_logloss: 0.152162\n",
      "[212]\tvalid_0's binary_logloss: 0.15216\n",
      "[213]\tvalid_0's binary_logloss: 0.152155\n",
      "[214]\tvalid_0's binary_logloss: 0.152153\n",
      "[215]\tvalid_0's binary_logloss: 0.152154\n",
      "[216]\tvalid_0's binary_logloss: 0.152147\n",
      "[217]\tvalid_0's binary_logloss: 0.152141\n",
      "[218]\tvalid_0's binary_logloss: 0.152135\n",
      "[219]\tvalid_0's binary_logloss: 0.152127\n",
      "[220]\tvalid_0's binary_logloss: 0.152125\n",
      "[221]\tvalid_0's binary_logloss: 0.152127\n",
      "[222]\tvalid_0's binary_logloss: 0.152126\n",
      "[223]\tvalid_0's binary_logloss: 0.152123\n",
      "[224]\tvalid_0's binary_logloss: 0.15212\n",
      "[225]\tvalid_0's binary_logloss: 0.152116\n",
      "[226]\tvalid_0's binary_logloss: 0.152116\n",
      "[227]\tvalid_0's binary_logloss: 0.152114\n",
      "[228]\tvalid_0's binary_logloss: 0.152112\n",
      "[229]\tvalid_0's binary_logloss: 0.152107\n",
      "[230]\tvalid_0's binary_logloss: 0.152095\n",
      "[231]\tvalid_0's binary_logloss: 0.152096\n",
      "[232]\tvalid_0's binary_logloss: 0.152095\n",
      "[233]\tvalid_0's binary_logloss: 0.15209\n",
      "[234]\tvalid_0's binary_logloss: 0.152083\n",
      "[235]\tvalid_0's binary_logloss: 0.152082\n",
      "[236]\tvalid_0's binary_logloss: 0.152078\n",
      "[237]\tvalid_0's binary_logloss: 0.152077\n",
      "[238]\tvalid_0's binary_logloss: 0.152077\n",
      "[239]\tvalid_0's binary_logloss: 0.152076\n",
      "[240]\tvalid_0's binary_logloss: 0.152076\n",
      "[241]\tvalid_0's binary_logloss: 0.152074\n",
      "[242]\tvalid_0's binary_logloss: 0.152074\n",
      "[243]\tvalid_0's binary_logloss: 0.152071\n",
      "[244]\tvalid_0's binary_logloss: 0.152072\n",
      "[245]\tvalid_0's binary_logloss: 0.152072\n",
      "[246]\tvalid_0's binary_logloss: 0.152071\n",
      "[247]\tvalid_0's binary_logloss: 0.15207\n",
      "[248]\tvalid_0's binary_logloss: 0.15207\n",
      "[249]\tvalid_0's binary_logloss: 0.152067\n",
      "[250]\tvalid_0's binary_logloss: 0.152065\n",
      "[251]\tvalid_0's binary_logloss: 0.152064\n",
      "[252]\tvalid_0's binary_logloss: 0.152064\n",
      "[253]\tvalid_0's binary_logloss: 0.152065\n",
      "[254]\tvalid_0's binary_logloss: 0.152062\n",
      "[255]\tvalid_0's binary_logloss: 0.15206\n",
      "[256]\tvalid_0's binary_logloss: 0.15206\n",
      "[257]\tvalid_0's binary_logloss: 0.152058\n",
      "[258]\tvalid_0's binary_logloss: 0.152055\n",
      "[259]\tvalid_0's binary_logloss: 0.15205\n",
      "[260]\tvalid_0's binary_logloss: 0.15205\n",
      "[261]\tvalid_0's binary_logloss: 0.152048\n",
      "[262]\tvalid_0's binary_logloss: 0.152047\n",
      "[263]\tvalid_0's binary_logloss: 0.152049\n",
      "[264]\tvalid_0's binary_logloss: 0.152048\n",
      "[265]\tvalid_0's binary_logloss: 0.152045\n",
      "[266]\tvalid_0's binary_logloss: 0.152042\n",
      "[267]\tvalid_0's binary_logloss: 0.152043\n",
      "[268]\tvalid_0's binary_logloss: 0.152043\n",
      "[269]\tvalid_0's binary_logloss: 0.152042\n",
      "[270]\tvalid_0's binary_logloss: 0.152042\n",
      "[271]\tvalid_0's binary_logloss: 0.152044\n",
      "[272]\tvalid_0's binary_logloss: 0.152044\n",
      "[273]\tvalid_0's binary_logloss: 0.152043\n",
      "[274]\tvalid_0's binary_logloss: 0.152043\n",
      "Early stopping, best iteration is:\n",
      "[269]\tvalid_0's binary_logloss: 0.152042\n",
      "269\n",
      "[1]\tvalid_0's binary_logloss: 0.667726\n",
      "Training until validation scores don't improve for 5 rounds.\n",
      "[2]\tvalid_0's binary_logloss: 0.643785\n",
      "[3]\tvalid_0's binary_logloss: 0.621205\n",
      "[4]\tvalid_0's binary_logloss: 0.59989\n",
      "[5]\tvalid_0's binary_logloss: 0.579737\n",
      "[6]\tvalid_0's binary_logloss: 0.560653\n",
      "[7]\tvalid_0's binary_logloss: 0.542567\n",
      "[8]\tvalid_0's binary_logloss: 0.525409\n",
      "[9]\tvalid_0's binary_logloss: 0.509122\n",
      "[10]\tvalid_0's binary_logloss: 0.49363\n",
      "[11]\tvalid_0's binary_logloss: 0.478899\n",
      "[12]\tvalid_0's binary_logloss: 0.464882\n",
      "[13]\tvalid_0's binary_logloss: 0.45152\n",
      "[14]\tvalid_0's binary_logloss: 0.438787\n",
      "[15]\tvalid_0's binary_logloss: 0.42665\n",
      "[16]\tvalid_0's binary_logloss: 0.415082\n",
      "[17]\tvalid_0's binary_logloss: 0.404036\n",
      "[18]\tvalid_0's binary_logloss: 0.393491\n",
      "[19]\tvalid_0's binary_logloss: 0.383414\n",
      "[20]\tvalid_0's binary_logloss: 0.373783\n",
      "[21]\tvalid_0's binary_logloss: 0.364573\n",
      "[22]\tvalid_0's binary_logloss: 0.355752\n",
      "[23]\tvalid_0's binary_logloss: 0.347315\n",
      "[24]\tvalid_0's binary_logloss: 0.339235\n",
      "[25]\tvalid_0's binary_logloss: 0.33152\n",
      "[26]\tvalid_0's binary_logloss: 0.324132\n",
      "[27]\tvalid_0's binary_logloss: 0.317054\n",
      "[28]\tvalid_0's binary_logloss: 0.310266\n",
      "[29]\tvalid_0's binary_logloss: 0.30376\n",
      "[30]\tvalid_0's binary_logloss: 0.297524\n",
      "[31]\tvalid_0's binary_logloss: 0.291551\n",
      "[32]\tvalid_0's binary_logloss: 0.285817\n",
      "[33]\tvalid_0's binary_logloss: 0.28032\n",
      "[34]\tvalid_0's binary_logloss: 0.275038\n",
      "[35]\tvalid_0's binary_logloss: 0.269976\n",
      "[36]\tvalid_0's binary_logloss: 0.265115\n",
      "[37]\tvalid_0's binary_logloss: 0.26046\n",
      "[38]\tvalid_0's binary_logloss: 0.255995\n",
      "[39]\tvalid_0's binary_logloss: 0.251713\n",
      "[40]\tvalid_0's binary_logloss: 0.247599\n",
      "[41]\tvalid_0's binary_logloss: 0.243649\n",
      "[42]\tvalid_0's binary_logloss: 0.239859\n",
      "[43]\tvalid_0's binary_logloss: 0.236222\n",
      "[44]\tvalid_0's binary_logloss: 0.232736\n",
      "[45]\tvalid_0's binary_logloss: 0.229394\n",
      "[46]\tvalid_0's binary_logloss: 0.226177\n",
      "[47]\tvalid_0's binary_logloss: 0.223087\n",
      "[48]\tvalid_0's binary_logloss: 0.220122\n",
      "[49]\tvalid_0's binary_logloss: 0.217279\n",
      "[50]\tvalid_0's binary_logloss: 0.214548\n",
      "[51]\tvalid_0's binary_logloss: 0.211929\n",
      "[52]\tvalid_0's binary_logloss: 0.209418\n",
      "[53]\tvalid_0's binary_logloss: 0.207009\n",
      "[54]\tvalid_0's binary_logloss: 0.2047\n",
      "[55]\tvalid_0's binary_logloss: 0.202479\n",
      "[56]\tvalid_0's binary_logloss: 0.200348\n",
      "[57]\tvalid_0's binary_logloss: 0.1983\n",
      "[58]\tvalid_0's binary_logloss: 0.196342\n",
      "[59]\tvalid_0's binary_logloss: 0.194464\n",
      "[60]\tvalid_0's binary_logloss: 0.19266\n",
      "[61]\tvalid_0's binary_logloss: 0.190934\n",
      "[62]\tvalid_0's binary_logloss: 0.189274\n",
      "[63]\tvalid_0's binary_logloss: 0.18768\n",
      "[64]\tvalid_0's binary_logloss: 0.186156\n",
      "[65]\tvalid_0's binary_logloss: 0.184688\n",
      "[66]\tvalid_0's binary_logloss: 0.183282\n",
      "[67]\tvalid_0's binary_logloss: 0.181934\n",
      "[68]\tvalid_0's binary_logloss: 0.180639\n",
      "[69]\tvalid_0's binary_logloss: 0.179404\n",
      "[70]\tvalid_0's binary_logloss: 0.17822\n",
      "[71]\tvalid_0's binary_logloss: 0.177076\n",
      "[72]\tvalid_0's binary_logloss: 0.175988\n",
      "[73]\tvalid_0's binary_logloss: 0.174942\n",
      "[74]\tvalid_0's binary_logloss: 0.173944\n",
      "[75]\tvalid_0's binary_logloss: 0.172983\n",
      "[76]\tvalid_0's binary_logloss: 0.172065\n",
      "[77]\tvalid_0's binary_logloss: 0.171182\n",
      "[78]\tvalid_0's binary_logloss: 0.170339\n",
      "[79]\tvalid_0's binary_logloss: 0.16953\n",
      "[80]\tvalid_0's binary_logloss: 0.168767\n",
      "[81]\tvalid_0's binary_logloss: 0.168025\n",
      "[82]\tvalid_0's binary_logloss: 0.167322\n",
      "[83]\tvalid_0's binary_logloss: 0.166643\n",
      "[84]\tvalid_0's binary_logloss: 0.165995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[85]\tvalid_0's binary_logloss: 0.165369\n",
      "[86]\tvalid_0's binary_logloss: 0.164772\n",
      "[87]\tvalid_0's binary_logloss: 0.164195\n",
      "[88]\tvalid_0's binary_logloss: 0.163656\n",
      "[89]\tvalid_0's binary_logloss: 0.163135\n",
      "[90]\tvalid_0's binary_logloss: 0.162637\n",
      "[91]\tvalid_0's binary_logloss: 0.162164\n",
      "[92]\tvalid_0's binary_logloss: 0.161715\n",
      "[93]\tvalid_0's binary_logloss: 0.161278\n",
      "[94]\tvalid_0's binary_logloss: 0.160868\n",
      "[95]\tvalid_0's binary_logloss: 0.16047\n",
      "[96]\tvalid_0's binary_logloss: 0.160101\n",
      "[97]\tvalid_0's binary_logloss: 0.159739\n",
      "[98]\tvalid_0's binary_logloss: 0.159392\n",
      "[99]\tvalid_0's binary_logloss: 0.15906\n",
      "[100]\tvalid_0's binary_logloss: 0.158745\n",
      "[101]\tvalid_0's binary_logloss: 0.158438\n",
      "[102]\tvalid_0's binary_logloss: 0.158143\n",
      "[103]\tvalid_0's binary_logloss: 0.15787\n",
      "[104]\tvalid_0's binary_logloss: 0.157614\n",
      "[105]\tvalid_0's binary_logloss: 0.157368\n",
      "[106]\tvalid_0's binary_logloss: 0.157134\n",
      "[107]\tvalid_0's binary_logloss: 0.156905\n",
      "[108]\tvalid_0's binary_logloss: 0.156685\n",
      "[109]\tvalid_0's binary_logloss: 0.156478\n",
      "[110]\tvalid_0's binary_logloss: 0.156279\n",
      "[111]\tvalid_0's binary_logloss: 0.156084\n",
      "[112]\tvalid_0's binary_logloss: 0.1559\n",
      "[113]\tvalid_0's binary_logloss: 0.15572\n",
      "[114]\tvalid_0's binary_logloss: 0.155554\n",
      "[115]\tvalid_0's binary_logloss: 0.155397\n",
      "[116]\tvalid_0's binary_logloss: 0.155246\n",
      "[117]\tvalid_0's binary_logloss: 0.155099\n",
      "[118]\tvalid_0's binary_logloss: 0.154963\n",
      "[119]\tvalid_0's binary_logloss: 0.154827\n",
      "[120]\tvalid_0's binary_logloss: 0.154702\n",
      "[121]\tvalid_0's binary_logloss: 0.154583\n",
      "[122]\tvalid_0's binary_logloss: 0.154471\n",
      "[123]\tvalid_0's binary_logloss: 0.154364\n",
      "[124]\tvalid_0's binary_logloss: 0.154262\n",
      "[125]\tvalid_0's binary_logloss: 0.154162\n",
      "[126]\tvalid_0's binary_logloss: 0.154068\n",
      "[127]\tvalid_0's binary_logloss: 0.153977\n",
      "[128]\tvalid_0's binary_logloss: 0.153893\n",
      "[129]\tvalid_0's binary_logloss: 0.153816\n",
      "[130]\tvalid_0's binary_logloss: 0.153742\n",
      "[131]\tvalid_0's binary_logloss: 0.153666\n",
      "[132]\tvalid_0's binary_logloss: 0.153596\n",
      "[133]\tvalid_0's binary_logloss: 0.153529\n",
      "[134]\tvalid_0's binary_logloss: 0.153466\n",
      "[135]\tvalid_0's binary_logloss: 0.153396\n",
      "[136]\tvalid_0's binary_logloss: 0.153331\n",
      "[137]\tvalid_0's binary_logloss: 0.153276\n",
      "[138]\tvalid_0's binary_logloss: 0.153215\n",
      "[139]\tvalid_0's binary_logloss: 0.153155\n",
      "[140]\tvalid_0's binary_logloss: 0.153108\n",
      "[141]\tvalid_0's binary_logloss: 0.153066\n",
      "[142]\tvalid_0's binary_logloss: 0.153025\n",
      "[143]\tvalid_0's binary_logloss: 0.152979\n",
      "[144]\tvalid_0's binary_logloss: 0.152939\n",
      "[145]\tvalid_0's binary_logloss: 0.15289\n",
      "[146]\tvalid_0's binary_logloss: 0.152849\n",
      "[147]\tvalid_0's binary_logloss: 0.152813\n",
      "[148]\tvalid_0's binary_logloss: 0.152777\n",
      "[149]\tvalid_0's binary_logloss: 0.152743\n",
      "[150]\tvalid_0's binary_logloss: 0.152711\n",
      "[151]\tvalid_0's binary_logloss: 0.15268\n",
      "[152]\tvalid_0's binary_logloss: 0.152648\n",
      "[153]\tvalid_0's binary_logloss: 0.15262\n",
      "[154]\tvalid_0's binary_logloss: 0.152591\n",
      "[155]\tvalid_0's binary_logloss: 0.152568\n",
      "[156]\tvalid_0's binary_logloss: 0.152537\n",
      "[157]\tvalid_0's binary_logloss: 0.152511\n",
      "[158]\tvalid_0's binary_logloss: 0.152486\n",
      "[159]\tvalid_0's binary_logloss: 0.152459\n",
      "[160]\tvalid_0's binary_logloss: 0.152439\n",
      "[161]\tvalid_0's binary_logloss: 0.152418\n",
      "[162]\tvalid_0's binary_logloss: 0.152398\n",
      "[163]\tvalid_0's binary_logloss: 0.152376\n",
      "[164]\tvalid_0's binary_logloss: 0.152358\n",
      "[165]\tvalid_0's binary_logloss: 0.152339\n",
      "[166]\tvalid_0's binary_logloss: 0.152314\n",
      "[167]\tvalid_0's binary_logloss: 0.152294\n",
      "[168]\tvalid_0's binary_logloss: 0.152277\n",
      "[169]\tvalid_0's binary_logloss: 0.152263\n",
      "[170]\tvalid_0's binary_logloss: 0.152246\n",
      "[171]\tvalid_0's binary_logloss: 0.152227\n",
      "[172]\tvalid_0's binary_logloss: 0.152212\n",
      "[173]\tvalid_0's binary_logloss: 0.152195\n",
      "[174]\tvalid_0's binary_logloss: 0.152177\n",
      "[175]\tvalid_0's binary_logloss: 0.152165\n",
      "[176]\tvalid_0's binary_logloss: 0.152153\n",
      "[177]\tvalid_0's binary_logloss: 0.152142\n",
      "[178]\tvalid_0's binary_logloss: 0.152133\n",
      "[179]\tvalid_0's binary_logloss: 0.152119\n",
      "[180]\tvalid_0's binary_logloss: 0.152109\n",
      "[181]\tvalid_0's binary_logloss: 0.152099\n",
      "[182]\tvalid_0's binary_logloss: 0.152088\n",
      "[183]\tvalid_0's binary_logloss: 0.152075\n",
      "[184]\tvalid_0's binary_logloss: 0.152061\n",
      "[185]\tvalid_0's binary_logloss: 0.152054\n",
      "[186]\tvalid_0's binary_logloss: 0.152042\n",
      "[187]\tvalid_0's binary_logloss: 0.152031\n",
      "[188]\tvalid_0's binary_logloss: 0.152017\n",
      "[189]\tvalid_0's binary_logloss: 0.152007\n",
      "[190]\tvalid_0's binary_logloss: 0.152003\n",
      "[191]\tvalid_0's binary_logloss: 0.151996\n",
      "[192]\tvalid_0's binary_logloss: 0.151987\n",
      "[193]\tvalid_0's binary_logloss: 0.151983\n",
      "[194]\tvalid_0's binary_logloss: 0.151977\n",
      "[195]\tvalid_0's binary_logloss: 0.15197\n",
      "[196]\tvalid_0's binary_logloss: 0.151969\n",
      "[197]\tvalid_0's binary_logloss: 0.151962\n",
      "[198]\tvalid_0's binary_logloss: 0.151956\n",
      "[199]\tvalid_0's binary_logloss: 0.15195\n",
      "[200]\tvalid_0's binary_logloss: 0.151945\n",
      "[201]\tvalid_0's binary_logloss: 0.151939\n",
      "[202]\tvalid_0's binary_logloss: 0.151939\n",
      "[203]\tvalid_0's binary_logloss: 0.151936\n",
      "[204]\tvalid_0's binary_logloss: 0.151933\n",
      "[205]\tvalid_0's binary_logloss: 0.15193\n",
      "[206]\tvalid_0's binary_logloss: 0.151925\n",
      "[207]\tvalid_0's binary_logloss: 0.151916\n",
      "[208]\tvalid_0's binary_logloss: 0.151912\n",
      "[209]\tvalid_0's binary_logloss: 0.151907\n",
      "[210]\tvalid_0's binary_logloss: 0.151898\n",
      "[211]\tvalid_0's binary_logloss: 0.151893\n",
      "[212]\tvalid_0's binary_logloss: 0.151881\n",
      "[213]\tvalid_0's binary_logloss: 0.151869\n",
      "[214]\tvalid_0's binary_logloss: 0.151862\n",
      "[215]\tvalid_0's binary_logloss: 0.151854\n",
      "[216]\tvalid_0's binary_logloss: 0.15185\n",
      "[217]\tvalid_0's binary_logloss: 0.151849\n",
      "[218]\tvalid_0's binary_logloss: 0.151845\n",
      "[219]\tvalid_0's binary_logloss: 0.151842\n",
      "[220]\tvalid_0's binary_logloss: 0.151839\n",
      "[221]\tvalid_0's binary_logloss: 0.151837\n",
      "[222]\tvalid_0's binary_logloss: 0.151832\n",
      "[223]\tvalid_0's binary_logloss: 0.15183\n",
      "[224]\tvalid_0's binary_logloss: 0.151826\n",
      "[225]\tvalid_0's binary_logloss: 0.151822\n",
      "[226]\tvalid_0's binary_logloss: 0.151817\n",
      "[227]\tvalid_0's binary_logloss: 0.151813\n",
      "[228]\tvalid_0's binary_logloss: 0.151812\n",
      "[229]\tvalid_0's binary_logloss: 0.151802\n",
      "[230]\tvalid_0's binary_logloss: 0.151795\n",
      "[231]\tvalid_0's binary_logloss: 0.151793\n",
      "[232]\tvalid_0's binary_logloss: 0.151792\n",
      "[233]\tvalid_0's binary_logloss: 0.151789\n",
      "[234]\tvalid_0's binary_logloss: 0.151789\n",
      "[235]\tvalid_0's binary_logloss: 0.151787\n",
      "[236]\tvalid_0's binary_logloss: 0.151789\n",
      "[237]\tvalid_0's binary_logloss: 0.151786\n",
      "[238]\tvalid_0's binary_logloss: 0.151783\n",
      "[239]\tvalid_0's binary_logloss: 0.151781\n",
      "[240]\tvalid_0's binary_logloss: 0.151779\n",
      "[241]\tvalid_0's binary_logloss: 0.15178\n",
      "[242]\tvalid_0's binary_logloss: 0.151774\n",
      "[243]\tvalid_0's binary_logloss: 0.151771\n",
      "[244]\tvalid_0's binary_logloss: 0.15177\n",
      "[245]\tvalid_0's binary_logloss: 0.151761\n",
      "[246]\tvalid_0's binary_logloss: 0.151751\n",
      "[247]\tvalid_0's binary_logloss: 0.15175\n",
      "[248]\tvalid_0's binary_logloss: 0.151745\n",
      "[249]\tvalid_0's binary_logloss: 0.15174\n",
      "[250]\tvalid_0's binary_logloss: 0.151739\n",
      "[251]\tvalid_0's binary_logloss: 0.151734\n",
      "[252]\tvalid_0's binary_logloss: 0.151729\n",
      "[253]\tvalid_0's binary_logloss: 0.151724\n",
      "[254]\tvalid_0's binary_logloss: 0.151722\n",
      "[255]\tvalid_0's binary_logloss: 0.151724\n",
      "[256]\tvalid_0's binary_logloss: 0.151722\n",
      "[257]\tvalid_0's binary_logloss: 0.151719\n",
      "[258]\tvalid_0's binary_logloss: 0.151715\n",
      "[259]\tvalid_0's binary_logloss: 0.151709\n",
      "[260]\tvalid_0's binary_logloss: 0.151708\n",
      "[261]\tvalid_0's binary_logloss: 0.151705\n",
      "[262]\tvalid_0's binary_logloss: 0.151701\n",
      "[263]\tvalid_0's binary_logloss: 0.151701\n",
      "[264]\tvalid_0's binary_logloss: 0.151698\n",
      "[265]\tvalid_0's binary_logloss: 0.151692\n",
      "[266]\tvalid_0's binary_logloss: 0.15169\n",
      "[267]\tvalid_0's binary_logloss: 0.151688\n",
      "[268]\tvalid_0's binary_logloss: 0.151684\n",
      "[269]\tvalid_0's binary_logloss: 0.151685\n",
      "[270]\tvalid_0's binary_logloss: 0.151683\n",
      "[271]\tvalid_0's binary_logloss: 0.151676\n",
      "[272]\tvalid_0's binary_logloss: 0.151676\n",
      "[273]\tvalid_0's binary_logloss: 0.151679\n",
      "[274]\tvalid_0's binary_logloss: 0.15168\n",
      "[275]\tvalid_0's binary_logloss: 0.151682\n",
      "[276]\tvalid_0's binary_logloss: 0.151678\n",
      "[277]\tvalid_0's binary_logloss: 0.151675\n",
      "[278]\tvalid_0's binary_logloss: 0.15167\n",
      "[279]\tvalid_0's binary_logloss: 0.151665\n",
      "[280]\tvalid_0's binary_logloss: 0.151665\n",
      "[281]\tvalid_0's binary_logloss: 0.151666\n",
      "[282]\tvalid_0's binary_logloss: 0.151666\n",
      "[283]\tvalid_0's binary_logloss: 0.151665\n",
      "[284]\tvalid_0's binary_logloss: 0.151665\n",
      "Early stopping, best iteration is:\n",
      "[279]\tvalid_0's binary_logloss: 0.151665\n",
      "279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's binary_logloss: 0.667707\n",
      "Training until validation scores don't improve for 5 rounds.\n",
      "[2]\tvalid_0's binary_logloss: 0.643752\n",
      "[3]\tvalid_0's binary_logloss: 0.621155\n",
      "[4]\tvalid_0's binary_logloss: 0.599837\n",
      "[5]\tvalid_0's binary_logloss: 0.579675\n",
      "[6]\tvalid_0's binary_logloss: 0.560585\n",
      "[7]\tvalid_0's binary_logloss: 0.542504\n",
      "[8]\tvalid_0's binary_logloss: 0.525351\n",
      "[9]\tvalid_0's binary_logloss: 0.509068\n",
      "[10]\tvalid_0's binary_logloss: 0.49359\n",
      "[11]\tvalid_0's binary_logloss: 0.478869\n",
      "[12]\tvalid_0's binary_logloss: 0.464857\n",
      "[13]\tvalid_0's binary_logloss: 0.451513\n",
      "[14]\tvalid_0's binary_logloss: 0.438797\n",
      "[15]\tvalid_0's binary_logloss: 0.426672\n",
      "[16]\tvalid_0's binary_logloss: 0.415095\n",
      "[17]\tvalid_0's binary_logloss: 0.404044\n",
      "[18]\tvalid_0's binary_logloss: 0.393491\n",
      "[19]\tvalid_0's binary_logloss: 0.383417\n",
      "[20]\tvalid_0's binary_logloss: 0.373791\n",
      "[21]\tvalid_0's binary_logloss: 0.364586\n",
      "[22]\tvalid_0's binary_logloss: 0.35577\n",
      "[23]\tvalid_0's binary_logloss: 0.347334\n",
      "[24]\tvalid_0's binary_logloss: 0.339259\n",
      "[25]\tvalid_0's binary_logloss: 0.331533\n",
      "[26]\tvalid_0's binary_logloss: 0.324132\n",
      "[27]\tvalid_0's binary_logloss: 0.317049\n",
      "[28]\tvalid_0's binary_logloss: 0.310256\n",
      "[29]\tvalid_0's binary_logloss: 0.303747\n",
      "[30]\tvalid_0's binary_logloss: 0.297506\n",
      "[31]\tvalid_0's binary_logloss: 0.291538\n",
      "[32]\tvalid_0's binary_logloss: 0.285811\n",
      "[33]\tvalid_0's binary_logloss: 0.280318\n",
      "[34]\tvalid_0's binary_logloss: 0.275044\n",
      "[35]\tvalid_0's binary_logloss: 0.269995\n",
      "[36]\tvalid_0's binary_logloss: 0.265145\n",
      "[37]\tvalid_0's binary_logloss: 0.260484\n",
      "[38]\tvalid_0's binary_logloss: 0.256015\n",
      "[39]\tvalid_0's binary_logloss: 0.251728\n",
      "[40]\tvalid_0's binary_logloss: 0.24762\n",
      "[41]\tvalid_0's binary_logloss: 0.243672\n",
      "[42]\tvalid_0's binary_logloss: 0.239885\n",
      "[43]\tvalid_0's binary_logloss: 0.236252\n",
      "[44]\tvalid_0's binary_logloss: 0.232766\n",
      "[45]\tvalid_0's binary_logloss: 0.229422\n",
      "[46]\tvalid_0's binary_logloss: 0.226207\n",
      "[47]\tvalid_0's binary_logloss: 0.223122\n",
      "[48]\tvalid_0's binary_logloss: 0.220159\n",
      "[49]\tvalid_0's binary_logloss: 0.217323\n",
      "[50]\tvalid_0's binary_logloss: 0.214597\n",
      "[51]\tvalid_0's binary_logloss: 0.211986\n",
      "[52]\tvalid_0's binary_logloss: 0.20948\n",
      "[53]\tvalid_0's binary_logloss: 0.207073\n",
      "[54]\tvalid_0's binary_logloss: 0.204763\n",
      "[55]\tvalid_0's binary_logloss: 0.20254\n",
      "[56]\tvalid_0's binary_logloss: 0.200406\n",
      "[57]\tvalid_0's binary_logloss: 0.198356\n",
      "[58]\tvalid_0's binary_logloss: 0.196395\n",
      "[59]\tvalid_0's binary_logloss: 0.194514\n",
      "[60]\tvalid_0's binary_logloss: 0.192707\n",
      "[61]\tvalid_0's binary_logloss: 0.190985\n",
      "[62]\tvalid_0's binary_logloss: 0.189327\n",
      "[63]\tvalid_0's binary_logloss: 0.187737\n",
      "[64]\tvalid_0's binary_logloss: 0.186213\n",
      "[65]\tvalid_0's binary_logloss: 0.184747\n",
      "[66]\tvalid_0's binary_logloss: 0.183342\n",
      "[67]\tvalid_0's binary_logloss: 0.181999\n",
      "[68]\tvalid_0's binary_logloss: 0.18071\n",
      "[69]\tvalid_0's binary_logloss: 0.179476\n",
      "[70]\tvalid_0's binary_logloss: 0.178289\n",
      "[71]\tvalid_0's binary_logloss: 0.177152\n",
      "[72]\tvalid_0's binary_logloss: 0.176063\n",
      "[73]\tvalid_0's binary_logloss: 0.175016\n",
      "[74]\tvalid_0's binary_logloss: 0.174017\n",
      "[75]\tvalid_0's binary_logloss: 0.173063\n",
      "[76]\tvalid_0's binary_logloss: 0.172147\n",
      "[77]\tvalid_0's binary_logloss: 0.171271\n",
      "[78]\tvalid_0's binary_logloss: 0.170437\n",
      "[79]\tvalid_0's binary_logloss: 0.169635\n",
      "[80]\tvalid_0's binary_logloss: 0.168871\n",
      "[81]\tvalid_0's binary_logloss: 0.168132\n",
      "[82]\tvalid_0's binary_logloss: 0.16743\n",
      "[83]\tvalid_0's binary_logloss: 0.166749\n",
      "[84]\tvalid_0's binary_logloss: 0.166101\n",
      "[85]\tvalid_0's binary_logloss: 0.16548\n",
      "[86]\tvalid_0's binary_logloss: 0.164882\n",
      "[87]\tvalid_0's binary_logloss: 0.164316\n",
      "[88]\tvalid_0's binary_logloss: 0.163772\n",
      "[89]\tvalid_0's binary_logloss: 0.163255\n",
      "[90]\tvalid_0's binary_logloss: 0.162757\n",
      "[91]\tvalid_0's binary_logloss: 0.16229\n",
      "[92]\tvalid_0's binary_logloss: 0.161843\n",
      "[93]\tvalid_0's binary_logloss: 0.161415\n",
      "[94]\tvalid_0's binary_logloss: 0.161009\n",
      "[95]\tvalid_0's binary_logloss: 0.160613\n",
      "[96]\tvalid_0's binary_logloss: 0.160236\n",
      "[97]\tvalid_0's binary_logloss: 0.15987\n",
      "[98]\tvalid_0's binary_logloss: 0.15952\n",
      "[99]\tvalid_0's binary_logloss: 0.159185\n",
      "[100]\tvalid_0's binary_logloss: 0.158871\n",
      "[101]\tvalid_0's binary_logloss: 0.158569\n",
      "[102]\tvalid_0's binary_logloss: 0.158283\n",
      "[103]\tvalid_0's binary_logloss: 0.158006\n",
      "[104]\tvalid_0's binary_logloss: 0.157747\n",
      "[105]\tvalid_0's binary_logloss: 0.157498\n",
      "[106]\tvalid_0's binary_logloss: 0.157264\n",
      "[107]\tvalid_0's binary_logloss: 0.157038\n",
      "[108]\tvalid_0's binary_logloss: 0.156824\n",
      "[109]\tvalid_0's binary_logloss: 0.156621\n",
      "[110]\tvalid_0's binary_logloss: 0.156424\n",
      "[111]\tvalid_0's binary_logloss: 0.15623\n",
      "[112]\tvalid_0's binary_logloss: 0.156056\n",
      "[113]\tvalid_0's binary_logloss: 0.155883\n",
      "[114]\tvalid_0's binary_logloss: 0.155718\n",
      "[115]\tvalid_0's binary_logloss: 0.155564\n",
      "[116]\tvalid_0's binary_logloss: 0.155412\n",
      "[117]\tvalid_0's binary_logloss: 0.15527\n",
      "[118]\tvalid_0's binary_logloss: 0.15513\n",
      "[119]\tvalid_0's binary_logloss: 0.154997\n",
      "[120]\tvalid_0's binary_logloss: 0.154875\n",
      "[121]\tvalid_0's binary_logloss: 0.154758\n",
      "[122]\tvalid_0's binary_logloss: 0.154642\n",
      "[123]\tvalid_0's binary_logloss: 0.154535\n",
      "[124]\tvalid_0's binary_logloss: 0.154431\n",
      "[125]\tvalid_0's binary_logloss: 0.154337\n",
      "[126]\tvalid_0's binary_logloss: 0.154244\n",
      "[127]\tvalid_0's binary_logloss: 0.154155\n",
      "[128]\tvalid_0's binary_logloss: 0.15407\n",
      "[129]\tvalid_0's binary_logloss: 0.153987\n",
      "[130]\tvalid_0's binary_logloss: 0.153908\n",
      "[131]\tvalid_0's binary_logloss: 0.153831\n",
      "[132]\tvalid_0's binary_logloss: 0.153761\n",
      "[133]\tvalid_0's binary_logloss: 0.153688\n",
      "[134]\tvalid_0's binary_logloss: 0.153626\n",
      "[135]\tvalid_0's binary_logloss: 0.153561\n",
      "[136]\tvalid_0's binary_logloss: 0.153504\n",
      "[137]\tvalid_0's binary_logloss: 0.153449\n",
      "[138]\tvalid_0's binary_logloss: 0.153394\n",
      "[139]\tvalid_0's binary_logloss: 0.153342\n",
      "[140]\tvalid_0's binary_logloss: 0.153292\n",
      "[141]\tvalid_0's binary_logloss: 0.153249\n",
      "[142]\tvalid_0's binary_logloss: 0.153207\n",
      "[143]\tvalid_0's binary_logloss: 0.15317\n",
      "[144]\tvalid_0's binary_logloss: 0.153129\n",
      "[145]\tvalid_0's binary_logloss: 0.153089\n",
      "[146]\tvalid_0's binary_logloss: 0.153051\n",
      "[147]\tvalid_0's binary_logloss: 0.153014\n",
      "[148]\tvalid_0's binary_logloss: 0.15298\n",
      "[149]\tvalid_0's binary_logloss: 0.152948\n",
      "[150]\tvalid_0's binary_logloss: 0.152916\n",
      "[151]\tvalid_0's binary_logloss: 0.152884\n",
      "[152]\tvalid_0's binary_logloss: 0.15285\n",
      "[153]\tvalid_0's binary_logloss: 0.152824\n",
      "[154]\tvalid_0's binary_logloss: 0.152799\n",
      "[155]\tvalid_0's binary_logloss: 0.152772\n",
      "[156]\tvalid_0's binary_logloss: 0.152751\n",
      "[157]\tvalid_0's binary_logloss: 0.152729\n",
      "[158]\tvalid_0's binary_logloss: 0.152707\n",
      "[159]\tvalid_0's binary_logloss: 0.152685\n",
      "[160]\tvalid_0's binary_logloss: 0.152664\n",
      "[161]\tvalid_0's binary_logloss: 0.152639\n",
      "[162]\tvalid_0's binary_logloss: 0.152624\n",
      "[163]\tvalid_0's binary_logloss: 0.152602\n",
      "[164]\tvalid_0's binary_logloss: 0.152585\n",
      "[165]\tvalid_0's binary_logloss: 0.152567\n",
      "[166]\tvalid_0's binary_logloss: 0.152548\n",
      "[167]\tvalid_0's binary_logloss: 0.152528\n",
      "[168]\tvalid_0's binary_logloss: 0.15251\n",
      "[169]\tvalid_0's binary_logloss: 0.152498\n",
      "[170]\tvalid_0's binary_logloss: 0.152484\n",
      "[171]\tvalid_0's binary_logloss: 0.152473\n",
      "[172]\tvalid_0's binary_logloss: 0.152452\n",
      "[173]\tvalid_0's binary_logloss: 0.152439\n",
      "[174]\tvalid_0's binary_logloss: 0.152426\n",
      "[175]\tvalid_0's binary_logloss: 0.152418\n",
      "[176]\tvalid_0's binary_logloss: 0.152407\n",
      "[177]\tvalid_0's binary_logloss: 0.152397\n",
      "[178]\tvalid_0's binary_logloss: 0.152387\n",
      "[179]\tvalid_0's binary_logloss: 0.152378\n",
      "[180]\tvalid_0's binary_logloss: 0.152368\n",
      "[181]\tvalid_0's binary_logloss: 0.152359\n",
      "[182]\tvalid_0's binary_logloss: 0.152351\n",
      "[183]\tvalid_0's binary_logloss: 0.152337\n",
      "[184]\tvalid_0's binary_logloss: 0.152327\n",
      "[185]\tvalid_0's binary_logloss: 0.152317\n",
      "[186]\tvalid_0's binary_logloss: 0.152313\n",
      "[187]\tvalid_0's binary_logloss: 0.152303\n",
      "[188]\tvalid_0's binary_logloss: 0.152293\n",
      "[189]\tvalid_0's binary_logloss: 0.152284\n",
      "[190]\tvalid_0's binary_logloss: 0.15228\n",
      "[191]\tvalid_0's binary_logloss: 0.152276\n",
      "[192]\tvalid_0's binary_logloss: 0.152265\n",
      "[193]\tvalid_0's binary_logloss: 0.152259\n",
      "[194]\tvalid_0's binary_logloss: 0.152254\n",
      "[195]\tvalid_0's binary_logloss: 0.152245\n",
      "[196]\tvalid_0's binary_logloss: 0.152239\n",
      "[197]\tvalid_0's binary_logloss: 0.152233\n",
      "[198]\tvalid_0's binary_logloss: 0.152228\n",
      "[199]\tvalid_0's binary_logloss: 0.152224\n",
      "[200]\tvalid_0's binary_logloss: 0.152222\n",
      "[201]\tvalid_0's binary_logloss: 0.152215\n",
      "[202]\tvalid_0's binary_logloss: 0.152213\n",
      "[203]\tvalid_0's binary_logloss: 0.152208\n",
      "[204]\tvalid_0's binary_logloss: 0.152207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[205]\tvalid_0's binary_logloss: 0.152203\n",
      "[206]\tvalid_0's binary_logloss: 0.152202\n",
      "[207]\tvalid_0's binary_logloss: 0.152193\n",
      "[208]\tvalid_0's binary_logloss: 0.15219\n",
      "[209]\tvalid_0's binary_logloss: 0.152189\n",
      "[210]\tvalid_0's binary_logloss: 0.152186\n",
      "[211]\tvalid_0's binary_logloss: 0.152178\n",
      "[212]\tvalid_0's binary_logloss: 0.152173\n",
      "[213]\tvalid_0's binary_logloss: 0.152162\n",
      "[214]\tvalid_0's binary_logloss: 0.152153\n",
      "[215]\tvalid_0's binary_logloss: 0.152156\n",
      "[216]\tvalid_0's binary_logloss: 0.152149\n",
      "[217]\tvalid_0's binary_logloss: 0.152149\n",
      "[218]\tvalid_0's binary_logloss: 0.152144\n",
      "[219]\tvalid_0's binary_logloss: 0.152138\n",
      "[220]\tvalid_0's binary_logloss: 0.152133\n",
      "[221]\tvalid_0's binary_logloss: 0.152133\n",
      "[222]\tvalid_0's binary_logloss: 0.152127\n",
      "[223]\tvalid_0's binary_logloss: 0.152124\n",
      "[224]\tvalid_0's binary_logloss: 0.152118\n",
      "[225]\tvalid_0's binary_logloss: 0.152115\n",
      "[226]\tvalid_0's binary_logloss: 0.152116\n",
      "[227]\tvalid_0's binary_logloss: 0.152114\n",
      "[228]\tvalid_0's binary_logloss: 0.152111\n",
      "[229]\tvalid_0's binary_logloss: 0.152105\n",
      "[230]\tvalid_0's binary_logloss: 0.1521\n",
      "[231]\tvalid_0's binary_logloss: 0.152097\n",
      "[232]\tvalid_0's binary_logloss: 0.152098\n",
      "[233]\tvalid_0's binary_logloss: 0.152093\n",
      "[234]\tvalid_0's binary_logloss: 0.152086\n",
      "[235]\tvalid_0's binary_logloss: 0.152081\n",
      "[236]\tvalid_0's binary_logloss: 0.152078\n",
      "[237]\tvalid_0's binary_logloss: 0.152073\n",
      "[238]\tvalid_0's binary_logloss: 0.152075\n",
      "[239]\tvalid_0's binary_logloss: 0.152071\n",
      "[240]\tvalid_0's binary_logloss: 0.152073\n",
      "[241]\tvalid_0's binary_logloss: 0.152069\n",
      "[242]\tvalid_0's binary_logloss: 0.152066\n",
      "[243]\tvalid_0's binary_logloss: 0.152064\n",
      "[244]\tvalid_0's binary_logloss: 0.152063\n",
      "[245]\tvalid_0's binary_logloss: 0.152066\n",
      "[246]\tvalid_0's binary_logloss: 0.152064\n",
      "[247]\tvalid_0's binary_logloss: 0.152061\n",
      "[248]\tvalid_0's binary_logloss: 0.152061\n",
      "[249]\tvalid_0's binary_logloss: 0.152057\n",
      "[250]\tvalid_0's binary_logloss: 0.152053\n",
      "[251]\tvalid_0's binary_logloss: 0.152048\n",
      "[252]\tvalid_0's binary_logloss: 0.152046\n",
      "[253]\tvalid_0's binary_logloss: 0.152046\n",
      "[254]\tvalid_0's binary_logloss: 0.152046\n",
      "[255]\tvalid_0's binary_logloss: 0.152046\n",
      "[256]\tvalid_0's binary_logloss: 0.152041\n",
      "[257]\tvalid_0's binary_logloss: 0.152041\n",
      "[258]\tvalid_0's binary_logloss: 0.152043\n",
      "[259]\tvalid_0's binary_logloss: 0.152039\n",
      "[260]\tvalid_0's binary_logloss: 0.15204\n",
      "[261]\tvalid_0's binary_logloss: 0.152039\n",
      "[262]\tvalid_0's binary_logloss: 0.152037\n",
      "[263]\tvalid_0's binary_logloss: 0.152034\n",
      "[264]\tvalid_0's binary_logloss: 0.152031\n",
      "[265]\tvalid_0's binary_logloss: 0.152029\n",
      "[266]\tvalid_0's binary_logloss: 0.152024\n",
      "[267]\tvalid_0's binary_logloss: 0.152023\n",
      "[268]\tvalid_0's binary_logloss: 0.152017\n",
      "[269]\tvalid_0's binary_logloss: 0.152016\n",
      "[270]\tvalid_0's binary_logloss: 0.152016\n",
      "[271]\tvalid_0's binary_logloss: 0.152016\n",
      "[272]\tvalid_0's binary_logloss: 0.152014\n",
      "[273]\tvalid_0's binary_logloss: 0.152014\n",
      "[274]\tvalid_0's binary_logloss: 0.15201\n",
      "[275]\tvalid_0's binary_logloss: 0.152011\n",
      "[276]\tvalid_0's binary_logloss: 0.152009\n",
      "[277]\tvalid_0's binary_logloss: 0.152006\n",
      "[278]\tvalid_0's binary_logloss: 0.152006\n",
      "[279]\tvalid_0's binary_logloss: 0.152002\n",
      "[280]\tvalid_0's binary_logloss: 0.152003\n",
      "[281]\tvalid_0's binary_logloss: 0.152\n",
      "[282]\tvalid_0's binary_logloss: 0.152\n",
      "[283]\tvalid_0's binary_logloss: 0.151996\n",
      "[284]\tvalid_0's binary_logloss: 0.151994\n",
      "[285]\tvalid_0's binary_logloss: 0.151994\n",
      "[286]\tvalid_0's binary_logloss: 0.151995\n",
      "[287]\tvalid_0's binary_logloss: 0.151999\n",
      "[288]\tvalid_0's binary_logloss: 0.151996\n",
      "[289]\tvalid_0's binary_logloss: 0.151997\n",
      "[290]\tvalid_0's binary_logloss: 0.151993\n",
      "[291]\tvalid_0's binary_logloss: 0.151994\n",
      "[292]\tvalid_0's binary_logloss: 0.151994\n",
      "[293]\tvalid_0's binary_logloss: 0.151996\n",
      "[294]\tvalid_0's binary_logloss: 0.151997\n",
      "[295]\tvalid_0's binary_logloss: 0.151995\n",
      "Early stopping, best iteration is:\n",
      "[290]\tvalid_0's binary_logloss: 0.151993\n",
      "290\n",
      "Epoch 0: iteration 0, the loss is [ 0.72247279]\n",
      "  acc for train: 0.04969153847704683, acc for val: 0.05007\n",
      "  auc for train: 0.36552702872453, auc for val: 0.36566420300961733\n",
      "--------------------------------------------------------------\n",
      "Epoch 0: iteration 100, the loss is [ 0.58777219]\n",
      "  acc for train: 0.9635138404467651, acc for val: 0.96325\n",
      "  auc for train: 0.637001587426101, auc for val: 0.638487991130709\n",
      "--------------------------------------------------------------\n",
      "Epoch 0: iteration 200, the loss is [ 0.45317721]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6400128945037775, auc for val: 0.6412278975293625\n",
      "--------------------------------------------------------------\n",
      "Epoch 0: iteration 300, the loss is [ 0.34829822]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405046080885026, auc for val: 0.6420240388881788\n",
      "--------------------------------------------------------------\n",
      "Epoch 0: iteration 400, the loss is [ 0.2827341]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.640432592597574, auc for val: 0.6420553108254246\n",
      "--------------------------------------------------------------\n",
      "Epoch 0: iteration 500, the loss is [ 0.26001525]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6199091616163008, auc for val: 0.6200311670800377\n",
      "--------------------------------------------------------------\n",
      "Epoch 0: iteration 600, the loss is [ 0.22523136]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.640410227764923, auc for val: 0.6419640584726661\n",
      "--------------------------------------------------------------\n",
      "Epoch 0: iteration 700, the loss is [ 0.23436423]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6398873623050161, auc for val: 0.6413641313054421\n",
      "--------------------------------------------------------------\n",
      "Epoch 0: iteration 800, the loss is [ 0.20433676]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6397292648214875, auc for val: 0.6410481485775961\n",
      "--------------------------------------------------------------\n",
      "Epoch 0: iteration 900, the loss is [ 0.17708814]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.638658609652752, auc for val: 0.6393922884010743\n",
      "--------------------------------------------------------------\n",
      "Epoch 0: iteration 1000, the loss is [ 0.20034319]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6395079243419435, auc for val: 0.6398739118657466\n",
      "--------------------------------------------------------------\n",
      "Epoch 0: iteration 1100, the loss is [ 0.18906331]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6393616656455787, auc for val: 0.640431040720881\n",
      "--------------------------------------------------------------\n",
      "Epoch 1: iteration 0, the loss is [ 0.17432883]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6394546635435882, auc for val: 0.6409275769278839\n",
      "--------------------------------------------------------------\n",
      "Epoch 1: iteration 100, the loss is [ 0.20699655]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6383376455585984, auc for val: 0.6391800656697392\n",
      "--------------------------------------------------------------\n",
      "Epoch 1: iteration 200, the loss is [ 0.17268051]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6374755653748695, auc for val: 0.6387538881401914\n",
      "--------------------------------------------------------------\n",
      "Epoch 1: iteration 300, the loss is [ 0.17418733]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6366167189819315, auc for val: 0.6370663276049843\n",
      "--------------------------------------------------------------\n",
      "Epoch 1: iteration 400, the loss is [ 0.20673846]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6354804519264797, auc for val: 0.6354196000161856\n",
      "--------------------------------------------------------------\n",
      "Epoch 1: iteration 500, the loss is [ 0.17652783]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6345254162418047, auc for val: 0.6344885596332344\n",
      "--------------------------------------------------------------\n",
      "Epoch 1: iteration 600, the loss is [ 0.20300044]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6362403260121112, auc for val: 0.6367779151400288\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: iteration 700, the loss is [ 0.1631472]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6353695461519324, auc for val: 0.6356473717593167\n",
      "--------------------------------------------------------------\n",
      "Epoch 1: iteration 800, the loss is [ 0.21186188]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6348519210896066, auc for val: 0.6351094582420296\n",
      "--------------------------------------------------------------\n",
      "Epoch 1: iteration 900, the loss is [ 0.17485549]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6328981801020968, auc for val: 0.633874896115224\n",
      "--------------------------------------------------------------\n",
      "Epoch 1: iteration 1000, the loss is [ 0.1812806]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6344001573935877, auc for val: 0.6347850668597186\n",
      "--------------------------------------------------------------\n",
      "Epoch 1: iteration 1100, the loss is [ 0.17109264]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6357763376151857, auc for val: 0.6360070859944678\n",
      "--------------------------------------------------------------\n",
      "Epoch 2: iteration 0, the loss is [ 0.17120907]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6367569674223824, auc for val: 0.637145589813155\n",
      "--------------------------------------------------------------\n",
      "Epoch 2: iteration 100, the loss is [ 0.2351366]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6365063049731312, auc for val: 0.6370730296433503\n",
      "--------------------------------------------------------------\n",
      "Epoch 2: iteration 200, the loss is [ 0.14822441]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6368030533516547, auc for val: 0.6375896988301626\n",
      "--------------------------------------------------------------\n",
      "Epoch 2: iteration 300, the loss is [ 0.18406808]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6358835267948489, auc for val: 0.6367088544522824\n",
      "--------------------------------------------------------------\n",
      "Epoch 2: iteration 400, the loss is [ 0.18339722]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6364324883102825, auc for val: 0.6373609034001588\n",
      "--------------------------------------------------------------\n",
      "Epoch 2: iteration 500, the loss is [ 0.18898121]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6366592266794371, auc for val: 0.6378900095341162\n",
      "--------------------------------------------------------------\n",
      "Epoch 2: iteration 600, the loss is [ 0.14263979]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6348746178249549, auc for val: 0.6361359786348518\n",
      "--------------------------------------------------------------\n",
      "Epoch 2: iteration 700, the loss is [ 0.16222811]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6332880539315002, auc for val: 0.6355723442779193\n",
      "--------------------------------------------------------------\n",
      "Epoch 2: iteration 800, the loss is [ 0.13749351]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6359246648461836, auc for val: 0.6377005171759402\n",
      "--------------------------------------------------------------\n",
      "Epoch 2: iteration 900, the loss is [ 0.17858192]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6361670984072276, auc for val: 0.6377124733861561\n",
      "--------------------------------------------------------------\n",
      "Epoch 2: iteration 1000, the loss is [ 0.21447606]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6356070066781085, auc for val: 0.6374810494562952\n",
      "--------------------------------------------------------------\n",
      "Epoch 2: iteration 1100, the loss is [ 0.20225328]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6362544155408132, auc for val: 0.6375061764444396\n",
      "--------------------------------------------------------------\n",
      "Epoch 3: iteration 0, the loss is [ 0.18670478]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6351655568815134, auc for val: 0.6366937324479942\n",
      "--------------------------------------------------------------\n",
      "Epoch 3: iteration 100, the loss is [ 0.13674915]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6383214422047511, auc for val: 0.6395211372095617\n",
      "--------------------------------------------------------------\n",
      "Epoch 3: iteration 200, the loss is [ 0.17375292]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6376218781626533, auc for val: 0.6397138137429279\n",
      "--------------------------------------------------------------\n",
      "Epoch 3: iteration 300, the loss is [ 0.09703524]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6375380171733669, auc for val: 0.6393978875723675\n",
      "--------------------------------------------------------------\n",
      "Epoch 3: iteration 400, the loss is [ 0.13804993]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6378567329371897, auc for val: 0.6396190166622826\n",
      "--------------------------------------------------------------\n",
      "Epoch 3: iteration 500, the loss is [ 0.15447505]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6385228937597658, auc for val: 0.6403717078862905\n",
      "--------------------------------------------------------------\n",
      "Epoch 3: iteration 600, the loss is [ 0.16196111]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6390300449792446, auc for val: 0.640638426390349\n",
      "--------------------------------------------------------------\n",
      "Epoch 3: iteration 700, the loss is [ 0.17525582]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6383591575070087, auc for val: 0.6402372061770883\n",
      "--------------------------------------------------------------\n",
      "Epoch 3: iteration 800, the loss is [ 0.16689587]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.639560796534041, auc for val: 0.6413039388001076\n",
      "--------------------------------------------------------------\n",
      "Epoch 3: iteration 900, the loss is [ 0.1491386]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6388759187141005, auc for val: 0.640394568341207\n",
      "--------------------------------------------------------------\n",
      "Epoch 3: iteration 1000, the loss is [ 0.1680368]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6395080885451665, auc for val: 0.6411901780615369\n",
      "--------------------------------------------------------------\n",
      "Epoch 3: iteration 1100, the loss is [ 0.14292148]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6392280765178842, auc for val: 0.6412628032722202\n",
      "--------------------------------------------------------------\n",
      "Epoch 4: iteration 0, the loss is [ 0.20234276]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6386838509352588, auc for val: 0.6411874986601226\n",
      "--------------------------------------------------------------\n",
      "Epoch 4: iteration 100, the loss is [ 0.17712633]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6387324705612563, auc for val: 0.6402813109624798\n",
      "--------------------------------------------------------------\n",
      "Epoch 4: iteration 200, the loss is [ 0.14423946]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6403692855036287, auc for val: 0.6422408399317988\n",
      "--------------------------------------------------------------\n",
      "Epoch 4: iteration 300, the loss is [ 0.15474488]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6394215386577726, auc for val: 0.6419781864826565\n",
      "--------------------------------------------------------------\n",
      "Epoch 4: iteration 400, the loss is [ 0.14536689]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6392422299033953, auc for val: 0.6411491415088996\n",
      "--------------------------------------------------------------\n",
      "Epoch 4: iteration 500, the loss is [ 0.19987699]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6400049568894234, auc for val: 0.6421401707909564\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: iteration 600, the loss is [ 0.17123534]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405515372169992, auc for val: 0.6423154587868063\n",
      "--------------------------------------------------------------\n",
      "Epoch 4: iteration 700, the loss is [ 0.14114846]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6401530472996687, auc for val: 0.6418341209364272\n",
      "--------------------------------------------------------------\n",
      "Epoch 4: iteration 800, the loss is [ 0.17291428]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6403812612473724, auc for val: 0.6423939772666689\n",
      "--------------------------------------------------------------\n",
      "Epoch 4: iteration 900, the loss is [ 0.13747206]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6409457073357301, auc for val: 0.6427124216503789\n",
      "--------------------------------------------------------------\n",
      "Epoch 4: iteration 1000, the loss is [ 0.16225606]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6408920212807647, auc for val: 0.6424298586227064\n",
      "--------------------------------------------------------------\n",
      "Epoch 4: iteration 1100, the loss is [ 0.13308357]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406768512014918, auc for val: 0.6422604115805465\n",
      "--------------------------------------------------------------\n",
      "Epoch 5: iteration 0, the loss is [ 0.15339194]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6407659963115864, auc for val: 0.6421767647687948\n",
      "--------------------------------------------------------------\n",
      "Epoch 5: iteration 100, the loss is [ 0.16623931]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.640798062235303, auc for val: 0.6422663076775902\n",
      "--------------------------------------------------------------\n",
      "Epoch 5: iteration 200, the loss is [ 0.19171186]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6408172371611375, auc for val: 0.6422430612191982\n",
      "--------------------------------------------------------------\n",
      "Epoch 5: iteration 300, the loss is [ 0.13950539]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6409837719817517, auc for val: 0.6425026492634506\n",
      "--------------------------------------------------------------\n",
      "Epoch 5: iteration 400, the loss is [ 0.1350445]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6409532077178107, auc for val: 0.6424002183631559\n",
      "--------------------------------------------------------------\n",
      "Epoch 5: iteration 500, the loss is [ 0.16066173]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.640804707241562, auc for val: 0.642221139615226\n",
      "--------------------------------------------------------------\n",
      "Epoch 5: iteration 600, the loss is [ 0.13858406]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6407737896553649, auc for val: 0.6422322121178514\n",
      "--------------------------------------------------------------\n",
      "Epoch 5: iteration 700, the loss is [ 0.15810077]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6408766123795656, auc for val: 0.6422707078344246\n",
      "--------------------------------------------------------------\n",
      "Epoch 5: iteration 800, the loss is [ 0.13905405]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6409779671607074, auc for val: 0.6423307801555269\n",
      "--------------------------------------------------------------\n",
      "Epoch 5: iteration 900, the loss is [ 0.1246593]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6409811909048012, auc for val: 0.6423578131242024\n",
      "--------------------------------------------------------------\n",
      "Epoch 5: iteration 1000, the loss is [ 0.15033671]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6409365611438586, auc for val: 0.6423140844447616\n",
      "--------------------------------------------------------------\n",
      "Epoch 5: iteration 1100, the loss is [ 0.13791506]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6410329492798317, auc for val: 0.642488078692699\n",
      "--------------------------------------------------------------\n",
      "Epoch 6: iteration 0, the loss is [ 0.14242536]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6411240732678313, auc for val: 0.6427424549830658\n",
      "--------------------------------------------------------------\n",
      "Epoch 6: iteration 100, the loss is [ 0.16394952]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6411374137951443, auc for val: 0.6427282336535538\n",
      "--------------------------------------------------------------\n",
      "Epoch 6: iteration 200, the loss is [ 0.1748337]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6410626565604794, auc for val: 0.6426064205714187\n",
      "--------------------------------------------------------------\n",
      "Epoch 6: iteration 300, the loss is [ 0.16926463]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6411669034643253, auc for val: 0.6427821115379297\n",
      "--------------------------------------------------------------\n",
      "Epoch 6: iteration 400, the loss is [ 0.15644161]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405657941236808, auc for val: 0.6425178009603139\n",
      "--------------------------------------------------------------\n",
      "Epoch 6: iteration 500, the loss is [ 0.14844979]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6410808737145519, auc for val: 0.6426232930237021\n",
      "--------------------------------------------------------------\n",
      "Epoch 6: iteration 600, the loss is [ 0.18768999]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6411205879479227, auc for val: 0.6427269328359806\n",
      "--------------------------------------------------------------\n",
      "Epoch 6: iteration 700, the loss is [ 0.15946788]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6411078023198036, auc for val: 0.6427117514465424\n",
      "--------------------------------------------------------------\n",
      "Epoch 6: iteration 800, the loss is [ 0.11606201]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6412106882980115, auc for val: 0.6428062657407586\n",
      "--------------------------------------------------------------\n",
      "Epoch 6: iteration 900, the loss is [ 0.2036539]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6409607746492154, auc for val: 0.6425436631931736\n",
      "--------------------------------------------------------------\n",
      "Epoch 6: iteration 1000, the loss is [ 0.18226603]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6409917986500321, auc for val: 0.6424387211493897\n",
      "--------------------------------------------------------------\n",
      "Epoch 6: iteration 1100, the loss is [ 0.11255697]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6409377103253004, auc for val: 0.6428229699351169\n",
      "--------------------------------------------------------------\n",
      "Epoch 7: iteration 0, the loss is [ 0.13988885]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6408934672014629, auc for val: 0.6426690082778063\n",
      "--------------------------------------------------------------\n",
      "Epoch 7: iteration 100, the loss is [ 0.17109549]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6408753900179793, auc for val: 0.6426074810205273\n",
      "--------------------------------------------------------------\n",
      "Epoch 7: iteration 200, the loss is [ 0.11455124]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6408723243285303, auc for val: 0.6425552927850643\n",
      "--------------------------------------------------------------\n",
      "Epoch 7: iteration 300, the loss is [ 0.1579411]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6408147235176201, auc for val: 0.6426009797605257\n",
      "--------------------------------------------------------------\n",
      "Epoch 7: iteration 400, the loss is [ 0.1529516]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6409223949789482, auc for val: 0.6426626936568478\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: iteration 500, the loss is [ 0.15307431]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.64088473489334, auc for val: 0.6426355871637007\n",
      "--------------------------------------------------------------\n",
      "Epoch 7: iteration 600, the loss is [ 0.14458653]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6409408652696099, auc for val: 0.6427008783083492\n",
      "--------------------------------------------------------------\n",
      "Epoch 7: iteration 700, the loss is [ 0.19057295]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6409441255836822, auc for val: 0.6427465751813357\n",
      "--------------------------------------------------------------\n",
      "Epoch 7: iteration 800, the loss is [ 0.15651868]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6408814534008294, auc for val: 0.642540189161894\n",
      "--------------------------------------------------------------\n",
      "Epoch 7: iteration 900, the loss is [ 0.13257293]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6408479677983882, auc for val: 0.6425318257532577\n",
      "--------------------------------------------------------------\n",
      "Epoch 7: iteration 1000, the loss is [ 0.17815001]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.640891967872521, auc for val: 0.6425525596542284\n",
      "--------------------------------------------------------------\n",
      "Epoch 7: iteration 1100, the loss is [ 0.13419841]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6410293618854769, auc for val: 0.6426289826866526\n",
      "--------------------------------------------------------------\n",
      "Epoch 8: iteration 0, the loss is [ 0.16746075]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6409953062736091, auc for val: 0.6426245584929716\n",
      "--------------------------------------------------------------\n",
      "Epoch 8: iteration 100, the loss is [ 0.17803469]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6407545431166709, auc for val: 0.6423202548446412\n",
      "--------------------------------------------------------------\n",
      "Epoch 8: iteration 200, the loss is [ 0.14755419]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6410019836704207, auc for val: 0.6426196012468721\n",
      "--------------------------------------------------------------\n",
      "Epoch 8: iteration 300, the loss is [ 0.16771948]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6407827100376644, auc for val: 0.6424264171118661\n",
      "--------------------------------------------------------------\n",
      "Epoch 8: iteration 400, the loss is [ 0.1963229]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406549533795254, auc for val: 0.6424320544593273\n",
      "--------------------------------------------------------------\n",
      "Epoch 8: iteration 500, the loss is [ 0.19328518]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.64091982877111, auc for val: 0.6426768216668383\n",
      "--------------------------------------------------------------\n",
      "Epoch 8: iteration 600, the loss is [ 0.1661804]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6408232055424161, auc for val: 0.6425136143072332\n",
      "--------------------------------------------------------------\n",
      "Epoch 8: iteration 700, the loss is [ 0.18596663]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6407670037542207, auc for val: 0.6423490594702942\n",
      "--------------------------------------------------------------\n",
      "Epoch 8: iteration 800, the loss is [ 0.15533747]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6407726583972312, auc for val: 0.6424483514412276\n",
      "--------------------------------------------------------------\n",
      "Epoch 8: iteration 900, the loss is [ 0.21142527]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6407132357583909, auc for val: 0.6422919592345604\n",
      "--------------------------------------------------------------\n",
      "Epoch 8: iteration 1000, the loss is [ 0.1610954]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.640883577071738, auc for val: 0.6424767955141837\n",
      "--------------------------------------------------------------\n",
      "Epoch 8: iteration 1100, the loss is [ 0.17755704]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.640938632652314, auc for val: 0.6426694041788069\n",
      "--------------------------------------------------------------\n",
      "Epoch 9: iteration 0, the loss is [ 0.15997182]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6407023220711289, auc for val: 0.642361150004064\n",
      "--------------------------------------------------------------\n",
      "Epoch 9: iteration 100, the loss is [ 0.14401951]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6408099476793299, auc for val: 0.6423894215772985\n",
      "--------------------------------------------------------------\n",
      "Epoch 9: iteration 200, the loss is [ 0.15320213]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406326257196548, auc for val: 0.642391497229687\n",
      "--------------------------------------------------------------\n",
      "Epoch 9: iteration 300, the loss is [ 0.13727029]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405604541030478, auc for val: 0.6422636466572939\n",
      "--------------------------------------------------------------\n",
      "Epoch 9: iteration 400, the loss is [ 0.11719643]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6407300646598085, auc for val: 0.6424808351183213\n",
      "--------------------------------------------------------------\n",
      "Epoch 9: iteration 500, the loss is [ 0.15620805]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6407590801650561, auc for val: 0.6424586957687988\n",
      "--------------------------------------------------------------\n",
      "Epoch 9: iteration 600, the loss is [ 0.15733384]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6407751236558534, auc for val: 0.6425293612695294\n",
      "--------------------------------------------------------------\n",
      "Epoch 9: iteration 700, the loss is [ 0.13189709]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406554451854594, auc for val: 0.6423976534902452\n",
      "--------------------------------------------------------------\n",
      "Epoch 9: iteration 800, the loss is [ 0.16378015]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6407664110392575, auc for val: 0.6424868202930901\n",
      "--------------------------------------------------------------\n",
      "Epoch 9: iteration 900, the loss is [ 0.14509431]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6407394293472565, auc for val: 0.6425066238267096\n",
      "--------------------------------------------------------------\n",
      "Epoch 9: iteration 1000, the loss is [ 0.15907109]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6380755858142567, auc for val: 0.6394247678363715\n",
      "--------------------------------------------------------------\n",
      "Epoch 9: iteration 1100, the loss is [ 0.15534356]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406058245065271, auc for val: 0.6422884767196879\n",
      "--------------------------------------------------------------\n",
      "Epoch 10: iteration 0, the loss is [ 0.13209735]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405237164248264, auc for val: 0.6420740327009532\n",
      "--------------------------------------------------------------\n",
      "Epoch 10: iteration 100, the loss is [ 0.15674283]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6407566335531044, auc for val: 0.6423696222854755\n",
      "--------------------------------------------------------------\n",
      "Epoch 10: iteration 200, the loss is [ 0.19533157]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406935295230081, auc for val: 0.6422765459602504\n",
      "--------------------------------------------------------------\n",
      "Epoch 10: iteration 300, the loss is [ 0.16269761]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405293273044228, auc for val: 0.6421362414735261\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: iteration 400, the loss is [ 0.18062015]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405471488999229, auc for val: 0.6422062339425558\n",
      "--------------------------------------------------------------\n",
      "Epoch 10: iteration 500, the loss is [ 0.2074195]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6407434591579804, auc for val: 0.6423324740462363\n",
      "--------------------------------------------------------------\n",
      "Epoch 10: iteration 600, the loss is [ 0.15216368]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406104202276254, auc for val: 0.6422619768034308\n",
      "--------------------------------------------------------------\n",
      "Epoch 10: iteration 700, the loss is [ 0.18350489]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405069468792973, auc for val: 0.6422693151112621\n",
      "--------------------------------------------------------------\n",
      "Epoch 10: iteration 800, the loss is [ 0.13223393]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.640695943181792, auc for val: 0.642271297444129\n",
      "--------------------------------------------------------------\n",
      "Epoch 10: iteration 900, the loss is [ 0.14375997]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406857905519561, auc for val: 0.6422774353235694\n",
      "--------------------------------------------------------------\n",
      "Epoch 10: iteration 1000, the loss is [ 0.15264067]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6408116234684658, auc for val: 0.6424877336932555\n",
      "--------------------------------------------------------------\n",
      "Epoch 10: iteration 1100, the loss is [ 0.17244117]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6407781157231035, auc for val: 0.6423688403809995\n",
      "--------------------------------------------------------------\n",
      "Epoch 11: iteration 0, the loss is [ 0.14525069]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6407257858919216, auc for val: 0.6423047157303703\n",
      "--------------------------------------------------------------\n",
      "Epoch 11: iteration 100, the loss is [ 0.13057418]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406315797378891, auc for val: 0.6423336546795771\n",
      "--------------------------------------------------------------\n",
      "Epoch 11: iteration 200, the loss is [ 0.14351925]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406520276204781, auc for val: 0.6422959988386979\n",
      "--------------------------------------------------------------\n",
      "Epoch 11: iteration 300, the loss is [ 0.21185626]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6408650551410514, auc for val: 0.6425163106758333\n",
      "--------------------------------------------------------------\n",
      "Epoch 11: iteration 400, the loss is [ 0.11827825]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6408219850294222, auc for val: 0.6424841408916757\n",
      "--------------------------------------------------------------\n",
      "Epoch 11: iteration 500, the loss is [ 0.15959981]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.640801057959551, auc for val: 0.6424851674064129\n",
      "--------------------------------------------------------------\n",
      "Epoch 11: iteration 600, the loss is [ 0.13078694]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406511652521554, auc for val: 0.6423681461403163\n",
      "--------------------------------------------------------------\n",
      "Epoch 11: iteration 700, the loss is [ 0.15850058]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6407180520647788, auc for val: 0.6423898641380598\n",
      "--------------------------------------------------------------\n",
      "Epoch 11: iteration 800, the loss is [ 0.23624411]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.640803563766644, auc for val: 0.6426299314351216\n",
      "--------------------------------------------------------------\n",
      "Epoch 11: iteration 900, the loss is [ 0.17570594]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406368660689709, auc for val: 0.6423968503767871\n",
      "--------------------------------------------------------------\n",
      "Epoch 11: iteration 1000, the loss is [ 0.1649815]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404594309031089, auc for val: 0.6422111162502518\n",
      "--------------------------------------------------------------\n",
      "Epoch 11: iteration 1100, the loss is [ 0.13830599]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406830209587744, auc for val: 0.6423578626118276\n",
      "--------------------------------------------------------------\n",
      "Epoch 12: iteration 0, the loss is [ 0.14228882]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405852039814887, auc for val: 0.6418989879014323\n",
      "--------------------------------------------------------------\n",
      "Epoch 12: iteration 100, the loss is [ 0.11429729]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405670878971926, auc for val: 0.6420826195108684\n",
      "--------------------------------------------------------------\n",
      "Epoch 12: iteration 200, the loss is [ 0.15175554]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.640535352951877, auc for val: 0.6420765975738638\n",
      "--------------------------------------------------------------\n",
      "Epoch 12: iteration 300, the loss is [ 0.13468972]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.640684612476212, auc for val: 0.642342924418718\n",
      "--------------------------------------------------------------\n",
      "Epoch 12: iteration 400, the loss is [ 0.1673853]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406481073589435, auc for val: 0.6423807159970832\n",
      "--------------------------------------------------------------\n",
      "Epoch 12: iteration 500, the loss is [ 0.12314152]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404452513961845, auc for val: 0.6422088808235308\n",
      "--------------------------------------------------------------\n",
      "Epoch 12: iteration 600, the loss is [ 0.15927395]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.640667072710669, auc for val: 0.6423618202079007\n",
      "--------------------------------------------------------------\n",
      "Epoch 12: iteration 700, the loss is [ 0.17654403]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405258779918779, auc for val: 0.6421599304926793\n",
      "--------------------------------------------------------------\n",
      "Epoch 12: iteration 800, the loss is [ 0.14343126]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406620694553348, auc for val: 0.6421145927584577\n",
      "--------------------------------------------------------------\n",
      "Epoch 12: iteration 900, the loss is [ 0.13358666]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6403735268576143, auc for val: 0.6419314998571688\n",
      "--------------------------------------------------------------\n",
      "Epoch 12: iteration 1000, the loss is [ 0.19477367]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6381198584351979, auc for val: 0.6393461956270874\n",
      "--------------------------------------------------------------\n",
      "Epoch 12: iteration 1100, the loss is [ 0.12100589]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406942603599679, auc for val: 0.6424038408573107\n",
      "--------------------------------------------------------------\n",
      "Epoch 13: iteration 0, the loss is [ 0.15363295]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405175434927876, auc for val: 0.6421446006403659\n",
      "--------------------------------------------------------------\n",
      "Epoch 13: iteration 100, the loss is [ 0.18200076]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6407120913993637, auc for val: 0.6425003813162904\n",
      "--------------------------------------------------------------\n",
      "Epoch 13: iteration 200, the loss is [ 0.15107128]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405400714436185, auc for val: 0.6424064976358107\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: iteration 300, the loss is [ 0.16375928]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405888305981503, auc for val: 0.6424322439262347\n",
      "--------------------------------------------------------------\n",
      "Epoch 13: iteration 400, the loss is [ 0.10325548]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405790920128098, auc for val: 0.6424467452143112\n",
      "--------------------------------------------------------------\n",
      "Epoch 13: iteration 500, the loss is [ 0.13005342]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406352089265053, auc for val: 0.6422670712009484\n",
      "--------------------------------------------------------------\n",
      "Epoch 13: iteration 600, the loss is [ 0.16938886]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404639038937512, auc for val: 0.6422423966710902\n",
      "--------------------------------------------------------------\n",
      "Epoch 13: iteration 700, the loss is [ 0.10705065]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6403991264857262, auc for val: 0.6422006347712627\n",
      "--------------------------------------------------------------\n",
      "Epoch 13: iteration 800, the loss is [ 0.21661738]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.640457421041189, auc for val: 0.642173415163544\n",
      "--------------------------------------------------------------\n",
      "Epoch 13: iteration 900, the loss is [ 0.14278653]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406854325680322, auc for val: 0.6424574048487506\n",
      "--------------------------------------------------------------\n",
      "Epoch 13: iteration 1000, the loss is [ 0.17648004]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406286414405638, auc for val: 0.6423246366203578\n",
      "--------------------------------------------------------------\n",
      "Epoch 13: iteration 1100, the loss is [ 0.13791929]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404550042880222, auc for val: 0.6421296992094924\n",
      "--------------------------------------------------------------\n",
      "Epoch 14: iteration 0, the loss is [ 0.17269406]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404465150697908, auc for val: 0.6422666145008655\n",
      "--------------------------------------------------------------\n",
      "Epoch 14: iteration 100, the loss is [ 0.14912935]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406449733117647, auc for val: 0.6424391354315081\n",
      "--------------------------------------------------------------\n",
      "Epoch 14: iteration 200, the loss is [ 0.1008984]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6403141577073913, auc for val: 0.6419337197306361\n",
      "--------------------------------------------------------------\n",
      "Epoch 14: iteration 300, the loss is [ 0.14312145]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406680968710079, auc for val: 0.6423135584620037\n",
      "--------------------------------------------------------------\n",
      "Epoch 14: iteration 400, the loss is [ 0.12105093]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.640607488078879, auc for val: 0.6422399336012939\n",
      "--------------------------------------------------------------\n",
      "Epoch 14: iteration 500, the loss is [ 0.14065543]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406351260211579, auc for val: 0.6424329395808498\n",
      "--------------------------------------------------------------\n",
      "Epoch 14: iteration 600, the loss is [ 0.17022137]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406941999592226, auc for val: 0.642541605921903\n",
      "--------------------------------------------------------------\n",
      "Epoch 14: iteration 700, the loss is [ 0.17305729]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405014289919325, auc for val: 0.6422925092541647\n",
      "--------------------------------------------------------------\n",
      "Epoch 14: iteration 800, the loss is [ 0.15001243]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406278321188033, auc for val: 0.6423701892722655\n",
      "--------------------------------------------------------------\n",
      "Epoch 14: iteration 900, the loss is [ 0.13933241]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406396358228995, auc for val: 0.6423062031869866\n",
      "--------------------------------------------------------------\n",
      "Epoch 14: iteration 1000, the loss is [ 0.16182132]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405587251066022, auc for val: 0.6422113354097342\n",
      "--------------------------------------------------------------\n",
      "Epoch 14: iteration 1100, the loss is [ 0.19256936]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406365017355342, auc for val: 0.642315622802935\n",
      "--------------------------------------------------------------\n",
      "Epoch 15: iteration 0, the loss is [ 0.19544216]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406749558719229, auc for val: 0.6424105867275732\n",
      "--------------------------------------------------------------\n",
      "Epoch 15: iteration 100, the loss is [ 0.10737761]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406497283734849, auc for val: 0.642470810339415\n",
      "--------------------------------------------------------------\n",
      "Epoch 15: iteration 200, the loss is [ 0.20264049]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405637660169582, auc for val: 0.6422616247343268\n",
      "--------------------------------------------------------------\n",
      "Epoch 15: iteration 300, the loss is [ 0.17263043]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406760969758201, auc for val: 0.6424733087575147\n",
      "--------------------------------------------------------------\n",
      "Epoch 15: iteration 400, the loss is [ 0.17694439]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406127826884394, auc for val: 0.6423717361140318\n",
      "--------------------------------------------------------------\n",
      "Epoch 15: iteration 500, the loss is [ 0.15746371]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404271253857513, auc for val: 0.6421438851907006\n",
      "--------------------------------------------------------------\n",
      "Epoch 15: iteration 600, the loss is [ 0.13764095]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404227048790565, auc for val: 0.6421530036391022\n",
      "--------------------------------------------------------------\n",
      "Epoch 15: iteration 700, the loss is [ 0.12037686]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406296442617173, auc for val: 0.6423187080028748\n",
      "--------------------------------------------------------------\n",
      "Epoch 15: iteration 800, the loss is [ 0.13588424]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6407007905927553, auc for val: 0.6424730132456965\n",
      "--------------------------------------------------------------\n",
      "Epoch 15: iteration 900, the loss is [ 0.1285741]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406352041844642, auc for val: 0.6423830489851221\n",
      "--------------------------------------------------------------\n",
      "Epoch 15: iteration 1000, the loss is [ 0.20248355]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6403983546582406, auc for val: 0.6421431174255461\n",
      "--------------------------------------------------------------\n",
      "Epoch 15: iteration 1100, the loss is [ 0.17236745]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406292797675335, auc for val: 0.6423752200428365\n",
      "--------------------------------------------------------------\n",
      "Epoch 16: iteration 0, the loss is [ 0.15105547]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405213040520196, auc for val: 0.6422853547575123\n",
      "--------------------------------------------------------------\n",
      "Epoch 16: iteration 100, the loss is [ 0.16533762]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404574648046032, auc for val: 0.6421505052210024\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: iteration 200, the loss is [ 0.16945383]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404929058962375, auc for val: 0.6422084806807339\n",
      "--------------------------------------------------------------\n",
      "Epoch 16: iteration 300, the loss is [ 0.18169078]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406833878641657, auc for val: 0.6423753529524582\n",
      "--------------------------------------------------------------\n",
      "Epoch 16: iteration 400, the loss is [ 0.17143849]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6403100451118803, auc for val: 0.642044757235896\n",
      "--------------------------------------------------------------\n",
      "Epoch 16: iteration 500, the loss is [ 0.13725767]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406577419006849, auc for val: 0.6423744918677821\n",
      "--------------------------------------------------------------\n",
      "Epoch 16: iteration 600, the loss is [ 0.18201628]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.640267337324348, auc for val: 0.6419831196919096\n",
      "--------------------------------------------------------------\n",
      "Epoch 16: iteration 700, the loss is [ 0.15074259]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.640409106312366, auc for val: 0.6420804180185189\n",
      "--------------------------------------------------------------\n",
      "Epoch 16: iteration 800, the loss is [ 0.17778239]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.640441338127151, auc for val: 0.6421034566288856\n",
      "--------------------------------------------------------------\n",
      "Epoch 16: iteration 900, the loss is [ 0.10973709]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404571973615169, auc for val: 0.6421440096167295\n",
      "--------------------------------------------------------------\n",
      "Epoch 16: iteration 1000, the loss is [ 0.16609739]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.639187104558552, auc for val: 0.6406470966222605\n",
      "--------------------------------------------------------------\n",
      "Epoch 16: iteration 1100, the loss is [ 0.18494825]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405084888062365, auc for val: 0.6421974209034976\n",
      "--------------------------------------------------------------\n",
      "Epoch 17: iteration 0, the loss is [ 0.15892294]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404351771305886, auc for val: 0.6421528382090413\n",
      "--------------------------------------------------------------\n",
      "Epoch 17: iteration 100, the loss is [ 0.16307306]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404434707597139, auc for val: 0.6420942929346554\n",
      "--------------------------------------------------------------\n",
      "Epoch 17: iteration 200, the loss is [ 0.17755529]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404427867403641, auc for val: 0.6421969712730756\n",
      "--------------------------------------------------------------\n",
      "Epoch 17: iteration 300, the loss is [ 0.16980208]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404719426984421, auc for val: 0.6421925202146839\n",
      "--------------------------------------------------------------\n",
      "Epoch 17: iteration 400, the loss is [ 0.19205388]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406628288651023, auc for val: 0.6424209833706338\n",
      "--------------------------------------------------------------\n",
      "Epoch 17: iteration 500, the loss is [ 0.14261393]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406412297515448, auc for val: 0.6423741850445065\n",
      "--------------------------------------------------------------\n",
      "Epoch 17: iteration 600, the loss is [ 0.11582474]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6407250727371493, auc for val: 0.6424608067694908\n",
      "--------------------------------------------------------------\n",
      "Epoch 17: iteration 700, the loss is [ 0.21950567]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405272234054147, auc for val: 0.6422227981576316\n",
      "--------------------------------------------------------------\n",
      "Epoch 17: iteration 800, the loss is [ 0.12273701]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404786575895286, auc for val: 0.6421737983391552\n",
      "--------------------------------------------------------------\n",
      "Epoch 17: iteration 900, the loss is [ 0.16893728]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.640473800172055, auc for val: 0.6421744529897382\n",
      "--------------------------------------------------------------\n",
      "Epoch 17: iteration 1000, the loss is [ 0.14342457]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404428235514635, auc for val: 0.6421523334352656\n",
      "--------------------------------------------------------------\n",
      "Epoch 17: iteration 1100, the loss is [ 0.11505789]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406837555331057, auc for val: 0.6423813876148519\n",
      "--------------------------------------------------------------\n",
      "Epoch 18: iteration 0, the loss is [ 0.14237553]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406363062669885, auc for val: 0.6422453079573763\n",
      "--------------------------------------------------------------\n",
      "Epoch 18: iteration 100, the loss is [ 0.19696572]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404012492725254, auc for val: 0.6420022558495568\n",
      "--------------------------------------------------------------\n",
      "Epoch 18: iteration 200, the loss is [ 0.15774129]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406537921420463, auc for val: 0.6424016421928256\n",
      "--------------------------------------------------------------\n",
      "Epoch 18: iteration 300, the loss is [ 0.12407375]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406741873397542, auc for val: 0.6424130512113017\n",
      "--------------------------------------------------------------\n",
      "Epoch 18: iteration 400, the loss is [ 0.16659661]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406573255655423, auc for val: 0.6424546929268968\n",
      "--------------------------------------------------------------\n",
      "Epoch 18: iteration 500, the loss is [ 0.10871743]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406797903427397, auc for val: 0.6425014643883133\n",
      "--------------------------------------------------------------\n",
      "Epoch 18: iteration 600, the loss is [ 0.18043154]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404554145549596, auc for val: 0.6422335638369818\n",
      "--------------------------------------------------------------\n",
      "Epoch 18: iteration 700, the loss is [ 0.17001489]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405709165730336, auc for val: 0.6423922155072164\n",
      "--------------------------------------------------------------\n",
      "Epoch 18: iteration 800, the loss is [ 0.12340672]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405380255341482, auc for val: 0.6423743434049067\n",
      "--------------------------------------------------------------\n",
      "Epoch 18: iteration 900, the loss is [ 0.14707464]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405237354733648, auc for val: 0.6423601772187484\n",
      "--------------------------------------------------------------\n",
      "Epoch 18: iteration 1000, the loss is [ 0.15531576]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404321138925335, auc for val: 0.6420625289490234\n",
      "--------------------------------------------------------------\n",
      "Epoch 18: iteration 1100, the loss is [ 0.16794075]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6403392499772815, auc for val: 0.6420116330475408\n",
      "--------------------------------------------------------------\n",
      "Epoch 19: iteration 0, the loss is [ 0.17925996]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6403813075023672, auc for val: 0.6421384655887898\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: iteration 100, the loss is [ 0.14438835]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.640269232252049, auc for val: 0.6419210636240082\n",
      "--------------------------------------------------------------\n",
      "Epoch 19: iteration 200, the loss is [ 0.13145542]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6402853019044465, auc for val: 0.6419544239390318\n",
      "--------------------------------------------------------------\n",
      "Epoch 19: iteration 300, the loss is [ 0.17170766]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406253638059849, auc for val: 0.6423879327067501\n",
      "--------------------------------------------------------------\n",
      "Epoch 19: iteration 400, the loss is [ 0.16417311]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6407717070151686, auc for val: 0.6424787736052543\n",
      "--------------------------------------------------------------\n",
      "Epoch 19: iteration 500, the loss is [ 0.11773927]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6407524746222246, auc for val: 0.6424687049944515\n",
      "--------------------------------------------------------------\n",
      "Epoch 19: iteration 600, the loss is [ 0.15439759]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.640518543058811, auc for val: 0.642330323455444\n",
      "--------------------------------------------------------------\n",
      "Epoch 19: iteration 700, the loss is [ 0.15987673]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406204330878066, auc for val: 0.6423153909180632\n",
      "--------------------------------------------------------------\n",
      "Epoch 19: iteration 800, the loss is [ 0.16472715]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6402655801167759, auc for val: 0.6419960232366627\n",
      "--------------------------------------------------------------\n",
      "Epoch 19: iteration 900, the loss is [ 0.1877851]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404767628627617, auc for val: 0.6422636848334616\n",
      "--------------------------------------------------------------\n",
      "Epoch 19: iteration 1000, the loss is [ 0.18315801]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405892091577104, auc for val: 0.6422463740622133\n",
      "--------------------------------------------------------------\n",
      "Epoch 19: iteration 1100, the loss is [ 0.15377249]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6402493609695203, auc for val: 0.6418784194305225\n",
      "--------------------------------------------------------------\n",
      "Epoch 20: iteration 0, the loss is [ 0.16811641]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6403598728733406, auc for val: 0.6419931331593588\n",
      "--------------------------------------------------------------\n",
      "Epoch 20: iteration 100, the loss is [ 0.13776918]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406063799683361, auc for val: 0.6422085725863232\n",
      "--------------------------------------------------------------\n",
      "Epoch 20: iteration 200, the loss is [ 0.17212503]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406532694726577, auc for val: 0.642331308966149\n",
      "--------------------------------------------------------------\n",
      "Epoch 20: iteration 300, the loss is [ 0.1806861]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406240314129681, auc for val: 0.6422019172077179\n",
      "--------------------------------------------------------------\n",
      "Epoch 20: iteration 400, the loss is [ 0.18343322]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404923997034326, auc for val: 0.6421938422412392\n",
      "--------------------------------------------------------------\n",
      "Epoch 20: iteration 500, the loss is [ 0.14653197]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6407147805385921, auc for val: 0.6423755197964512\n",
      "--------------------------------------------------------------\n",
      "Epoch 20: iteration 600, the loss is [ 0.2001601]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405589444862878, auc for val: 0.6422077199852401\n",
      "--------------------------------------------------------------\n",
      "Epoch 20: iteration 700, the loss is [ 0.17933334]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.640376990155162, auc for val: 0.6420669333476544\n",
      "--------------------------------------------------------------\n",
      "Epoch 20: iteration 800, the loss is [ 0.16623572]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406406430646001, auc for val: 0.642314128276658\n",
      "--------------------------------------------------------------\n",
      "Epoch 20: iteration 900, the loss is [ 0.13274764]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.640551535529154, auc for val: 0.6421678442708936\n",
      "--------------------------------------------------------------\n",
      "Epoch 20: iteration 1000, the loss is [ 0.16181579]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405340736456095, auc for val: 0.642131052342555\n",
      "--------------------------------------------------------------\n",
      "Epoch 20: iteration 1100, the loss is [ 0.1380973]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6403801967796832, auc for val: 0.6419782642489246\n",
      "--------------------------------------------------------------\n",
      "Epoch 21: iteration 0, the loss is [ 0.12641084]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406786472696897, auc for val: 0.6423020207757024\n",
      "--------------------------------------------------------------\n",
      "Epoch 21: iteration 100, the loss is [ 0.17147993]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404458996091044, auc for val: 0.6421190848208815\n",
      "--------------------------------------------------------------\n",
      "Epoch 21: iteration 200, the loss is [ 0.14306149]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404236697638805, auc for val: 0.6420679768295772\n",
      "--------------------------------------------------------------\n",
      "Epoch 21: iteration 300, the loss is [ 0.1975334]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.640421667858945, auc for val: 0.642044200146631\n",
      "--------------------------------------------------------------\n",
      "Epoch 21: iteration 400, the loss is [ 0.1437643]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406325158087846, auc for val: 0.6421599630131187\n",
      "--------------------------------------------------------------\n",
      "Epoch 21: iteration 500, the loss is [ 0.17895265]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406570853289126, auc for val: 0.642262340183992\n",
      "--------------------------------------------------------------\n",
      "Epoch 21: iteration 600, the loss is [ 0.17364839]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405600468099327, auc for val: 0.6420924703761208\n",
      "--------------------------------------------------------------\n",
      "Epoch 21: iteration 700, the loss is [ 0.16548924]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.640658881718605, auc for val: 0.6422734522767176\n",
      "--------------------------------------------------------------\n",
      "Epoch 21: iteration 800, the loss is [ 0.12389213]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6403875866480919, auc for val: 0.6420047033660993\n",
      "--------------------------------------------------------------\n",
      "Epoch 21: iteration 900, the loss is [ 0.15347317]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406631074399295, auc for val: 0.6422772854467621\n",
      "--------------------------------------------------------------\n",
      "Epoch 21: iteration 1000, the loss is [ 0.17235854]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404970426440092, auc for val: 0.6421363545880976\n",
      "--------------------------------------------------------------\n",
      "Epoch 21: iteration 1100, the loss is [ 0.17197211]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405409210325423, auc for val: 0.6422274372689987\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: iteration 0, the loss is [ 0.15243264]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.640661952230469, auc for val: 0.6422377391786054\n",
      "--------------------------------------------------------------\n",
      "Epoch 22: iteration 100, the loss is [ 0.13250232]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6402834993661755, auc for val: 0.6419124612608397\n",
      "--------------------------------------------------------------\n",
      "Epoch 22: iteration 200, the loss is [ 0.19594081]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405448670136227, auc for val: 0.6421182194944091\n",
      "--------------------------------------------------------------\n",
      "Epoch 22: iteration 300, the loss is [ 0.08763209]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405016437501374, auc for val: 0.6420507169598861\n",
      "--------------------------------------------------------------\n",
      "Epoch 22: iteration 400, the loss is [ 0.141773]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406121885669385, auc for val: 0.6421023240692376\n",
      "--------------------------------------------------------------\n",
      "Epoch 22: iteration 500, the loss is [ 0.1966933]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404103423374609, auc for val: 0.6419934696752092\n",
      "--------------------------------------------------------------\n",
      "Epoch 22: iteration 600, the loss is [ 0.13490997]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404208168232967, auc for val: 0.6420186772574861\n",
      "--------------------------------------------------------------\n",
      "Epoch 22: iteration 700, the loss is [ 0.14786109]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404729218495764, auc for val: 0.642104300746376\n",
      "--------------------------------------------------------------\n",
      "Epoch 22: iteration 800, the loss is [ 0.15463544]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404715070334535, auc for val: 0.6420364362452241\n",
      "--------------------------------------------------------------\n",
      "Epoch 22: iteration 900, the loss is [ 0.14693467]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405118605984637, auc for val: 0.6421424359102523\n",
      "--------------------------------------------------------------\n",
      "Epoch 22: iteration 1000, the loss is [ 0.13310571]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406561302496655, auc for val: 0.6422489247958025\n",
      "--------------------------------------------------------------\n",
      "Epoch 22: iteration 1100, the loss is [ 0.13460805]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405265460570722, auc for val: 0.6421411987196257\n",
      "--------------------------------------------------------------\n",
      "Epoch 23: iteration 0, the loss is [ 0.1454647]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405618699238405, auc for val: 0.642131318161798\n",
      "--------------------------------------------------------------\n",
      "Epoch 23: iteration 100, the loss is [ 0.13010006]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405526437602571, auc for val: 0.6420845551839744\n",
      "--------------------------------------------------------------\n",
      "Epoch 23: iteration 200, the loss is [ 0.18303756]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405509570001274, auc for val: 0.6421853162304063\n",
      "--------------------------------------------------------------\n",
      "Epoch 23: iteration 300, the loss is [ 0.14826344]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406390548424791, auc for val: 0.6422273001175807\n",
      "--------------------------------------------------------------\n",
      "Epoch 23: iteration 400, the loss is [ 0.14606474]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406825147257829, auc for val: 0.6423267801414893\n",
      "--------------------------------------------------------------\n",
      "Epoch 23: iteration 500, the loss is [ 0.16916557]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406446370287074, auc for val: 0.6422377108999624\n",
      "--------------------------------------------------------------\n",
      "Epoch 23: iteration 600, the loss is [ 0.16739878]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6403558613877925, auc for val: 0.6418414776253765\n",
      "--------------------------------------------------------------\n",
      "Epoch 23: iteration 700, the loss is [ 0.12702765]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405925372267104, auc for val: 0.642170602852508\n",
      "--------------------------------------------------------------\n",
      "Epoch 23: iteration 800, the loss is [ 0.13980991]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405987562931842, auc for val: 0.6421898464689981\n",
      "--------------------------------------------------------------\n",
      "Epoch 23: iteration 900, the loss is [ 0.15071864]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406080126370467, auc for val: 0.6422245811260663\n",
      "--------------------------------------------------------------\n",
      "Epoch 23: iteration 1000, the loss is [ 0.19655767]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404988206281517, auc for val: 0.6421583680976594\n",
      "--------------------------------------------------------------\n",
      "Epoch 23: iteration 1100, the loss is [ 0.13542981]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405613075097167, auc for val: 0.6422097093877677\n",
      "--------------------------------------------------------------\n",
      "Epoch 24: iteration 0, the loss is [ 0.11707906]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405247358833108, auc for val: 0.6421991501425106\n",
      "--------------------------------------------------------------\n",
      "Epoch 24: iteration 100, the loss is [ 0.16667162]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405700136160528, auc for val: 0.642175301349025\n",
      "--------------------------------------------------------------\n",
      "Epoch 24: iteration 200, the loss is [ 0.14503612]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406082164644445, auc for val: 0.6421958288159026\n",
      "--------------------------------------------------------------\n",
      "Epoch 24: iteration 300, the loss is [ 0.20592281]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406424213702369, auc for val: 0.6422579725475969\n",
      "--------------------------------------------------------------\n",
      "Epoch 24: iteration 400, the loss is [ 0.18574217]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406234385774445, auc for val: 0.6424016040166578\n",
      "--------------------------------------------------------------\n",
      "Epoch 24: iteration 500, the loss is [ 0.10994133]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404373493069525, auc for val: 0.6420973031961916\n",
      "--------------------------------------------------------------\n",
      "Epoch 24: iteration 600, the loss is [ 0.15129317]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406970377493868, auc for val: 0.6423568445806833\n",
      "--------------------------------------------------------------\n",
      "Epoch 24: iteration 700, the loss is [ 0.18249887]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404946491589936, auc for val: 0.642038360606873\n",
      "--------------------------------------------------------------\n",
      "Epoch 24: iteration 800, the loss is [ 0.15926184]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6402840743587643, auc for val: 0.6419188338530161\n",
      "--------------------------------------------------------------\n",
      "Epoch 24: iteration 900, the loss is [ 0.1794083]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6402972572734876, auc for val: 0.6419590319438915\n",
      "--------------------------------------------------------------\n",
      "Epoch 24: iteration 1000, the loss is [ 0.16388746]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405334502279388, auc for val: 0.6421606699791911\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: iteration 1100, the loss is [ 0.19413586]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405848308471451, auc for val: 0.6422364836068608\n",
      "--------------------------------------------------------------\n",
      "Epoch 25: iteration 0, the loss is [ 0.1414497]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406086259678331, auc for val: 0.6421911458726391\n",
      "--------------------------------------------------------------\n",
      "Epoch 25: iteration 100, the loss is [ 0.10052525]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6403401804220265, auc for val: 0.6419895191487968\n",
      "--------------------------------------------------------------\n",
      "Epoch 25: iteration 200, the loss is [ 0.14733085]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404272662404497, auc for val: 0.6421052707038273\n",
      "--------------------------------------------------------------\n",
      "Epoch 25: iteration 300, the loss is [ 0.17002209]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405495691091538, auc for val: 0.6422173545188745\n",
      "--------------------------------------------------------------\n",
      "Epoch 25: iteration 400, the loss is [ 0.16569331]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406647008461464, auc for val: 0.6423532715741536\n",
      "--------------------------------------------------------------\n",
      "Epoch 25: iteration 500, the loss is [ 0.16152175]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404575596856139, auc for val: 0.6421169950291716\n",
      "--------------------------------------------------------------\n",
      "Epoch 25: iteration 600, the loss is [ 0.14220732]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6407071761530991, auc for val: 0.6423459516474401\n",
      "--------------------------------------------------------------\n",
      "Epoch 25: iteration 700, the loss is [ 0.15791608]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406768936387419, auc for val: 0.6423373323170856\n",
      "--------------------------------------------------------------\n",
      "Epoch 25: iteration 800, the loss is [ 0.14500986]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405962943700655, auc for val: 0.6421648212839681\n",
      "--------------------------------------------------------------\n",
      "Epoch 25: iteration 900, the loss is [ 0.17270429]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405480484008399, auc for val: 0.6421842826460086\n",
      "--------------------------------------------------------------\n",
      "Epoch 25: iteration 1000, the loss is [ 0.15827978]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405363167114707, auc for val: 0.6421990610647854\n",
      "--------------------------------------------------------------\n",
      "Epoch 25: iteration 1100, the loss is [ 0.14885701]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6407037322255842, auc for val: 0.642411491644146\n",
      "--------------------------------------------------------------\n",
      "Epoch 26: iteration 0, the loss is [ 0.12899643]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6403792637629837, auc for val: 0.642064057409672\n",
      "--------------------------------------------------------------\n",
      "Epoch 26: iteration 100, the loss is [ 0.15142559]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405797436014202, auc for val: 0.6421550199063406\n",
      "--------------------------------------------------------------\n",
      "Epoch 26: iteration 200, the loss is [ 0.19446181]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404333880147053, auc for val: 0.6420684702918957\n",
      "--------------------------------------------------------------\n",
      "Epoch 26: iteration 300, the loss is [ 0.11440462]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406816493434505, auc for val: 0.6423181198071026\n",
      "--------------------------------------------------------------\n",
      "Epoch 26: iteration 400, the loss is [ 0.20183274]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405193271030807, auc for val: 0.6421866198758438\n",
      "--------------------------------------------------------------\n",
      "Epoch 26: iteration 500, the loss is [ 0.20498511]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406658647761403, auc for val: 0.6422814056450319\n",
      "--------------------------------------------------------------\n",
      "Epoch 26: iteration 600, the loss is [ 0.17031918]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406391227983407, auc for val: 0.6422534352393444\n",
      "--------------------------------------------------------------\n",
      "Epoch 26: iteration 700, the loss is [ 0.17015055]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.64074245404618, auc for val: 0.6423685066930132\n",
      "--------------------------------------------------------------\n",
      "Epoch 26: iteration 800, the loss is [ 0.1658003]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404114034696464, auc for val: 0.6420611899552824\n",
      "--------------------------------------------------------------\n",
      "Epoch 26: iteration 900, the loss is [ 0.1598599]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405600190006744, auc for val: 0.6422437498041528\n",
      "--------------------------------------------------------------\n",
      "Epoch 26: iteration 1000, the loss is [ 0.14544727]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405985795114956, auc for val: 0.6422246164743699\n",
      "--------------------------------------------------------------\n",
      "Epoch 26: iteration 1100, the loss is [ 0.18588716]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404497282849454, auc for val: 0.6421140384970571\n",
      "--------------------------------------------------------------\n",
      "Epoch 27: iteration 0, the loss is [ 0.11981631]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.640493979848009, auc for val: 0.6422020727402539\n",
      "--------------------------------------------------------------\n",
      "Epoch 27: iteration 100, the loss is [ 0.12540808]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6403732463538213, auc for val: 0.6420328123371372\n",
      "--------------------------------------------------------------\n",
      "Epoch 27: iteration 200, the loss is [ 0.19486696]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404617732705278, auc for val: 0.6421376554056708\n",
      "--------------------------------------------------------------\n",
      "Epoch 27: iteration 300, the loss is [ 0.18656908]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405867660420095, auc for val: 0.6421763278637622\n",
      "--------------------------------------------------------------\n",
      "Epoch 27: iteration 400, the loss is [ 0.19412014]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405547620863227, auc for val: 0.6422298819576769\n",
      "--------------------------------------------------------------\n",
      "Epoch 27: iteration 500, the loss is [ 0.21425538]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405098287543698, auc for val: 0.6421874173335734\n",
      "--------------------------------------------------------------\n",
      "Epoch 27: iteration 600, the loss is [ 0.1580102]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406487596307292, auc for val: 0.6422468265204997\n",
      "--------------------------------------------------------------\n",
      "Epoch 27: iteration 700, the loss is [ 0.14907113]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6403082425334223, auc for val: 0.64194034117487\n",
      "--------------------------------------------------------------\n",
      "Epoch 27: iteration 800, the loss is [ 0.14801659]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406696217185613, auc for val: 0.6423229300042591\n",
      "--------------------------------------------------------------\n",
      "Epoch 27: iteration 900, the loss is [ 0.13571681]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.640467208493666, auc for val: 0.6421674568534859\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: iteration 1000, the loss is [ 0.17144346]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404279911297647, auc for val: 0.6420920730611882\n",
      "--------------------------------------------------------------\n",
      "Epoch 27: iteration 1100, the loss is [ 0.1321985]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.640524874125868, auc for val: 0.6422107542836226\n",
      "--------------------------------------------------------------\n",
      "Epoch 28: iteration 0, the loss is [ 0.12761624]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404358499780107, auc for val: 0.6421321071359347\n",
      "--------------------------------------------------------------\n",
      "Epoch 28: iteration 100, the loss is [ 0.17987025]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406665332432022, auc for val: 0.642301303912105\n",
      "--------------------------------------------------------------\n",
      "Epoch 28: iteration 200, the loss is [ 0.19450544]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406448456383336, auc for val: 0.6422685558297003\n",
      "--------------------------------------------------------------\n",
      "Epoch 28: iteration 300, the loss is [ 0.1376891]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404959864547988, auc for val: 0.6422613164971192\n",
      "--------------------------------------------------------------\n",
      "Epoch 28: iteration 400, the loss is [ 0.15651594]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6407088805552298, auc for val: 0.6424078917729055\n",
      "--------------------------------------------------------------\n",
      "Epoch 28: iteration 500, the loss is [ 0.12813145]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405234725713858, auc for val: 0.6422029606896407\n",
      "--------------------------------------------------------------\n",
      "Epoch 28: iteration 600, the loss is [ 0.11421033]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404406412078417, auc for val: 0.6421166415461355\n",
      "--------------------------------------------------------------\n",
      "Epoch 28: iteration 700, the loss is [ 0.15665656]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405583727890157, auc for val: 0.6422438714023172\n",
      "--------------------------------------------------------------\n",
      "Epoch 28: iteration 800, the loss is [ 0.15927497]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404696316767192, auc for val: 0.6421085383010139\n",
      "--------------------------------------------------------------\n",
      "Epoch 28: iteration 900, the loss is [ 0.12883699]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404632070950023, auc for val: 0.6421409131053324\n",
      "--------------------------------------------------------------\n",
      "Epoch 28: iteration 1000, the loss is [ 0.1441551]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406557952525855, auc for val: 0.6422812402149709\n",
      "--------------------------------------------------------------\n",
      "Epoch 28: iteration 1100, the loss is [ 0.13477965]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405081088401384, auc for val: 0.6422475589373506\n",
      "--------------------------------------------------------------\n",
      "Epoch 29: iteration 0, the loss is [ 0.14395513]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406325766113976, auc for val: 0.6422415058938389\n",
      "--------------------------------------------------------------\n",
      "Epoch 29: iteration 100, the loss is [ 0.13285722]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404315375737808, auc for val: 0.642082889571908\n",
      "--------------------------------------------------------------\n",
      "Epoch 29: iteration 200, the loss is [ 0.19080935]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404140744444458, auc for val: 0.6420700878302693\n",
      "--------------------------------------------------------------\n",
      "Epoch 29: iteration 300, the loss is [ 0.19949998]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405789601599524, auc for val: 0.6421751387468284\n",
      "--------------------------------------------------------------\n",
      "Epoch 29: iteration 400, the loss is [ 0.13142882]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406310901824155, auc for val: 0.6422267048521476\n",
      "--------------------------------------------------------------\n",
      "Epoch 29: iteration 500, the loss is [ 0.17221646]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404692762647499, auc for val: 0.6422474090605435\n",
      "--------------------------------------------------------------\n",
      "Epoch 29: iteration 600, the loss is [ 0.17175765]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404268596305102, auc for val: 0.6421252212863899\n",
      "--------------------------------------------------------------\n",
      "Epoch 29: iteration 700, the loss is [ 0.13009922]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404164340519976, auc for val: 0.6420886838658372\n",
      "--------------------------------------------------------------\n",
      "Epoch 29: iteration 800, the loss is [ 0.12829427]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.640231677094149, auc for val: 0.6419575402454787\n",
      "--------------------------------------------------------------\n",
      "Epoch 29: iteration 900, the loss is [ 0.18881875]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406224604711667, auc for val: 0.6422366490369218\n",
      "--------------------------------------------------------------\n",
      "Epoch 29: iteration 1000, the loss is [ 0.17747682]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405408032048745, auc for val: 0.6422007860620021\n",
      "--------------------------------------------------------------\n",
      "Epoch 29: iteration 1100, the loss is [ 0.15124807]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405765068368068, auc for val: 0.6421953169724662\n",
      "--------------------------------------------------------------\n",
      "Epoch 30: iteration 0, the loss is [ 0.14055528]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404707350854077, auc for val: 0.6421333782609331\n",
      "--------------------------------------------------------------\n",
      "Epoch 30: iteration 100, the loss is [ 0.1187919]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405771763487252, auc for val: 0.6421566332029178\n",
      "--------------------------------------------------------------\n",
      "Epoch 30: iteration 200, the loss is [ 0.1693913]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405605264794563, auc for val: 0.6421497360419157\n",
      "--------------------------------------------------------------\n",
      "Epoch 30: iteration 300, the loss is [ 0.1415066]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406673869311788, auc for val: 0.6422516452012491\n",
      "--------------------------------------------------------------\n",
      "Epoch 30: iteration 400, the loss is [ 0.10220896]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405787336270181, auc for val: 0.6421382365317823\n",
      "--------------------------------------------------------------\n",
      "Epoch 30: iteration 500, the loss is [ 0.16294803]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.640560831899059, auc for val: 0.6421657262005407\n",
      "--------------------------------------------------------------\n",
      "Epoch 30: iteration 600, the loss is [ 0.12991858]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404487435478474, auc for val: 0.6421051335524094\n",
      "--------------------------------------------------------------\n",
      "Epoch 30: iteration 700, the loss is [ 0.12618731]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404407035375527, auc for val: 0.6420889666522662\n",
      "--------------------------------------------------------------\n",
      "Epoch 30: iteration 800, the loss is [ 0.13291085]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404130591653874, auc for val: 0.6420705614975378\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: iteration 900, the loss is [ 0.20856793]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406825232051955, auc for val: 0.6423749146334932\n",
      "--------------------------------------------------------------\n",
      "Epoch 30: iteration 1000, the loss is [ 0.18689081]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404344204133361, auc for val: 0.6420993152216337\n",
      "--------------------------------------------------------------\n",
      "Epoch 30: iteration 1100, the loss is [ 0.17332338]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405643449880394, auc for val: 0.6421691366048738\n",
      "--------------------------------------------------------------\n",
      "Epoch 31: iteration 0, the loss is [ 0.11813226]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404628985810167, auc for val: 0.6421289173050163\n",
      "--------------------------------------------------------------\n",
      "Epoch 31: iteration 100, the loss is [ 0.18983264]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.640652387372621, auc for val: 0.6422401456911158\n",
      "--------------------------------------------------------------\n",
      "Epoch 31: iteration 200, the loss is [ 0.11415139]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406557188976848, auc for val: 0.6422456586125482\n",
      "--------------------------------------------------------------\n",
      "Epoch 31: iteration 300, the loss is [ 0.13437353]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.640526386957571, auc for val: 0.642214693498578\n",
      "--------------------------------------------------------------\n",
      "Epoch 31: iteration 400, the loss is [ 0.14841561]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404962076027032, auc for val: 0.6422420318765969\n",
      "--------------------------------------------------------------\n",
      "Epoch 31: iteration 500, the loss is [ 0.14110968]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404804287017303, auc for val: 0.642193468963153\n",
      "--------------------------------------------------------------\n",
      "Epoch 31: iteration 600, the loss is [ 0.14807382]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404326750608668, auc for val: 0.6421216327266065\n",
      "--------------------------------------------------------------\n",
      "Epoch 31: iteration 700, the loss is [ 0.16063322]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405585988397084, auc for val: 0.6421941971382075\n",
      "--------------------------------------------------------------\n",
      "Epoch 31: iteration 800, the loss is [ 0.16155988]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405437050125946, auc for val: 0.642161989177882\n",
      "--------------------------------------------------------------\n",
      "Epoch 31: iteration 900, the loss is [ 0.12774421]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.640499653901239, auc for val: 0.6422323506832017\n",
      "--------------------------------------------------------------\n",
      "Epoch 31: iteration 1000, the loss is [ 0.11374204]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405609759285139, auc for val: 0.6421812582451509\n",
      "--------------------------------------------------------------\n",
      "Epoch 31: iteration 1100, the loss is [ 0.12997425]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405488693767692, auc for val: 0.6421787555852546\n",
      "--------------------------------------------------------------\n",
      "Epoch 32: iteration 0, the loss is [ 0.16491967]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404587814845852, auc for val: 0.6421189674645136\n",
      "--------------------------------------------------------------\n",
      "Epoch 32: iteration 100, the loss is [ 0.12701887]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404700322184536, auc for val: 0.642187970181042\n",
      "--------------------------------------------------------------\n",
      "Epoch 32: iteration 200, the loss is [ 0.18510234]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405443167760964, auc for val: 0.642181405294094\n",
      "--------------------------------------------------------------\n",
      "Epoch 32: iteration 300, the loss is [ 0.13049068]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404796871750843, auc for val: 0.6421853487508457\n",
      "--------------------------------------------------------------\n",
      "Epoch 32: iteration 400, the loss is [ 0.14817543]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405416382461804, auc for val: 0.6422138663482733\n",
      "--------------------------------------------------------------\n",
      "Epoch 32: iteration 500, the loss is [ 0.13984953]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.640484548048622, auc for val: 0.6421964353927927\n",
      "--------------------------------------------------------------\n",
      "Epoch 32: iteration 600, the loss is [ 0.10767303]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405175494002455, auc for val: 0.6422220911915594\n",
      "--------------------------------------------------------------\n",
      "Epoch 32: iteration 700, the loss is [ 0.1238608]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405295376822677, auc for val: 0.6422482673173552\n",
      "--------------------------------------------------------------\n",
      "Epoch 32: iteration 800, the loss is [ 0.16137621]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405073018090249, auc for val: 0.6422483945712483\n",
      "--------------------------------------------------------------\n",
      "Epoch 32: iteration 900, the loss is [ 0.13349427]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6403617538160387, auc for val: 0.641976940808437\n",
      "--------------------------------------------------------------\n",
      "Epoch 32: iteration 1000, the loss is [ 0.16910647]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404281988150945, auc for val: 0.6421620443212358\n",
      "--------------------------------------------------------------\n",
      "Epoch 32: iteration 1100, the loss is [ 0.15430214]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404328065118564, auc for val: 0.6421214856776635\n",
      "--------------------------------------------------------------\n",
      "Epoch 33: iteration 0, the loss is [ 0.16095807]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405722433800867, auc for val: 0.6421865619046259\n",
      "--------------------------------------------------------------\n",
      "Epoch 33: iteration 100, the loss is [ 0.15200178]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406395220139107, auc for val: 0.6422503274164901\n",
      "--------------------------------------------------------------\n",
      "Epoch 33: iteration 200, the loss is [ 0.15497535]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405141226726765, auc for val: 0.6422622610037919\n",
      "--------------------------------------------------------------\n",
      "Epoch 33: iteration 300, the loss is [ 0.2150275]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404693033506463, auc for val: 0.6422025633747082\n",
      "--------------------------------------------------------------\n",
      "Epoch 33: iteration 400, the loss is [ 0.15761499]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405797504331745, auc for val: 0.6421833098606928\n",
      "--------------------------------------------------------------\n",
      "Epoch 33: iteration 500, the loss is [ 0.13558766]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405187652515719, auc for val: 0.6422528046256077\n",
      "--------------------------------------------------------------\n",
      "Epoch 33: iteration 600, the loss is [ 0.11303563]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404401586850562, auc for val: 0.6421317904151345\n",
      "--------------------------------------------------------------\n",
      "Epoch 33: iteration 700, the loss is [ 0.13853216]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.640430209922805, auc for val: 0.6421308331830724\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33: iteration 800, the loss is [ 0.16771905]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404857674363859, auc for val: 0.6422079349029259\n",
      "--------------------------------------------------------------\n",
      "Epoch 33: iteration 900, the loss is [ 0.15204683]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6403385481551839, auc for val: 0.6420223308581481\n",
      "--------------------------------------------------------------\n",
      "Epoch 33: iteration 1000, the loss is [ 0.14227487]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6402963917304082, auc for val: 0.6420334146722307\n",
      "--------------------------------------------------------------\n",
      "Epoch 33: iteration 1100, the loss is [ 0.14593346]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404907655076242, auc for val: 0.642205268226901\n",
      "--------------------------------------------------------------\n",
      "Epoch 34: iteration 0, the loss is [ 0.13222595]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6403864276208863, auc for val: 0.6420605466161565\n",
      "--------------------------------------------------------------\n",
      "Epoch 34: iteration 100, the loss is [ 0.19404852]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404329689870477, auc for val: 0.6421134361619633\n",
      "--------------------------------------------------------------\n",
      "Epoch 34: iteration 200, the loss is [ 0.15165858]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405903216486134, auc for val: 0.6421815579987656\n",
      "--------------------------------------------------------------\n",
      "Epoch 34: iteration 300, the loss is [ 0.17810538]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6403178532042109, auc for val: 0.6419751041105811\n",
      "--------------------------------------------------------------\n",
      "Epoch 34: iteration 400, the loss is [ 0.18151113]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406227476459669, auc for val: 0.6422398289703153\n",
      "--------------------------------------------------------------\n",
      "Epoch 34: iteration 500, the loss is [ 0.15350987]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404541243178852, auc for val: 0.6421128649333768\n",
      "--------------------------------------------------------------\n",
      "Epoch 34: iteration 600, the loss is [ 0.12123095]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6407597635012305, auc for val: 0.6423809167754477\n",
      "--------------------------------------------------------------\n",
      "Epoch 34: iteration 700, the loss is [ 0.20162307]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406879317041182, auc for val: 0.6423752002477865\n",
      "--------------------------------------------------------------\n",
      "Epoch 34: iteration 800, the loss is [ 0.11703953]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404900277987232, auc for val: 0.6422160692545548\n",
      "--------------------------------------------------------------\n",
      "Epoch 34: iteration 900, the loss is [ 0.13937008]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405737592257992, auc for val: 0.6422016556302712\n",
      "--------------------------------------------------------------\n",
      "Epoch 34: iteration 1000, the loss is [ 0.12452238]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404639706440091, auc for val: 0.6421811663395615\n",
      "--------------------------------------------------------------\n",
      "Epoch 34: iteration 1100, the loss is [ 0.13974878]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6403647961167761, auc for val: 0.6420270661169009\n",
      "--------------------------------------------------------------\n",
      "Epoch 35: iteration 0, the loss is [ 0.12001299]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404010387339335, auc for val: 0.6420793646390711\n",
      "--------------------------------------------------------------\n",
      "Epoch 35: iteration 100, the loss is [ 0.13158031]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404233911086797, auc for val: 0.6421226606552757\n",
      "--------------------------------------------------------------\n",
      "Epoch 35: iteration 200, the loss is [ 0.1925043]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405745875960979, auc for val: 0.6421692709284277\n",
      "--------------------------------------------------------------\n",
      "Epoch 35: iteration 300, the loss is [ 0.1636098]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.640437998604916, auc for val: 0.6421300371392749\n",
      "--------------------------------------------------------------\n",
      "Epoch 35: iteration 400, the loss is [ 0.15758546]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405189337949685, auc for val: 0.6422296175523659\n",
      "--------------------------------------------------------------\n",
      "Epoch 35: iteration 500, the loss is [ 0.16884392]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404406909590876, auc for val: 0.6421305461548471\n",
      "--------------------------------------------------------------\n",
      "Epoch 35: iteration 600, the loss is [ 0.14954492]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405611701110823, auc for val: 0.6422591701481235\n",
      "--------------------------------------------------------------\n",
      "Epoch 35: iteration 700, the loss is [ 0.12558399]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.640598043500093, auc for val: 0.6421871713093802\n",
      "--------------------------------------------------------------\n",
      "Epoch 35: iteration 800, the loss is [ 0.11658535]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405981770407956, auc for val: 0.6422118430113741\n",
      "--------------------------------------------------------------\n",
      "Epoch 35: iteration 900, the loss is [ 0.17992383]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405656200746941, auc for val: 0.642245110006876\n",
      "--------------------------------------------------------------\n",
      "Epoch 35: iteration 1000, the loss is [ 0.20780353]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404542487763732, auc for val: 0.6421813049049117\n",
      "--------------------------------------------------------------\n",
      "Epoch 35: iteration 1100, the loss is [ 0.13228169]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405443888310105, auc for val: 0.6422083901890766\n",
      "--------------------------------------------------------------\n",
      "Epoch 36: iteration 0, the loss is [ 0.12856342]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404321032430343, auc for val: 0.6421431216673424\n",
      "--------------------------------------------------------------\n",
      "Epoch 36: iteration 100, the loss is [ 0.1578809]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405106748068562, auc for val: 0.6422489346933277\n",
      "--------------------------------------------------------------\n",
      "Epoch 36: iteration 200, the loss is [ 0.11959817]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406568401493078, auc for val: 0.6422887114324239\n",
      "--------------------------------------------------------------\n",
      "Epoch 36: iteration 300, the loss is [ 0.21114632]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404782488095025, auc for val: 0.6422255044237568\n",
      "--------------------------------------------------------------\n",
      "Epoch 36: iteration 400, the loss is [ 0.14839099]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.640436632334435, auc for val: 0.6421538661377105\n",
      "--------------------------------------------------------------\n",
      "Epoch 36: iteration 500, the loss is [ 0.11542094]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405214644374977, auc for val: 0.6422374846708193\n",
      "--------------------------------------------------------------\n",
      "Epoch 36: iteration 600, the loss is [ 0.17995392]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404705686317242, auc for val: 0.6421780203405395\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36: iteration 700, the loss is [ 0.17838658]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405180261361333, auc for val: 0.6422292697250582\n",
      "--------------------------------------------------------------\n",
      "Epoch 36: iteration 800, the loss is [ 0.15344155]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404326382497674, auc for val: 0.6421328353109894\n",
      "--------------------------------------------------------------\n",
      "Epoch 36: iteration 900, the loss is [ 0.16628027]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404834419474186, auc for val: 0.6422145945233277\n",
      "--------------------------------------------------------------\n",
      "Epoch 36: iteration 1000, the loss is [ 0.14365144]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405375475926566, auc for val: 0.6422296981464981\n",
      "--------------------------------------------------------------\n",
      "Epoch 36: iteration 1100, the loss is [ 0.13770318]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405231974526225, auc for val: 0.6422462835705561\n",
      "--------------------------------------------------------------\n",
      "Epoch 37: iteration 0, the loss is [ 0.15375407]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.640356019241503, auc for val: 0.6420042593914058\n",
      "--------------------------------------------------------------\n",
      "Epoch 37: iteration 100, the loss is [ 0.14316526]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404582373554509, auc for val: 0.6421307327938901\n",
      "--------------------------------------------------------------\n",
      "Epoch 37: iteration 200, the loss is [ 0.17680201]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405403074204483, auc for val: 0.6422290449098473\n",
      "--------------------------------------------------------------\n",
      "Epoch 37: iteration 300, the loss is [ 0.1586002]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406326724167047, auc for val: 0.6422513171689914\n",
      "--------------------------------------------------------------\n",
      "Epoch 37: iteration 400, the loss is [ 0.16689862]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404668664638976, auc for val: 0.6421887478437216\n",
      "--------------------------------------------------------------\n",
      "Epoch 37: iteration 500, the loss is [ 0.1876848]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404729625989813, auc for val: 0.642189727698698\n",
      "--------------------------------------------------------------\n",
      "Epoch 37: iteration 600, the loss is [ 0.15051566]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404372164494253, auc for val: 0.642138838866876\n",
      "--------------------------------------------------------------\n",
      "Epoch 37: iteration 700, the loss is [ 0.19906598]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404816455979132, auc for val: 0.6421832490616106\n",
      "--------------------------------------------------------------\n",
      "Epoch 37: iteration 800, the loss is [ 0.13338906]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404436439647778, auc for val: 0.6421168635334823\n",
      "--------------------------------------------------------------\n",
      "Epoch 37: iteration 900, the loss is [ 0.17873384]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.64068423769421, auc for val: 0.6423746643675037\n",
      "--------------------------------------------------------------\n",
      "Epoch 37: iteration 1000, the loss is [ 0.11877298]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404837035634205, auc for val: 0.6421797934114489\n",
      "--------------------------------------------------------------\n",
      "Epoch 37: iteration 1100, the loss is [ 0.16084415]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404360922641668, auc for val: 0.6421140455667178\n",
      "--------------------------------------------------------------\n",
      "Epoch 38: iteration 0, the loss is [ 0.14556122]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405989987802739, auc for val: 0.6421899510999769\n",
      "--------------------------------------------------------------\n",
      "Epoch 38: iteration 100, the loss is [ 0.13095093]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405163864749214, auc for val: 0.6422454790431658\n",
      "--------------------------------------------------------------\n",
      "Epoch 38: iteration 200, the loss is [ 0.15972292]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6403812321923231, auc for val: 0.6420331163325482\n",
      "--------------------------------------------------------------\n",
      "Epoch 38: iteration 300, the loss is [ 0.15158284]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405181881290831, auc for val: 0.6422335850459641\n",
      "--------------------------------------------------------------\n",
      "Epoch 38: iteration 400, the loss is [ 0.16391642]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404310528005349, auc for val: 0.6421425730616703\n",
      "--------------------------------------------------------------\n",
      "Epoch 38: iteration 500, the loss is [ 0.12472691]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404767625010807, auc for val: 0.6421826778330242\n",
      "--------------------------------------------------------------\n",
      "Epoch 38: iteration 600, the loss is [ 0.14480422]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405927388438349, auc for val: 0.6421797213009095\n",
      "--------------------------------------------------------------\n",
      "Epoch 38: iteration 700, the loss is [ 0.14504929]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404979671411092, auc for val: 0.6422238006355223\n",
      "--------------------------------------------------------------\n",
      "Epoch 38: iteration 800, the loss is [ 0.15561594]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406754634712652, auc for val: 0.6422818680008433\n",
      "--------------------------------------------------------------\n",
      "Epoch 38: iteration 900, the loss is [ 0.17738177]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404438310342846, auc for val: 0.6421374899756099\n",
      "--------------------------------------------------------------\n",
      "Epoch 38: iteration 1000, the loss is [ 0.16840471]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405614461943284, auc for val: 0.6421947528135405\n",
      "--------------------------------------------------------------\n",
      "Epoch 38: iteration 1100, the loss is [ 0.16545406]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406912805898753, auc for val: 0.6423662542991067\n",
      "--------------------------------------------------------------\n",
      "Epoch 39: iteration 0, the loss is [ 0.19308752]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405217115460684, auc for val: 0.6422411396854134\n",
      "--------------------------------------------------------------\n",
      "Epoch 39: iteration 100, the loss is [ 0.1693778]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404227703835239, auc for val: 0.6421071682007656\n",
      "--------------------------------------------------------------\n",
      "Epoch 39: iteration 200, the loss is [ 0.16507311]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405028380211575, auc for val: 0.6422263216765364\n",
      "--------------------------------------------------------------\n",
      "Epoch 39: iteration 300, the loss is [ 0.17005979]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404985875447706, auc for val: 0.6422268985608516\n",
      "--------------------------------------------------------------\n",
      "Epoch 39: iteration 400, the loss is [ 0.16587302]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405278442109441, auc for val: 0.6422182481239899\n",
      "--------------------------------------------------------------\n",
      "Epoch 39: iteration 500, the loss is [ 0.15930179]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405322392392141, auc for val: 0.642209389839103\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39: iteration 600, the loss is [ 0.18496211]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405242494624068, auc for val: 0.6422188815655908\n",
      "--------------------------------------------------------------\n",
      "Epoch 39: iteration 700, the loss is [ 0.12952688]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404762393092638, auc for val: 0.6421713508226127\n",
      "--------------------------------------------------------------\n",
      "Epoch 39: iteration 800, the loss is [ 0.14720708]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404399359296801, auc for val: 0.6421200703315866\n",
      "--------------------------------------------------------------\n",
      "Epoch 39: iteration 900, the loss is [ 0.14904442]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406923954116122, auc for val: 0.6423942741924193\n",
      "--------------------------------------------------------------\n",
      "Epoch 39: iteration 1000, the loss is [ 0.17957245]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.640466293319901, auc for val: 0.6421309816459477\n",
      "--------------------------------------------------------------\n",
      "Epoch 39: iteration 1100, the loss is [ 0.15594216]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.640542462838921, auc for val: 0.6422132456320617\n",
      "--------------------------------------------------------------\n",
      "Epoch 40: iteration 0, the loss is [ 0.14188993]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404383755168178, auc for val: 0.6421315924646341\n",
      "--------------------------------------------------------------\n",
      "Epoch 40: iteration 100, the loss is [ 0.18420628]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404835968674935, auc for val: 0.6421931890045883\n",
      "--------------------------------------------------------------\n",
      "Epoch 40: iteration 200, the loss is [ 0.13746509]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406926772817559, auc for val: 0.6423929408544067\n",
      "--------------------------------------------------------------\n",
      "Epoch 40: iteration 300, the loss is [ 0.13696523]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405302671930634, auc for val: 0.6422130872716615\n",
      "--------------------------------------------------------------\n",
      "Epoch 40: iteration 400, the loss is [ 0.18494423]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404823937553794, auc for val: 0.6422032321646125\n",
      "--------------------------------------------------------------\n",
      "Epoch 40: iteration 500, the loss is [ 0.18940596]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404629576957845, auc for val: 0.6421636095441199\n",
      "--------------------------------------------------------------\n",
      "Epoch 40: iteration 600, the loss is [ 0.15736638]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404847393377416, auc for val: 0.6422063541267882\n",
      "--------------------------------------------------------------\n",
      "Epoch 40: iteration 700, the loss is [ 0.1687301]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406783180193207, auc for val: 0.6423655529887629\n",
      "--------------------------------------------------------------\n",
      "Epoch 40: iteration 800, the loss is [ 0.15628701]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404263894852561, auc for val: 0.6421250007129753\n",
      "--------------------------------------------------------------\n",
      "Epoch 40: iteration 900, the loss is [ 0.14413504]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406840950311062, auc for val: 0.6423366380764026\n",
      "--------------------------------------------------------------\n",
      "Epoch 40: iteration 1000, the loss is [ 0.13591035]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404745679808614, auc for val: 0.6422058861152482\n",
      "--------------------------------------------------------------\n",
      "Epoch 40: iteration 1100, the loss is [ 0.14100219]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405349044673025, auc for val: 0.6422326631622057\n",
      "--------------------------------------------------------------\n",
      "Epoch 41: iteration 0, the loss is [ 0.17103301]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404276183571023, auc for val: 0.6421127277819588\n",
      "--------------------------------------------------------------\n",
      "Epoch 41: iteration 100, the loss is [ 0.16984846]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404850154611745, auc for val: 0.6421982918856988\n",
      "--------------------------------------------------------------\n",
      "Epoch 41: iteration 200, the loss is [ 0.14451736]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405871740584865, auc for val: 0.6422051508705329\n",
      "--------------------------------------------------------------\n",
      "Epoch 41: iteration 300, the loss is [ 0.15332474]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404591096499109, auc for val: 0.6421651196236506\n",
      "--------------------------------------------------------------\n",
      "Epoch 41: iteration 400, the loss is [ 0.14785857]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405591592444926, auc for val: 0.6422218960689232\n",
      "--------------------------------------------------------------\n",
      "Epoch 41: iteration 500, the loss is [ 0.19285625]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6403183446484636, auc for val: 0.6419748255659484\n",
      "--------------------------------------------------------------\n",
      "Epoch 41: iteration 600, the loss is [ 0.14316759]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405241871326957, auc for val: 0.6422283393577071\n",
      "--------------------------------------------------------------\n",
      "Epoch 41: iteration 700, the loss is [ 0.19404577]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405305134978984, auc for val: 0.6422140558151807\n",
      "--------------------------------------------------------------\n",
      "Epoch 41: iteration 800, the loss is [ 0.17858808]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6403985512118295, auc for val: 0.6420764293159384\n",
      "--------------------------------------------------------------\n",
      "Epoch 41: iteration 900, the loss is [ 0.1130064]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404776114068289, auc for val: 0.6421945718302259\n",
      "--------------------------------------------------------------\n",
      "Epoch 41: iteration 1000, the loss is [ 0.1384923]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404303881512178, auc for val: 0.6421027044169846\n",
      "--------------------------------------------------------------\n",
      "Epoch 41: iteration 1100, the loss is [ 0.14815931]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406299088917283, auc for val: 0.6422327791046416\n",
      "--------------------------------------------------------------\n",
      "Epoch 42: iteration 0, the loss is [ 0.12531464]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.640481543523467, auc for val: 0.6421909564057318\n",
      "--------------------------------------------------------------\n",
      "Epoch 42: iteration 100, the loss is [ 0.14160955]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.640491591185408, auc for val: 0.6422130703044757\n",
      "--------------------------------------------------------------\n",
      "Epoch 42: iteration 200, the loss is [ 0.13287838]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404705546467212, auc for val: 0.6421754286029181\n",
      "--------------------------------------------------------------\n",
      "Epoch 42: iteration 300, the loss is [ 0.14073282]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404298787434699, auc for val: 0.6421299169550426\n",
      "--------------------------------------------------------------\n",
      "Epoch 42: iteration 400, the loss is [ 0.15769106]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.640535108616195, auc for val: 0.6422049868544042\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42: iteration 500, the loss is [ 0.17590813]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404282517812836, auc for val: 0.6421311145555693\n",
      "--------------------------------------------------------------\n",
      "Epoch 42: iteration 600, the loss is [ 0.14297199]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404432521435771, auc for val: 0.642123610817677\n",
      "--------------------------------------------------------------\n",
      "Epoch 42: iteration 700, the loss is [ 0.13285777]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.640432106739285, auc for val: 0.6421044944550798\n",
      "--------------------------------------------------------------\n",
      "Epoch 42: iteration 800, the loss is [ 0.1970634]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404222973046342, auc for val: 0.642087655937168\n",
      "--------------------------------------------------------------\n",
      "Epoch 42: iteration 900, the loss is [ 0.11966123]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406001383972599, auc for val: 0.6422113453072591\n",
      "--------------------------------------------------------------\n",
      "Epoch 42: iteration 1000, the loss is [ 0.14028624]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404362238357165, auc for val: 0.6421453500244027\n",
      "--------------------------------------------------------------\n",
      "Epoch 42: iteration 1100, the loss is [ 0.15030313]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405734996191367, auc for val: 0.642190714623335\n",
      "--------------------------------------------------------------\n",
      "Epoch 43: iteration 0, the loss is [ 0.12749389]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.640498830152421, auc for val: 0.6422078232022865\n",
      "--------------------------------------------------------------\n",
      "Epoch 43: iteration 100, the loss is [ 0.12844901]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404245293995019, auc for val: 0.642116968164461\n",
      "--------------------------------------------------------------\n",
      "Epoch 43: iteration 200, the loss is [ 0.17546731]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404363134924447, auc for val: 0.6421193845744964\n",
      "--------------------------------------------------------------\n",
      "Epoch 43: iteration 300, the loss is [ 0.13675985]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405181974122316, auc for val: 0.6422277370226134\n",
      "--------------------------------------------------------------\n",
      "Epoch 43: iteration 400, the loss is [ 0.17758979]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406064192710166, auc for val: 0.6421799178374776\n",
      "--------------------------------------------------------------\n",
      "Epoch 43: iteration 500, the loss is [ 0.15871237]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404399333577258, auc for val: 0.6421256058759331\n",
      "--------------------------------------------------------------\n",
      "Epoch 43: iteration 600, the loss is [ 0.14028545]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405248977958871, auc for val: 0.6422289303813435\n",
      "--------------------------------------------------------------\n",
      "Epoch 43: iteration 700, the loss is [ 0.17732945]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405136329966424, auc for val: 0.6422263909592115\n",
      "--------------------------------------------------------------\n",
      "Epoch 43: iteration 800, the loss is [ 0.14914615]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.640575908495693, auc for val: 0.6421798245179561\n",
      "--------------------------------------------------------------\n",
      "Epoch 43: iteration 900, the loss is [ 0.17413208]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405950327057987, auc for val: 0.6422087224631308\n",
      "--------------------------------------------------------------\n",
      "Epoch 43: iteration 1000, the loss is [ 0.14005086]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.640304200345167, auc for val: 0.6419733635601108\n",
      "--------------------------------------------------------------\n",
      "Epoch 43: iteration 1100, the loss is [ 0.13820234]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405583964992216, auc for val: 0.6422109762709693\n",
      "--------------------------------------------------------------\n",
      "Epoch 44: iteration 0, the loss is [ 0.1792154]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.640528881150682, auc for val: 0.6422139851185734\n",
      "--------------------------------------------------------------\n",
      "Epoch 44: iteration 100, the loss is [ 0.1041955]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404606404451092, auc for val: 0.6421623539723755\n",
      "--------------------------------------------------------------\n",
      "Epoch 44: iteration 200, the loss is [ 0.19285445]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405693650816384, auc for val: 0.642242737428737\n",
      "--------------------------------------------------------------\n",
      "Epoch 44: iteration 300, the loss is [ 0.17938635]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404843936107888, auc for val: 0.6421895453014513\n",
      "--------------------------------------------------------------\n",
      "Epoch 44: iteration 400, the loss is [ 0.15214655]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405123355259459, auc for val: 0.6422283337019783\n",
      "--------------------------------------------------------------\n",
      "Epoch 44: iteration 500, the loss is [ 0.18102805]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404263420246571, auc for val: 0.6421249738482645\n",
      "--------------------------------------------------------------\n",
      "Epoch 44: iteration 600, the loss is [ 0.19564182]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405687750190033, auc for val: 0.6421948984485514\n",
      "--------------------------------------------------------------\n",
      "Epoch 44: iteration 700, the loss is [ 0.1444336]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404636809776278, auc for val: 0.6421781942541932\n",
      "--------------------------------------------------------------\n",
      "Epoch 44: iteration 800, the loss is [ 0.12914762]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404363653334035, auc for val: 0.6421177585525298\n",
      "--------------------------------------------------------------\n",
      "Epoch 44: iteration 900, the loss is [ 0.13256168]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.640454327020053, auc for val: 0.6421288282272911\n",
      "--------------------------------------------------------------\n",
      "Epoch 44: iteration 1000, the loss is [ 0.18688127]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404776341525521, auc for val: 0.6421982975414275\n",
      "--------------------------------------------------------------\n",
      "Epoch 44: iteration 1100, the loss is [ 0.16326548]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404841428050334, auc for val: 0.6422035743361916\n",
      "--------------------------------------------------------------\n",
      "Epoch 45: iteration 0, the loss is [ 0.17775087]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405939062700796, auc for val: 0.6422122770885426\n",
      "--------------------------------------------------------------\n",
      "Epoch 45: iteration 100, the loss is [ 0.12819447]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.640685015951582, auc for val: 0.6423918337455374\n",
      "--------------------------------------------------------------\n",
      "Epoch 45: iteration 200, the loss is [ 0.20820019]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6402484301630942, auc for val: 0.6419704593434854\n",
      "--------------------------------------------------------------\n",
      "Epoch 45: iteration 300, the loss is [ 0.1271809]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404819229269498, auc for val: 0.6422029875543515\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45: iteration 400, the loss is [ 0.15344939]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404253287147516, auc for val: 0.6420918185534021\n",
      "--------------------------------------------------------------\n",
      "Epoch 45: iteration 500, the loss is [ 0.13301294]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404589828605891, auc for val: 0.6421218391606996\n",
      "--------------------------------------------------------------\n",
      "Epoch 45: iteration 600, the loss is [ 0.16338055]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405822841298998, auc for val: 0.6421761850566156\n",
      "--------------------------------------------------------------\n",
      "Epoch 45: iteration 700, the loss is [ 0.11523358]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405304221131382, auc for val: 0.6422006149762126\n",
      "--------------------------------------------------------------\n",
      "Epoch 45: iteration 800, the loss is [ 0.13670184]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406928158458074, auc for val: 0.6424042805902077\n",
      "--------------------------------------------------------------\n",
      "Epoch 45: iteration 900, the loss is [ 0.15741554]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404808775881728, auc for val: 0.6421818775474304\n",
      "--------------------------------------------------------------\n",
      "Epoch 45: iteration 1000, the loss is [ 0.18432261]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405679550075568, auc for val: 0.6421904502180239\n",
      "--------------------------------------------------------------\n",
      "Epoch 45: iteration 1100, the loss is [ 0.14218719]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406799769701916, auc for val: 0.6423033173514793\n",
      "--------------------------------------------------------------\n",
      "Epoch 46: iteration 0, the loss is [ 0.14871667]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404991868101807, auc for val: 0.6422090462535918\n",
      "--------------------------------------------------------------\n",
      "Epoch 46: iteration 100, the loss is [ 0.18040742]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404336431204471, auc for val: 0.6420732734193915\n",
      "--------------------------------------------------------------\n",
      "Epoch 46: iteration 200, the loss is [ 0.15794164]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405915687250755, auc for val: 0.6422003293619194\n",
      "--------------------------------------------------------------\n",
      "Epoch 46: iteration 300, the loss is [ 0.23960614]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406542184836996, auc for val: 0.6422484864768376\n",
      "--------------------------------------------------------------\n",
      "Epoch 46: iteration 400, the loss is [ 0.16958749]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406814898420817, auc for val: 0.6423795339498102\n",
      "--------------------------------------------------------------\n",
      "Epoch 46: iteration 500, the loss is [ 0.17164856]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406092996591778, auc for val: 0.6422212909059654\n",
      "--------------------------------------------------------------\n",
      "Epoch 46: iteration 600, the loss is [ 0.17718777]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405151057219295, auc for val: 0.6422260615130218\n",
      "--------------------------------------------------------------\n",
      "Epoch 46: iteration 700, the loss is [ 0.11945626]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404347511506163, auc for val: 0.6421291067719237\n",
      "--------------------------------------------------------------\n",
      "Epoch 46: iteration 800, the loss is [ 0.21483716]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.640509772854545, auc for val: 0.6422350371542767\n",
      "--------------------------------------------------------------\n",
      "Epoch 46: iteration 900, the loss is [ 0.16386214]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405609381127445, auc for val: 0.6422127026821182\n",
      "--------------------------------------------------------------\n",
      "Epoch 46: iteration 1000, the loss is [ 0.14379412]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405256785850268, auc for val: 0.6422243944870232\n",
      "--------------------------------------------------------------\n",
      "Epoch 46: iteration 1100, the loss is [ 0.11910468]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406621928689666, auc for val: 0.6423094905792233\n",
      "--------------------------------------------------------------\n",
      "Epoch 47: iteration 0, the loss is [ 0.13211775]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405719545978149, auc for val: 0.6421890391137434\n",
      "--------------------------------------------------------------\n",
      "Epoch 47: iteration 100, the loss is [ 0.14840937]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404381789230419, auc for val: 0.6421141841320679\n",
      "--------------------------------------------------------------\n",
      "Epoch 47: iteration 200, the loss is [ 0.13199648]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.640516637441424, auc for val: 0.6422292584136011\n",
      "--------------------------------------------------------------\n",
      "Epoch 47: iteration 300, the loss is [ 0.16315611]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405779374061511, auc for val: 0.6421776513042496\n",
      "--------------------------------------------------------------\n",
      "Epoch 47: iteration 400, the loss is [ 0.09988081]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404850191181725, auc for val: 0.6421925428375982\n",
      "--------------------------------------------------------------\n",
      "Epoch 47: iteration 500, the loss is [ 0.19045249]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405317212314932, auc for val: 0.6421964721550284\n",
      "--------------------------------------------------------------\n",
      "Epoch 47: iteration 600, the loss is [ 0.16544734]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405275409212412, auc for val: 0.6421987938316103\n",
      "--------------------------------------------------------------\n",
      "Epoch 47: iteration 700, the loss is [ 0.14889371]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405771871589718, auc for val: 0.6421827669107494\n",
      "--------------------------------------------------------------\n",
      "Epoch 47: iteration 800, the loss is [ 0.1864963]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406714550399134, auc for val: 0.6422917867348388\n",
      "--------------------------------------------------------------\n",
      "Epoch 47: iteration 900, the loss is [ 0.19573565]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404952384178928, auc for val: 0.6422179526121716\n",
      "--------------------------------------------------------------\n",
      "Epoch 47: iteration 1000, the loss is [ 0.16331875]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405402464972749, auc for val: 0.6422163393155944\n",
      "--------------------------------------------------------------\n",
      "Epoch 47: iteration 1100, the loss is [ 0.16161354]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.640647102649011, auc for val: 0.6422537986199055\n",
      "--------------------------------------------------------------\n",
      "Epoch 48: iteration 0, the loss is [ 0.11277666]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.640411410622895, auc for val: 0.6421196998813646\n",
      "--------------------------------------------------------------\n",
      "Epoch 48: iteration 100, the loss is [ 0.13560762]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404855269586356, auc for val: 0.6422026878007367\n",
      "--------------------------------------------------------------\n",
      "Epoch 48: iteration 200, the loss is [ 0.14825341]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404320635786727, auc for val: 0.6421353351430212\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48: iteration 300, the loss is [ 0.18043232]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404343002548344, auc for val: 0.6421146394182184\n",
      "--------------------------------------------------------------\n",
      "Epoch 48: iteration 400, the loss is [ 0.14553396]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404829538386694, auc for val: 0.6422008397914236\n",
      "--------------------------------------------------------------\n",
      "Epoch 48: iteration 500, the loss is [ 0.14744569]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404602337146093, auc for val: 0.64213747018056\n",
      "--------------------------------------------------------------\n",
      "Epoch 48: iteration 600, the loss is [ 0.15659545]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404516974376471, auc for val: 0.642112924318527\n",
      "--------------------------------------------------------------\n",
      "Epoch 48: iteration 700, the loss is [ 0.13222709]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406806962735428, auc for val: 0.6423745823594393\n",
      "--------------------------------------------------------------\n",
      "Epoch 48: iteration 800, the loss is [ 0.12692571]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404783782913395, auc for val: 0.6421744643011953\n",
      "--------------------------------------------------------------\n",
      "Epoch 48: iteration 900, the loss is [ 0.1657567]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405495088289691, auc for val: 0.6422181123865041\n",
      "--------------------------------------------------------------\n",
      "Epoch 48: iteration 1000, the loss is [ 0.18057941]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.640569458475738, auc for val: 0.6421951260916267\n",
      "--------------------------------------------------------------\n",
      "Epoch 48: iteration 1100, the loss is [ 0.15013604]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404746868533858, auc for val: 0.6421646855464822\n",
      "--------------------------------------------------------------\n",
      "Epoch 49: iteration 0, the loss is [ 0.18703648]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404390142456553, auc for val: 0.6421249625368074\n",
      "--------------------------------------------------------------\n",
      "Epoch 49: iteration 100, the loss is [ 0.13348107]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404639419506412, auc for val: 0.6421445879149766\n",
      "--------------------------------------------------------------\n",
      "Epoch 49: iteration 200, the loss is [ 0.17805091]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405861089879957, auc for val: 0.6421811691674258\n",
      "--------------------------------------------------------------\n",
      "Epoch 49: iteration 300, the loss is [ 0.08938269]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405758384501183, auc for val: 0.6421747583990814\n",
      "--------------------------------------------------------------\n",
      "Epoch 49: iteration 400, the loss is [ 0.1516695]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404309781736662, auc for val: 0.6421404776142318\n",
      "--------------------------------------------------------------\n",
      "Epoch 49: iteration 500, the loss is [ 0.16175394]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404769168183537, auc for val: 0.6421720012313992\n",
      "--------------------------------------------------------------\n",
      "Epoch 49: iteration 600, the loss is [ 0.1695009]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405265856812469, auc for val: 0.6422137942377338\n",
      "--------------------------------------------------------------\n",
      "Epoch 49: iteration 700, the loss is [ 0.12396251]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405693306415596, auc for val: 0.642178465729165\n",
      "--------------------------------------------------------------\n",
      "Epoch 49: iteration 800, the loss is [ 0.16067135]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406175758472363, auc for val: 0.6422111897747234\n",
      "--------------------------------------------------------------\n",
      "Epoch 49: iteration 900, the loss is [ 0.19468783]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404400698320638, auc for val: 0.6421123248112974\n",
      "--------------------------------------------------------------\n",
      "Epoch 49: iteration 1000, the loss is [ 0.14641605]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405653673399726, auc for val: 0.6422411410993456\n",
      "--------------------------------------------------------------\n",
      "Epoch 49: iteration 1100, the loss is [ 0.18762785]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405298478840984, auc for val: 0.6422246631341306\n",
      "--------------------------------------------------------------\n",
      "Epoch 50: iteration 0, the loss is [ 0.14544168]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405623632970592, auc for val: 0.6422423768760401\n",
      "--------------------------------------------------------------\n",
      "Epoch 50: iteration 100, the loss is [ 0.16836266]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404240246534215, auc for val: 0.6421042993324438\n",
      "--------------------------------------------------------------\n",
      "Epoch 50: iteration 200, the loss is [ 0.18116544]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404302580665792, auc for val: 0.6421116333984787\n",
      "--------------------------------------------------------------\n",
      "Epoch 50: iteration 300, the loss is [ 0.17062135]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405077089413928, auc for val: 0.6422234881565183\n",
      "--------------------------------------------------------------\n",
      "Epoch 50: iteration 400, the loss is [ 0.14998052]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404773077956318, auc for val: 0.6421692779980883\n",
      "--------------------------------------------------------------\n",
      "Epoch 50: iteration 500, the loss is [ 0.16538864]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404924209622445, auc for val: 0.6422075814198899\n",
      "--------------------------------------------------------------\n",
      "Epoch 50: iteration 600, the loss is [ 0.16284709]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404826712451633, auc for val: 0.6421969726870078\n",
      "--------------------------------------------------------------\n",
      "Epoch 50: iteration 700, the loss is [ 0.13434774]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404173506323, auc for val: 0.6421049398437055\n",
      "--------------------------------------------------------------\n",
      "Epoch 50: iteration 800, the loss is [ 0.17156526]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405192356781337, auc for val: 0.642237549711698\n",
      "--------------------------------------------------------------\n",
      "Epoch 50: iteration 900, the loss is [ 0.18150152]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.640531281266518, auc for val: 0.6422013869831636\n",
      "--------------------------------------------------------------\n",
      "Epoch 50: iteration 1000, the loss is [ 0.17794946]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.64048108229968, auc for val: 0.6421926630218305\n",
      "--------------------------------------------------------------\n",
      "Epoch 50: iteration 1100, the loss is [ 0.20798562]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404662725433308, auc for val: 0.64214710188633\n",
      "--------------------------------------------------------------\n",
      "Epoch 51: iteration 0, the loss is [ 0.15424953]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.640431245697126, auc for val: 0.6421078497160593\n",
      "--------------------------------------------------------------\n",
      "Epoch 51: iteration 100, the loss is [ 0.14946738]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405283623794121, auc for val: 0.6422166475528021\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51: iteration 200, the loss is [ 0.18785766]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404812112992755, auc for val: 0.6421860613726467\n",
      "--------------------------------------------------------------\n",
      "Epoch 51: iteration 300, the loss is [ 0.11964673]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404608614322665, auc for val: 0.6421281650931152\n",
      "--------------------------------------------------------------\n",
      "Epoch 51: iteration 400, the loss is [ 0.16745371]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404296834758582, auc for val: 0.6421090515583825\n",
      "--------------------------------------------------------------\n",
      "Epoch 51: iteration 500, the loss is [ 0.19081527]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405286628962266, auc for val: 0.6421904035582632\n",
      "--------------------------------------------------------------\n",
      "Epoch 51: iteration 600, the loss is [ 0.17482972]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405251584072191, auc for val: 0.6422416600124428\n",
      "--------------------------------------------------------------\n",
      "Epoch 51: iteration 700, the loss is [ 0.14003414]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404849049073158, auc for val: 0.6421948220962156\n",
      "--------------------------------------------------------------\n",
      "Epoch 51: iteration 800, the loss is [ 0.14340143]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.640537362974544, auc for val: 0.6422152392763858\n",
      "--------------------------------------------------------------\n",
      "Epoch 51: iteration 900, the loss is [ 0.12193475]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404877079760933, auc for val: 0.6421973926248548\n",
      "--------------------------------------------------------------\n",
      "Epoch 51: iteration 1000, the loss is [ 0.12808265]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404231425935714, auc for val: 0.6420848761465713\n",
      "--------------------------------------------------------------\n",
      "Epoch 51: iteration 1100, the loss is [ 0.14956219]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405021105598881, auc for val: 0.6422171777773563\n",
      "--------------------------------------------------------------\n",
      "Epoch 52: iteration 0, the loss is [ 0.17588225]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404827404870022, auc for val: 0.6422038500529598\n",
      "--------------------------------------------------------------\n",
      "Epoch 52: iteration 100, the loss is [ 0.18370372]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405238593692378, auc for val: 0.6422379951003236\n",
      "--------------------------------------------------------------\n",
      "Epoch 52: iteration 200, the loss is [ 0.19234043]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404818881251898, auc for val: 0.6421911161800641\n",
      "--------------------------------------------------------------\n",
      "Epoch 52: iteration 300, the loss is [ 0.12059411]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404761306843709, auc for val: 0.6421632447496268\n",
      "--------------------------------------------------------------\n",
      "Epoch 52: iteration 400, the loss is [ 0.16067085]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404540858591274, auc for val: 0.6421115301814322\n",
      "--------------------------------------------------------------\n",
      "Epoch 52: iteration 500, the loss is [ 0.13429187]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404764967458395, auc for val: 0.6421770786617309\n",
      "--------------------------------------------------------------\n",
      "Epoch 52: iteration 600, the loss is [ 0.13061048]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404528579115772, auc for val: 0.6421433959701786\n",
      "--------------------------------------------------------------\n",
      "Epoch 52: iteration 700, the loss is [ 0.12417249]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404964808326872, auc for val: 0.6422044481462571\n",
      "--------------------------------------------------------------\n",
      "Epoch 52: iteration 800, the loss is [ 0.15186086]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404357483858061, auc for val: 0.6421077408432843\n",
      "--------------------------------------------------------------\n",
      "Epoch 52: iteration 900, the loss is [ 0.13612652]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405738128751635, auc for val: 0.6421860203686145\n",
      "--------------------------------------------------------------\n",
      "Epoch 52: iteration 1000, the loss is [ 0.14370158]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406053678238474, auc for val: 0.6421868036870226\n",
      "--------------------------------------------------------------\n",
      "Epoch 52: iteration 1100, the loss is [ 0.18108523]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404798812370924, auc for val: 0.6421942381422399\n",
      "--------------------------------------------------------------\n",
      "Epoch 53: iteration 0, the loss is [ 0.12917808]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406642449672024, auc for val: 0.6422953088398113\n",
      "--------------------------------------------------------------\n",
      "Epoch 53: iteration 100, the loss is [ 0.15723753]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.640336298498689, auc for val: 0.6419809662732532\n",
      "--------------------------------------------------------------\n",
      "Epoch 53: iteration 200, the loss is [ 0.11796007]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405186202576343, auc for val: 0.6422304602559241\n",
      "--------------------------------------------------------------\n",
      "Epoch 53: iteration 300, the loss is [ 0.17172028]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405744286573442, auc for val: 0.6421800889232672\n",
      "--------------------------------------------------------------\n",
      "Epoch 53: iteration 400, the loss is [ 0.2010386]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.640573256127377, auc for val: 0.6421864869662222\n",
      "--------------------------------------------------------------\n",
      "Epoch 53: iteration 500, the loss is [ 0.14319435]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405775968632941, auc for val: 0.6421815551709014\n",
      "--------------------------------------------------------------\n",
      "Epoch 53: iteration 600, the loss is [ 0.0998648]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404549062724417, auc for val: 0.6421078765807701\n",
      "--------------------------------------------------------------\n",
      "Epoch 53: iteration 700, the loss is [ 0.17886016]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404787885582769, auc for val: 0.642188556962882\n",
      "--------------------------------------------------------------\n",
      "Epoch 53: iteration 800, the loss is [ 0.18815668]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405809867395769, auc for val: 0.6421703257218077\n",
      "--------------------------------------------------------------\n",
      "Epoch 53: iteration 900, the loss is [ 0.16673455]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404230934049406, auc for val: 0.6420907453789043\n",
      "--------------------------------------------------------------\n",
      "Epoch 53: iteration 1000, the loss is [ 0.15349194]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405895877172707, auc for val: 0.6421775791937103\n",
      "--------------------------------------------------------------\n",
      "Epoch 53: iteration 1100, the loss is [ 0.14086306]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404817528966421, auc for val: 0.6421882897297068\n",
      "--------------------------------------------------------------\n",
      "Epoch 54: iteration 0, the loss is [ 0.19258474]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406863762744179, auc for val: 0.6423708919965414\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54: iteration 100, the loss is [ 0.13409941]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.640535021973476, auc for val: 0.6422022508957042\n",
      "--------------------------------------------------------------\n",
      "Epoch 54: iteration 200, the loss is [ 0.11104378]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404396336446472, auc for val: 0.6421043431643404\n",
      "--------------------------------------------------------------\n",
      "Epoch 54: iteration 300, the loss is [ 0.17271379]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404831237484168, auc for val: 0.6421777276565854\n",
      "--------------------------------------------------------------\n",
      "Epoch 54: iteration 400, the loss is [ 0.15544707]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404619891539629, auc for val: 0.6421233195476552\n",
      "--------------------------------------------------------------\n",
      "Epoch 54: iteration 500, the loss is [ 0.17436144]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405651541490526, auc for val: 0.6422385536035208\n",
      "--------------------------------------------------------------\n",
      "Epoch 54: iteration 600, the loss is [ 0.13496923]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406546565197089, auc for val: 0.6422600651671712\n",
      "--------------------------------------------------------------\n",
      "Epoch 54: iteration 700, the loss is [ 0.15311854]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404327011420935, auc for val: 0.6421241961855849\n",
      "--------------------------------------------------------------\n",
      "Epoch 54: iteration 800, the loss is [ 0.18285948]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.640573271840412, auc for val: 0.6422049218135255\n",
      "--------------------------------------------------------------\n",
      "Epoch 54: iteration 900, the loss is [ 0.15598217]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404604028206211, auc for val: 0.6421182802934913\n",
      "--------------------------------------------------------------\n",
      "Epoch 54: iteration 1000, the loss is [ 0.1506734]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404684313776806, auc for val: 0.6421530220202201\n",
      "--------------------------------------------------------------\n",
      "Epoch 54: iteration 1100, the loss is [ 0.16267611]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404359660374597, auc for val: 0.6421179084293372\n",
      "--------------------------------------------------------------\n",
      "Epoch 55: iteration 0, the loss is [ 0.17295784]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.640521472635603, auc for val: 0.6422335864598963\n",
      "--------------------------------------------------------------\n",
      "Epoch 55: iteration 100, the loss is [ 0.11955021]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405428174069676, auc for val: 0.6422086885287592\n",
      "--------------------------------------------------------------\n",
      "Epoch 55: iteration 200, the loss is [ 0.17954206]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404572100605426, auc for val: 0.6421126881918587\n",
      "--------------------------------------------------------------\n",
      "Epoch 55: iteration 300, the loss is [ 0.18284649]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404806654822959, auc for val: 0.6421853939966743\n",
      "--------------------------------------------------------------\n",
      "Epoch 55: iteration 400, the loss is [ 0.16865093]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405711044462761, auc for val: 0.6421961625038888\n",
      "--------------------------------------------------------------\n",
      "Epoch 55: iteration 500, the loss is [ 0.17320621]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.640523550533758, auc for val: 0.6422007097096663\n",
      "--------------------------------------------------------------\n",
      "Epoch 55: iteration 600, the loss is [ 0.14817593]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406693244568769, auc for val: 0.6422928853601152\n",
      "--------------------------------------------------------------\n",
      "Epoch 55: iteration 700, the loss is [ 0.13333219]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405675860124526, auc for val: 0.6422359689355601\n",
      "--------------------------------------------------------------\n",
      "Epoch 55: iteration 800, the loss is [ 0.1673978]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404611002221718, auc for val: 0.6421233944860588\n",
      "--------------------------------------------------------------\n",
      "Epoch 55: iteration 900, the loss is [ 0.09114946]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404359916364448, auc for val: 0.6421325101065961\n",
      "--------------------------------------------------------------\n",
      "Epoch 55: iteration 1000, the loss is [ 0.13495815]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405728523305129, auc for val: 0.6421784148276077\n",
      "--------------------------------------------------------------\n",
      "Epoch 55: iteration 1100, the loss is [ 0.15684898]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404830351365453, auc for val: 0.6421874441982842\n",
      "--------------------------------------------------------------\n",
      "Epoch 56: iteration 0, the loss is [ 0.13767111]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404916513450324, auc for val: 0.6421942296586469\n",
      "--------------------------------------------------------------\n",
      "Epoch 56: iteration 100, the loss is [ 0.1282367]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404677950198635, auc for val: 0.6421516816125469\n",
      "--------------------------------------------------------------\n",
      "Epoch 56: iteration 200, the loss is [ 0.16028167]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404191542556146, auc for val: 0.6421133414285095\n",
      "--------------------------------------------------------------\n",
      "Epoch 56: iteration 300, the loss is [ 0.11789512]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404932282344786, auc for val: 0.6422018168185356\n",
      "--------------------------------------------------------------\n",
      "Epoch 56: iteration 400, the loss is [ 0.22264609]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405473432432386, auc for val: 0.6422114103481379\n",
      "--------------------------------------------------------------\n",
      "Epoch 56: iteration 500, the loss is [ 0.21955217]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404623353631573, auc for val: 0.642141075707529\n",
      "--------------------------------------------------------------\n",
      "Epoch 56: iteration 600, the loss is [ 0.16020732]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405659041551113, auc for val: 0.6422211622381402\n",
      "--------------------------------------------------------------\n",
      "Epoch 56: iteration 700, the loss is [ 0.17038873]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405269169811423, auc for val: 0.6422379456126985\n",
      "--------------------------------------------------------------\n",
      "Epoch 56: iteration 800, the loss is [ 0.15201649]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404937662954078, auc for val: 0.6422157723288043\n",
      "--------------------------------------------------------------\n",
      "Epoch 56: iteration 900, the loss is [ 0.1755399]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406201775801967, auc for val: 0.6422219710073269\n",
      "--------------------------------------------------------------\n",
      "Epoch 56: iteration 1000, the loss is [ 0.16124359]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404195939392822, auc for val: 0.6421160052766705\n",
      "--------------------------------------------------------------\n",
      "Epoch 56: iteration 1100, the loss is [ 0.16928568]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.640492200216208, auc for val: 0.6421940359499431\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57: iteration 0, the loss is [ 0.12593019]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405431851160945, auc for val: 0.6422046291295715\n",
      "--------------------------------------------------------------\n",
      "Epoch 57: iteration 100, the loss is [ 0.13583386]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405185449074032, auc for val: 0.6422381972926202\n",
      "--------------------------------------------------------------\n",
      "Epoch 57: iteration 200, the loss is [ 0.12651226]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404821855476213, auc for val: 0.6421840083431725\n",
      "--------------------------------------------------------------\n",
      "Epoch 57: iteration 300, the loss is [ 0.17672005]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404793435780313, auc for val: 0.6421925682883767\n",
      "--------------------------------------------------------------\n",
      "Epoch 57: iteration 400, the loss is [ 0.12681355]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6403234559660755, auc for val: 0.6419672242667382\n",
      "--------------------------------------------------------------\n",
      "Epoch 57: iteration 500, the loss is [ 0.1495617]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405280499673079, auc for val: 0.6422100529732788\n",
      "--------------------------------------------------------------\n",
      "Epoch 57: iteration 600, the loss is [ 0.16546601]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405221557306564, auc for val: 0.6422287946438575\n",
      "--------------------------------------------------------------\n",
      "Epoch 57: iteration 700, the loss is [ 0.14417721]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404423994602704, auc for val: 0.6421381870441573\n",
      "--------------------------------------------------------------\n",
      "Epoch 57: iteration 800, the loss is [ 0.16107331]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404231073899436, auc for val: 0.6420994877213553\n",
      "--------------------------------------------------------------\n",
      "Epoch 57: iteration 900, the loss is [ 0.11666556]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405907899854622, auc for val: 0.6421773939685993\n",
      "--------------------------------------------------------------\n",
      "Epoch 57: iteration 1000, the loss is [ 0.11538514]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405855682747384, auc for val: 0.6421745180306169\n",
      "--------------------------------------------------------------\n",
      "Epoch 57: iteration 1100, the loss is [ 0.13923186]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405378459795712, auc for val: 0.6422267614094335\n",
      "--------------------------------------------------------------\n",
      "Epoch 58: iteration 0, the loss is [ 0.13223636]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405380096201794, auc for val: 0.6422169812407882\n",
      "--------------------------------------------------------------\n",
      "Epoch 58: iteration 100, the loss is [ 0.15066501]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.640536944991743, auc for val: 0.6422194301712629\n",
      "--------------------------------------------------------------\n",
      "Epoch 58: iteration 200, the loss is [ 0.22285429]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404306511737574, auc for val: 0.6420974657983882\n",
      "--------------------------------------------------------------\n",
      "Epoch 58: iteration 300, the loss is [ 0.19943993]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405396542645533, auc for val: 0.6422130773741366\n",
      "--------------------------------------------------------------\n",
      "Epoch 58: iteration 400, the loss is [ 0.16992694]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404829471274754, auc for val: 0.6421852582591884\n",
      "--------------------------------------------------------------\n",
      "Epoch 58: iteration 500, the loss is [ 0.18164992]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6407076863243962, auc for val: 0.6423883597142579\n",
      "--------------------------------------------------------------\n",
      "Epoch 58: iteration 600, the loss is [ 0.20665874]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405409603352228, auc for val: 0.6422118826014742\n",
      "--------------------------------------------------------------\n",
      "Epoch 58: iteration 700, the loss is [ 0.13793786]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404246237982711, auc for val: 0.6421177967286977\n",
      "--------------------------------------------------------------\n",
      "Epoch 58: iteration 800, the loss is [ 0.17974068]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405649526123016, auc for val: 0.6422388264924247\n",
      "--------------------------------------------------------------\n",
      "Epoch 58: iteration 900, the loss is [ 0.15922561]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406908325875423, auc for val: 0.6423786657954733\n",
      "--------------------------------------------------------------\n",
      "Epoch 58: iteration 1000, the loss is [ 0.14415872]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405658559711503, auc for val: 0.6422214464385012\n",
      "--------------------------------------------------------------\n",
      "Epoch 58: iteration 1100, the loss is [ 0.1932732]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405731299006703, auc for val: 0.6421732864957188\n",
      "--------------------------------------------------------------\n",
      "Epoch 59: iteration 0, the loss is [ 0.14582881]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404342794380772, auc for val: 0.6420746322081826\n",
      "--------------------------------------------------------------\n",
      "Epoch 59: iteration 100, the loss is [ 0.12858963]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404846455819609, auc for val: 0.6421888072288717\n",
      "--------------------------------------------------------------\n",
      "Epoch 59: iteration 200, the loss is [ 0.15717946]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404368803673021, auc for val: 0.6421156913837343\n",
      "--------------------------------------------------------------\n",
      "Epoch 59: iteration 300, the loss is [ 0.1423447]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406855203359811, auc for val: 0.6423213633674427\n",
      "--------------------------------------------------------------\n",
      "Epoch 59: iteration 400, the loss is [ 0.19457304]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405392362415653, auc for val: 0.642214656736342\n",
      "--------------------------------------------------------------\n",
      "Epoch 59: iteration 500, the loss is [ 0.15325269]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404689646563816, auc for val: 0.6421510114087102\n",
      "--------------------------------------------------------------\n",
      "Epoch 59: iteration 600, the loss is [ 0.18129751]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405671551698788, auc for val: 0.6421913155444965\n",
      "--------------------------------------------------------------\n",
      "Epoch 59: iteration 700, the loss is [ 0.13344611]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.640434452562768, auc for val: 0.6421302223643859\n",
      "--------------------------------------------------------------\n",
      "Epoch 59: iteration 800, the loss is [ 0.1312582]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.640591943587451, auc for val: 0.6422007959595272\n",
      "--------------------------------------------------------------\n",
      "Epoch 59: iteration 900, the loss is [ 0.15369298]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405875795833829, auc for val: 0.6421953099028055\n",
      "--------------------------------------------------------------\n",
      "Epoch 59: iteration 1000, the loss is [ 0.14676262]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6403993311570467, auc for val: 0.6420693752084684\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59: iteration 1100, the loss is [ 0.18208]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.640635887601012, auc for val: 0.6422335468697962\n",
      "--------------------------------------------------------------\n",
      "Learning rate is 0.0006241303598307956. Weight decay is 0.0026706282114709093. dropout is 0.5\n",
      " Val aus is 0.6422335468697962. Train auc is 0.640635887601012\n",
      "BLOWINGUP: Learning rate is 0.0006241303598307956. Weight decay is 0.0026706282114709093. dropout is 0.5\n",
      " Val aus is 0.6422335468697962. Train auc is 0.640635887601012\n",
      "WARNING: YOUR TRAINING IS BLOWING UP THIS TIME!\n",
      "Epoch 0: iteration 0, the loss is [ 0.72375709]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.3618346059936167, auc for val: 0.36121835495787485\n",
      "--------------------------------------------------------------\n",
      "Epoch 0: iteration 100, the loss is [ 0.59839362]\n",
      "  acc for train: 0.9634583980161691, acc for val: 0.9632\n",
      "  auc for train: 0.36894363123183477, auc for val: 0.3688886809214649\n",
      "--------------------------------------------------------------\n",
      "Epoch 0: iteration 200, the loss is [ 0.53822672]\n",
      "  acc for train: 0.958324092928234, acc for val: 0.95813\n",
      "  auc for train: 0.40643511292238615, auc for val: 0.4079481605193841\n",
      "--------------------------------------------------------------\n",
      "Epoch 0: iteration 300, the loss is [ 0.4537352]\n",
      "  acc for train: 0.9608475635571864, acc for val: 0.96057\n",
      "  auc for train: 0.439035182329859, auc for val: 0.4395897901192776\n",
      "--------------------------------------------------------------\n",
      "Epoch 0: iteration 400, the loss is [ 0.40955666]\n",
      "  acc for train: 0.9628972534155897, acc for val: 0.96259\n",
      "  auc for train: 0.4602228800901167, auc for val: 0.46052731287209686\n",
      "--------------------------------------------------------------\n",
      "Epoch 0: iteration 500, the loss is [ 0.34970352]\n",
      "  acc for train: 0.9635222408150373, acc for val: 0.96325\n",
      "  auc for train: 0.45547543280218616, auc for val: 0.45716730108058146\n",
      "--------------------------------------------------------------\n",
      "Epoch 0: iteration 600, the loss is [ 0.31652662]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.4420791573695506, auc for val: 0.44527192388111003\n",
      "--------------------------------------------------------------\n",
      "Epoch 0: iteration 700, the loss is [ 0.30246302]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.4448751968346153, auc for val: 0.44904424394176523\n",
      "--------------------------------------------------------------\n",
      "Epoch 0: iteration 800, the loss is [ 0.31501189]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.4590391479349526, auc for val: 0.4623567629119626\n",
      "--------------------------------------------------------------\n",
      "Epoch 0: iteration 900, the loss is [ 0.29512733]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.4653229449187066, auc for val: 0.4709350807506121\n",
      "--------------------------------------------------------------\n",
      "Epoch 0: iteration 1000, the loss is [ 0.24775003]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.5028303067809415, auc for val: 0.5076929403943133\n",
      "--------------------------------------------------------------\n",
      "Epoch 0: iteration 1100, the loss is [ 0.24585184]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.5339954135035382, auc for val: 0.5358640721309679\n",
      "--------------------------------------------------------------\n",
      "Epoch 1: iteration 0, the loss is [ 0.25050053]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.539605892919875, auc for val: 0.5438451393759434\n",
      "--------------------------------------------------------------\n",
      "Epoch 1: iteration 100, the loss is [ 0.22973295]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.560280229998823, auc for val: 0.5610443414804602\n",
      "--------------------------------------------------------------\n",
      "Epoch 1: iteration 200, the loss is [ 0.21088168]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.5563942871384835, auc for val: 0.5567307399161492\n",
      "--------------------------------------------------------------\n",
      "Epoch 1: iteration 300, the loss is [ 0.24547826]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.5608358982876117, auc for val: 0.5623060440347843\n",
      "--------------------------------------------------------------\n",
      "Epoch 1: iteration 400, the loss is [ 0.2234281]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.5757383315631112, auc for val: 0.5763159044412511\n",
      "--------------------------------------------------------------\n",
      "Epoch 1: iteration 500, the loss is [ 0.2094008]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.5907521161640967, auc for val: 0.5883413973324116\n",
      "--------------------------------------------------------------\n",
      "Epoch 1: iteration 600, the loss is [ 0.18977809]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.575105278546965, auc for val: 0.5735110603809854\n",
      "--------------------------------------------------------------\n",
      "Epoch 1: iteration 700, the loss is [ 0.20642185]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.5822985494424214, auc for val: 0.5800813542056149\n",
      "--------------------------------------------------------------\n",
      "Epoch 1: iteration 800, the loss is [ 0.24900863]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.5942957545392781, auc for val: 0.5908475326738439\n",
      "--------------------------------------------------------------\n",
      "Epoch 1: iteration 900, the loss is [ 0.19164446]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6029932936545539, auc for val: 0.6006874724726854\n",
      "--------------------------------------------------------------\n",
      "Epoch 1: iteration 1000, the loss is [ 0.23081553]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6014283361082788, auc for val: 0.5998305263759158\n",
      "--------------------------------------------------------------\n",
      "Epoch 1: iteration 1100, the loss is [ 0.15947509]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6215773319799457, auc for val: 0.6221756298902982\n",
      "--------------------------------------------------------------\n",
      "Epoch 2: iteration 0, the loss is [ 0.20543456]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6237240624705088, auc for val: 0.6242887005790807\n",
      "--------------------------------------------------------------\n",
      "Epoch 2: iteration 100, the loss is [ 0.16911046]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6188301707428709, auc for val: 0.6202221539644908\n",
      "--------------------------------------------------------------\n",
      "Epoch 2: iteration 200, the loss is [ 0.17378992]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6210534552194725, auc for val: 0.6220474823921545\n",
      "--------------------------------------------------------------\n",
      "Epoch 2: iteration 300, the loss is [ 0.22828284]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6314459957703056, auc for val: 0.6341582127687297\n",
      "--------------------------------------------------------------\n",
      "Epoch 2: iteration 400, the loss is [ 0.16067894]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6383742507007869, auc for val: 0.6402671490181179\n",
      "--------------------------------------------------------------\n",
      "Epoch 2: iteration 500, the loss is [ 0.17329894]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6391328497399503, auc for val: 0.6411976733158362\n",
      "--------------------------------------------------------------\n",
      "Epoch 2: iteration 600, the loss is [ 0.16004789]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404791704533406, auc for val: 0.6424893441619686\n",
      "--------------------------------------------------------------\n",
      "Epoch 2: iteration 700, the loss is [ 0.20581819]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6395426899338273, auc for val: 0.6416564108748135\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: iteration 800, the loss is [ 0.18292734]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6408074564600411, auc for val: 0.6424712472444478\n",
      "--------------------------------------------------------------\n",
      "Epoch 2: iteration 900, the loss is [ 0.20819022]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6402649091983197, auc for val: 0.6423129009835564\n",
      "--------------------------------------------------------------\n",
      "Epoch 2: iteration 1000, the loss is [ 0.18566027]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6397002750357855, auc for val: 0.6411602578434217\n",
      "--------------------------------------------------------------\n",
      "Epoch 2: iteration 1100, the loss is [ 0.16648756]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6407774405850345, auc for val: 0.6425852186589079\n",
      "--------------------------------------------------------------\n",
      "Epoch 3: iteration 0, the loss is [ 0.17679201]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6407508538488038, auc for val: 0.6425411506357523\n",
      "--------------------------------------------------------------\n",
      "Epoch 3: iteration 100, the loss is [ 0.1615518]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404862465834811, auc for val: 0.6424739125065406\n",
      "--------------------------------------------------------------\n",
      "Epoch 3: iteration 200, the loss is [ 0.16610382]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6402124676076753, auc for val: 0.6421134446455562\n",
      "--------------------------------------------------------------\n",
      "Epoch 3: iteration 300, the loss is [ 0.20721771]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6400434447425288, auc for val: 0.6420341131547103\n",
      "--------------------------------------------------------------\n",
      "Epoch 3: iteration 400, the loss is [ 0.18122478]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6407616680335743, auc for val: 0.6426325472095895\n",
      "--------------------------------------------------------------\n",
      "Epoch 3: iteration 500, the loss is [ 0.19860537]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.640887624484276, auc for val: 0.6427408049242529\n",
      "--------------------------------------------------------------\n",
      "Epoch 3: iteration 600, the loss is [ 0.15913121]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6407545836651418, auc for val: 0.6425010882823629\n",
      "--------------------------------------------------------------\n",
      "Epoch 3: iteration 700, the loss is [ 0.17153156]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406776905427841, auc for val: 0.6421392220424873\n",
      "--------------------------------------------------------------\n",
      "Epoch 3: iteration 800, the loss is [ 0.17425755]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406143758937223, auc for val: 0.6419575968027647\n",
      "--------------------------------------------------------------\n",
      "Epoch 3: iteration 900, the loss is [ 0.1459762]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6403695444271156, auc for val: 0.64161074228047\n",
      "--------------------------------------------------------------\n",
      "Epoch 3: iteration 1000, the loss is [ 0.17089297]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6407057764472094, auc for val: 0.6418462835807365\n",
      "--------------------------------------------------------------\n",
      "Epoch 3: iteration 1100, the loss is [ 0.16009606]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6408469566987561, auc for val: 0.6419598223319605\n",
      "--------------------------------------------------------------\n",
      "Epoch 4: iteration 0, the loss is [ 0.18191302]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6403841100487168, auc for val: 0.641519386710665\n",
      "--------------------------------------------------------------\n",
      "Epoch 4: iteration 100, the loss is [ 0.17362642]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6408702492836372, auc for val: 0.6421816711133371\n",
      "--------------------------------------------------------------\n",
      "Epoch 4: iteration 200, the loss is [ 0.20007011]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6408749092232259, auc for val: 0.6421739538716911\n",
      "--------------------------------------------------------------\n",
      "Epoch 4: iteration 300, the loss is [ 0.15863149]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6408627837836902, auc for val: 0.642323768466021\n",
      "--------------------------------------------------------------\n",
      "Epoch 4: iteration 400, the loss is [ 0.19670178]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6409030638872484, auc for val: 0.6427490948084176\n",
      "--------------------------------------------------------------\n",
      "Epoch 4: iteration 500, the loss is [ 0.17165226]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6409841136498388, auc for val: 0.6426689389951312\n",
      "--------------------------------------------------------------\n",
      "Epoch 4: iteration 600, the loss is [ 0.217748]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6409863315185829, auc for val: 0.6425815622303817\n",
      "--------------------------------------------------------------\n",
      "Epoch 4: iteration 700, the loss is [ 0.13985491]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6409730431537228, auc for val: 0.6423954930019281\n",
      "--------------------------------------------------------------\n",
      "Epoch 4: iteration 800, the loss is [ 0.15544607]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6411481168232539, auc for val: 0.6425591740288016\n",
      "--------------------------------------------------------------\n",
      "Epoch 4: iteration 900, the loss is [ 0.11427885]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6411434773789281, auc for val: 0.6425457897471194\n",
      "--------------------------------------------------------------\n",
      "Epoch 4: iteration 1000, the loss is [ 0.1644191]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.64119054089205, auc for val: 0.6425863271817095\n",
      "--------------------------------------------------------------\n",
      "Epoch 4: iteration 1100, the loss is [ 0.13707595]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6411237780556732, auc for val: 0.6425292608803472\n",
      "--------------------------------------------------------------\n",
      "Epoch 5: iteration 0, the loss is [ 0.13857335]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6411199452405928, auc for val: 0.6425651818264848\n",
      "--------------------------------------------------------------\n",
      "Epoch 5: iteration 100, the loss is [ 0.17811784]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6411394109982254, auc for val: 0.6424459519983781\n",
      "--------------------------------------------------------------\n",
      "Epoch 5: iteration 200, the loss is [ 0.17510627]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6411242916428472, auc for val: 0.64242190384046\n",
      "--------------------------------------------------------------\n",
      "Epoch 5: iteration 300, the loss is [ 0.18770985]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6411741337911782, auc for val: 0.6424160176409415\n",
      "--------------------------------------------------------------\n",
      "Epoch 5: iteration 400, the loss is [ 0.17619193]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6411395964602604, auc for val: 0.6423175202998733\n",
      "--------------------------------------------------------------\n",
      "Epoch 5: iteration 500, the loss is [ 0.14392063]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6412247642837655, auc for val: 0.6424280502034933\n",
      "--------------------------------------------------------------\n",
      "Epoch 5: iteration 600, the loss is [ 0.19307184]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.641254018217237, auc for val: 0.6424476967906447\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: iteration 700, the loss is [ 0.12625016]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6412495521789092, auc for val: 0.6426763126512662\n",
      "--------------------------------------------------------------\n",
      "Epoch 5: iteration 800, the loss is [ 0.19907188]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413283024533857, auc for val: 0.6425937969852302\n",
      "--------------------------------------------------------------\n",
      "Epoch 5: iteration 900, the loss is [ 0.15510945]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6412682119904718, auc for val: 0.6424564037847921\n",
      "--------------------------------------------------------------\n",
      "Epoch 5: iteration 1000, the loss is [ 0.17080772]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413354005255134, auc for val: 0.6426735653811089\n",
      "--------------------------------------------------------------\n",
      "Epoch 5: iteration 1100, the loss is [ 0.17987826]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6412807941930609, auc for val: 0.6426212809982601\n",
      "--------------------------------------------------------------\n",
      "Epoch 6: iteration 0, the loss is [ 0.16817415]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6412786831006174, auc for val: 0.6425148514978599\n",
      "--------------------------------------------------------------\n",
      "Epoch 6: iteration 100, the loss is [ 0.195068]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413088055509909, auc for val: 0.6425708333132674\n",
      "--------------------------------------------------------------\n",
      "Epoch 6: iteration 200, the loss is [ 0.16475424]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413397177923449, auc for val: 0.6426587827205354\n",
      "--------------------------------------------------------------\n",
      "Epoch 6: iteration 300, the loss is [ 0.15742022]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413056912355259, auc for val: 0.6425832278424483\n",
      "--------------------------------------------------------------\n",
      "Epoch 6: iteration 400, the loss is [ 0.13233785]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413592159003434, auc for val: 0.6429427879589957\n",
      "--------------------------------------------------------------\n",
      "Epoch 6: iteration 500, the loss is [ 0.13816589]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413760900118998, auc for val: 0.6429921582276943\n",
      "--------------------------------------------------------------\n",
      "Epoch 6: iteration 600, the loss is [ 0.13468459]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413859099754894, auc for val: 0.6429973544283262\n",
      "--------------------------------------------------------------\n",
      "Epoch 6: iteration 700, the loss is [ 0.15144099]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413058644004034, auc for val: 0.6429707343278368\n",
      "--------------------------------------------------------------\n",
      "Epoch 6: iteration 800, the loss is [ 0.11033799]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413507733389748, auc for val: 0.6429577219103086\n",
      "--------------------------------------------------------------\n",
      "Epoch 6: iteration 900, the loss is [ 0.16298911]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413759995514355, auc for val: 0.6430380657765702\n",
      "--------------------------------------------------------------\n",
      "Epoch 6: iteration 1000, the loss is [ 0.13513725]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413463399725075, auc for val: 0.6430860376663775\n",
      "--------------------------------------------------------------\n",
      "Epoch 6: iteration 1100, the loss is [ 0.14326566]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413127226377694, auc for val: 0.6430750429300198\n",
      "--------------------------------------------------------------\n",
      "Epoch 7: iteration 0, the loss is [ 0.14291668]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413292406943678, auc for val: 0.6430123746295\n",
      "--------------------------------------------------------------\n",
      "Epoch 7: iteration 100, the loss is [ 0.1984802]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6412524049989328, auc for val: 0.6431280597297198\n",
      "--------------------------------------------------------------\n",
      "Epoch 7: iteration 200, the loss is [ 0.1485763]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.641281965517424, auc for val: 0.6427954802663585\n",
      "--------------------------------------------------------------\n",
      "Epoch 7: iteration 300, the loss is [ 0.15642588]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413041004816372, auc for val: 0.6427381198671099\n",
      "--------------------------------------------------------------\n",
      "Epoch 7: iteration 400, the loss is [ 0.17626841]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6412690787793418, auc for val: 0.6427201671706679\n",
      "--------------------------------------------------------------\n",
      "Epoch 7: iteration 500, the loss is [ 0.18010899]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413160050143893, auc for val: 0.6428081010246826\n",
      "--------------------------------------------------------------\n",
      "Epoch 7: iteration 600, the loss is [ 0.17841686]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413468634456319, auc for val: 0.6429014502388115\n",
      "--------------------------------------------------------------\n",
      "Epoch 7: iteration 700, the loss is [ 0.14209563]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413294498666089, auc for val: 0.6429686417082626\n",
      "--------------------------------------------------------------\n",
      "Epoch 7: iteration 800, the loss is [ 0.16415632]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6412409919908071, auc for val: 0.6429704190209686\n",
      "--------------------------------------------------------------\n",
      "Epoch 7: iteration 900, the loss is [ 0.16911262]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413422157630143, auc for val: 0.6428999613682631\n",
      "--------------------------------------------------------------\n",
      "Epoch 7: iteration 1000, the loss is [ 0.16185339]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413339498627738, auc for val: 0.6429724904315606\n",
      "--------------------------------------------------------------\n",
      "Epoch 7: iteration 1100, the loss is [ 0.14625469]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6412680712965206, auc for val: 0.6428766993566175\n",
      "--------------------------------------------------------------\n",
      "Epoch 8: iteration 0, the loss is [ 0.14567907]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6412643927987133, auc for val: 0.6429654433937512\n",
      "--------------------------------------------------------------\n",
      "Epoch 8: iteration 100, the loss is [ 0.13348247]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413247753794022, auc for val: 0.642997869099627\n",
      "--------------------------------------------------------------\n",
      "Epoch 8: iteration 200, the loss is [ 0.15758739]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6412818964765191, auc for val: 0.6427874024720155\n",
      "--------------------------------------------------------------\n",
      "Epoch 8: iteration 300, the loss is [ 0.19722483]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6412629896367597, auc for val: 0.6429545575301687\n",
      "--------------------------------------------------------------\n",
      "Epoch 8: iteration 400, the loss is [ 0.18349957]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413366674944362, auc for val: 0.6428809072186805\n",
      "--------------------------------------------------------------\n",
      "Epoch 8: iteration 500, the loss is [ 0.13936363]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413388910295178, auc for val: 0.6428829206580546\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: iteration 600, the loss is [ 0.16101722]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6412264657522602, auc for val: 0.6430053473867405\n",
      "--------------------------------------------------------------\n",
      "Epoch 8: iteration 700, the loss is [ 0.14648403]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.641158570411959, auc for val: 0.6426527098819738\n",
      "--------------------------------------------------------------\n",
      "Epoch 8: iteration 800, the loss is [ 0.16347633]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.641181739703769, auc for val: 0.6426810917419155\n",
      "--------------------------------------------------------------\n",
      "Epoch 8: iteration 900, the loss is [ 0.14594373]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6412883534889753, auc for val: 0.6428222940755517\n",
      "--------------------------------------------------------------\n",
      "Epoch 8: iteration 1000, the loss is [ 0.15691836]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6412562185645406, auc for val: 0.6428329395706696\n",
      "--------------------------------------------------------------\n",
      "Epoch 8: iteration 1100, the loss is [ 0.20974596]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6412721050857368, auc for val: 0.6429360477444617\n",
      "--------------------------------------------------------------\n",
      "Epoch 9: iteration 0, the loss is [ 0.18209574]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6412644824956282, auc for val: 0.6429133979654348\n",
      "--------------------------------------------------------------\n",
      "Epoch 9: iteration 100, the loss is [ 0.17235705]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6412556203439872, auc for val: 0.6429886488481109\n",
      "--------------------------------------------------------------\n",
      "Epoch 9: iteration 200, the loss is [ 0.14400263]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413031231790955, auc for val: 0.6428833080754622\n",
      "--------------------------------------------------------------\n",
      "Epoch 9: iteration 300, the loss is [ 0.18579078]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6412343465023066, auc for val: 0.6428312075037922\n",
      "--------------------------------------------------------------\n",
      "Epoch 9: iteration 400, the loss is [ 0.14810432]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6411424104598449, auc for val: 0.6426507402744961\n",
      "--------------------------------------------------------------\n",
      "Epoch 9: iteration 500, the loss is [ 0.14845373]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6411975621672221, auc for val: 0.6427373733109375\n",
      "--------------------------------------------------------------\n",
      "Epoch 9: iteration 600, the loss is [ 0.1527826]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6410845056760566, auc for val: 0.6425019083630068\n",
      "--------------------------------------------------------------\n",
      "Epoch 9: iteration 700, the loss is [ 0.11417346]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6411052566474147, auc for val: 0.6424197857701072\n",
      "--------------------------------------------------------------\n",
      "Epoch 9: iteration 800, the loss is [ 0.18697523]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6412598358176802, auc for val: 0.6428135489052363\n",
      "--------------------------------------------------------------\n",
      "Epoch 9: iteration 900, the loss is [ 0.13851573]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6412643629399285, auc for val: 0.6429925527147626\n",
      "--------------------------------------------------------------\n",
      "Epoch 9: iteration 1000, the loss is [ 0.15852588]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6412683433610877, auc for val: 0.6426846025354309\n",
      "--------------------------------------------------------------\n",
      "Epoch 9: iteration 1100, the loss is [ 0.17514169]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6412895396020772, auc for val: 0.6427586558175804\n",
      "--------------------------------------------------------------\n",
      "Epoch 10: iteration 0, the loss is [ 0.12642649]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413276304096992, auc for val: 0.6429268600133851\n",
      "--------------------------------------------------------------\n",
      "Epoch 10: iteration 100, the loss is [ 0.13549046]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6412789230559395, auc for val: 0.6428556798413536\n",
      "--------------------------------------------------------------\n",
      "Epoch 10: iteration 200, the loss is [ 0.15087681]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413375663121778, auc for val: 0.6428744625159646\n",
      "--------------------------------------------------------------\n",
      "Epoch 10: iteration 300, the loss is [ 0.15578458]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413341589546413, auc for val: 0.6428274266492371\n",
      "--------------------------------------------------------------\n",
      "Epoch 10: iteration 400, the loss is [ 0.20020321]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413133336779089, auc for val: 0.6427643596198523\n",
      "--------------------------------------------------------------\n",
      "Epoch 10: iteration 500, the loss is [ 0.16502349]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413320968498948, auc for val: 0.6427940677481458\n",
      "--------------------------------------------------------------\n",
      "Epoch 10: iteration 600, the loss is [ 0.17070329]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413184867897821, auc for val: 0.642867374474123\n",
      "--------------------------------------------------------------\n",
      "Epoch 10: iteration 700, the loss is [ 0.17656393]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413577482787787, auc for val: 0.6429888142781719\n",
      "--------------------------------------------------------------\n",
      "Epoch 10: iteration 800, the loss is [ 0.12546378]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413645893563855, auc for val: 0.6430292258728012\n",
      "--------------------------------------------------------------\n",
      "Epoch 10: iteration 900, the loss is [ 0.17474261]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413642634817067, auc for val: 0.6427542443492887\n",
      "--------------------------------------------------------------\n",
      "Epoch 10: iteration 1000, the loss is [ 0.14904879]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413809044295081, auc for val: 0.6428975463721599\n",
      "--------------------------------------------------------------\n",
      "Epoch 10: iteration 1100, the loss is [ 0.18409677]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413757949604886, auc for val: 0.6430646165943844\n",
      "--------------------------------------------------------------\n",
      "Epoch 11: iteration 0, the loss is [ 0.15921094]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413678247144611, auc for val: 0.6430414111400247\n",
      "--------------------------------------------------------------\n",
      "Epoch 11: iteration 100, the loss is [ 0.13072121]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413385584034583, auc for val: 0.6429810136145293\n",
      "--------------------------------------------------------------\n",
      "Epoch 11: iteration 200, the loss is [ 0.14331809]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413567738294988, auc for val: 0.6428891476152201\n",
      "--------------------------------------------------------------\n",
      "Epoch 11: iteration 300, the loss is [ 0.12074091]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413616146096417, auc for val: 0.642843240066344\n",
      "--------------------------------------------------------------\n",
      "Epoch 11: iteration 400, the loss is [ 0.16289777]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.64137512757847, auc for val: 0.6429688933881843\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: iteration 500, the loss is [ 0.17340796]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413679085039179, auc for val: 0.6428830252890332\n",
      "--------------------------------------------------------------\n",
      "Epoch 11: iteration 600, the loss is [ 0.13652141]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413630739929144, auc for val: 0.6429244817795174\n",
      "--------------------------------------------------------------\n",
      "Epoch 11: iteration 700, the loss is [ 0.15047871]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413600222482813, auc for val: 0.6431005799584864\n",
      "--------------------------------------------------------------\n",
      "Epoch 11: iteration 800, the loss is [ 0.14771162]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6412690706616102, auc for val: 0.643168021693927\n",
      "--------------------------------------------------------------\n",
      "Epoch 11: iteration 900, the loss is [ 0.18699712]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6412975617292505, auc for val: 0.6429601680129191\n",
      "--------------------------------------------------------------\n",
      "Epoch 11: iteration 1000, the loss is [ 0.15373991]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.641375023333937, auc for val: 0.6430902441145082\n",
      "--------------------------------------------------------------\n",
      "Epoch 11: iteration 1100, the loss is [ 0.14916106]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413382822398386, auc for val: 0.642872806801423\n",
      "--------------------------------------------------------------\n",
      "Epoch 12: iteration 0, the loss is [ 0.12272305]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413407470162193, auc for val: 0.643035867112085\n",
      "--------------------------------------------------------------\n",
      "Epoch 12: iteration 100, the loss is [ 0.14704145]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413657885703808, auc for val: 0.6429971819286044\n",
      "--------------------------------------------------------------\n",
      "Epoch 12: iteration 200, the loss is [ 0.13334154]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.641365042060573, auc for val: 0.6430765558374147\n",
      "--------------------------------------------------------------\n",
      "Epoch 12: iteration 300, the loss is [ 0.15234178]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6411585931576821, auc for val: 0.6428577752887922\n",
      "--------------------------------------------------------------\n",
      "Epoch 12: iteration 400, the loss is [ 0.15830183]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.641334871506612, auc for val: 0.6429880974145745\n",
      "--------------------------------------------------------------\n",
      "Epoch 12: iteration 500, the loss is [ 0.16316624]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413114413221623, auc for val: 0.6428341583801782\n",
      "--------------------------------------------------------------\n",
      "Epoch 12: iteration 600, the loss is [ 0.14465193]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413303482422956, auc for val: 0.6430100232603432\n",
      "--------------------------------------------------------------\n",
      "Epoch 12: iteration 700, the loss is [ 0.12945977]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413267018135467, auc for val: 0.6430815795383251\n",
      "--------------------------------------------------------------\n",
      "Epoch 12: iteration 800, the loss is [ 0.18322049]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.641289569541236, auc for val: 0.643019397630463\n",
      "--------------------------------------------------------------\n",
      "Epoch 12: iteration 900, the loss is [ 0.17483039]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6412569015390339, auc for val: 0.6429928227758022\n",
      "--------------------------------------------------------------\n",
      "Epoch 12: iteration 1000, the loss is [ 0.12978506]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6412249922232374, auc for val: 0.643022181662856\n",
      "--------------------------------------------------------------\n",
      "Epoch 12: iteration 1100, the loss is [ 0.17048554]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413323960003582, auc for val: 0.6431014594242803\n",
      "--------------------------------------------------------------\n",
      "Epoch 13: iteration 0, the loss is [ 0.16507237]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.641313943994686, auc for val: 0.6430583048012903\n",
      "--------------------------------------------------------------\n",
      "Epoch 13: iteration 100, the loss is [ 0.16747059]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6412848766886662, auc for val: 0.6430631517606825\n",
      "--------------------------------------------------------------\n",
      "Epoch 13: iteration 200, the loss is [ 0.1549264]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6412946699682278, auc for val: 0.6430926237623078\n",
      "--------------------------------------------------------------\n",
      "Epoch 13: iteration 300, the loss is [ 0.12647101]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6412783360876871, auc for val: 0.6428861727019874\n",
      "--------------------------------------------------------------\n",
      "Epoch 13: iteration 400, the loss is [ 0.15519199]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413333569066899, auc for val: 0.6429927874274985\n",
      "--------------------------------------------------------------\n",
      "Epoch 13: iteration 500, the loss is [ 0.12499294]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413506102609818, auc for val: 0.6430204156616072\n",
      "--------------------------------------------------------------\n",
      "Epoch 13: iteration 600, the loss is [ 0.15654355]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413304781260002, auc for val: 0.6430840341245283\n",
      "--------------------------------------------------------------\n",
      "Epoch 13: iteration 700, the loss is [ 0.13265383]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413562675563204, auc for val: 0.6430991009854629\n",
      "--------------------------------------------------------------\n",
      "Epoch 13: iteration 800, the loss is [ 0.16350734]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413582512968271, auc for val: 0.6431248614152083\n",
      "--------------------------------------------------------------\n",
      "Epoch 13: iteration 900, the loss is [ 0.15815134]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413817311923355, auc for val: 0.6430083081606517\n",
      "--------------------------------------------------------------\n",
      "Epoch 13: iteration 1000, the loss is [ 0.16126578]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413945808380108, auc for val: 0.6429955573205702\n",
      "--------------------------------------------------------------\n",
      "Epoch 13: iteration 1100, the loss is [ 0.14612246]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413745969520969, auc for val: 0.6430376104904196\n",
      "--------------------------------------------------------------\n",
      "Epoch 14: iteration 0, the loss is [ 0.15256467]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413947982083569, auc for val: 0.6429662959948345\n",
      "--------------------------------------------------------------\n",
      "Epoch 14: iteration 100, the loss is [ 0.15237151]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413516440259635, auc for val: 0.6428845792004604\n",
      "--------------------------------------------------------------\n",
      "Epoch 14: iteration 200, the loss is [ 0.12217382]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413455259087057, auc for val: 0.6427570354513426\n",
      "--------------------------------------------------------------\n",
      "Epoch 14: iteration 300, the loss is [ 0.14796183]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413680166465694, auc for val: 0.6429146860576187\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: iteration 400, the loss is [ 0.18012829]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413629885156122, auc for val: 0.6430525925154255\n",
      "--------------------------------------------------------------\n",
      "Epoch 14: iteration 500, the loss is [ 0.19677559]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413431786384985, auc for val: 0.6428461527665623\n",
      "--------------------------------------------------------------\n",
      "Epoch 14: iteration 600, the loss is [ 0.18527341]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413755207258349, auc for val: 0.6428948669707456\n",
      "--------------------------------------------------------------\n",
      "Epoch 14: iteration 700, the loss is [ 0.16303727]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413747213703984, auc for val: 0.6429328593274752\n",
      "--------------------------------------------------------------\n",
      "Epoch 14: iteration 800, the loss is [ 0.18641032]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413794189650088, auc for val: 0.6429922515472157\n",
      "--------------------------------------------------------------\n",
      "Epoch 14: iteration 900, the loss is [ 0.16101578]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413302911368672, auc for val: 0.6430639209397693\n",
      "--------------------------------------------------------------\n",
      "Epoch 14: iteration 1000, the loss is [ 0.17417659]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413652970457544, auc for val: 0.6429380258355322\n",
      "--------------------------------------------------------------\n",
      "Epoch 14: iteration 1100, the loss is [ 0.19618553]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413776115641363, auc for val: 0.643193007288857\n",
      "--------------------------------------------------------------\n",
      "Epoch 15: iteration 0, the loss is [ 0.16952181]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413858416981336, auc for val: 0.6430780277407775\n",
      "--------------------------------------------------------------\n",
      "Epoch 15: iteration 100, the loss is [ 0.15109789]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413803920077512, auc for val: 0.643084743918465\n",
      "--------------------------------------------------------------\n",
      "Epoch 15: iteration 200, the loss is [ 0.10711596]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.641379204809606, auc for val: 0.6429393775546626\n",
      "--------------------------------------------------------------\n",
      "Epoch 15: iteration 300, the loss is [ 0.1532813]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413843259327946, auc for val: 0.6429865802653831\n",
      "--------------------------------------------------------------\n",
      "Epoch 15: iteration 400, the loss is [ 0.1454664]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413913949096864, auc for val: 0.6431203877339022\n",
      "--------------------------------------------------------------\n",
      "Epoch 15: iteration 500, the loss is [ 0.12955375]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413890450675247, auc for val: 0.6430402828221731\n",
      "--------------------------------------------------------------\n",
      "Epoch 15: iteration 600, the loss is [ 0.18732777]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413965055843098, auc for val: 0.6430710853339467\n",
      "--------------------------------------------------------------\n",
      "Epoch 15: iteration 700, the loss is [ 0.12788729]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413733704512714, auc for val: 0.6430349240193445\n",
      "--------------------------------------------------------------\n",
      "Epoch 15: iteration 800, the loss is [ 0.14692844]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413735139984846, auc for val: 0.6430599407207818\n",
      "--------------------------------------------------------------\n",
      "Epoch 15: iteration 900, the loss is [ 0.18569347]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413598685338101, auc for val: 0.6430993865997562\n",
      "--------------------------------------------------------------\n",
      "Epoch 15: iteration 1000, the loss is [ 0.15296128]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413540258166233, auc for val: 0.6430169317328025\n",
      "--------------------------------------------------------------\n",
      "Epoch 15: iteration 1100, the loss is [ 0.19950438]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413787076588289, auc for val: 0.6430341689795793\n",
      "--------------------------------------------------------------\n",
      "Epoch 16: iteration 0, the loss is [ 0.1225433]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413832435417973, auc for val: 0.642948193421585\n",
      "--------------------------------------------------------------\n",
      "Epoch 16: iteration 100, the loss is [ 0.15608653]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413719541482371, auc for val: 0.6429263736207272\n",
      "--------------------------------------------------------------\n",
      "Epoch 16: iteration 200, the loss is [ 0.12554479]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413728257997087, auc for val: 0.6429591796743499\n",
      "--------------------------------------------------------------\n",
      "Epoch 16: iteration 300, the loss is [ 0.14520928]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413672368219127, auc for val: 0.6429695027929387\n",
      "--------------------------------------------------------------\n",
      "Epoch 16: iteration 400, the loss is [ 0.17300656]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413650165017746, auc for val: 0.6429304004994755\n",
      "--------------------------------------------------------------\n",
      "Epoch 16: iteration 500, the loss is [ 0.17658331]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413515097618987, auc for val: 0.6429127517984445\n",
      "--------------------------------------------------------------\n",
      "Epoch 16: iteration 600, the loss is [ 0.14098337]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413736028514768, auc for val: 0.6429255804047941\n",
      "--------------------------------------------------------------\n",
      "Epoch 16: iteration 700, the loss is [ 0.14736047]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413589044125354, auc for val: 0.6428748994209973\n",
      "--------------------------------------------------------------\n",
      "Epoch 16: iteration 800, the loss is [ 0.15669805]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413635451026516, auc for val: 0.64290719787298\n",
      "--------------------------------------------------------------\n",
      "Epoch 16: iteration 900, the loss is [ 0.14510868]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413797439957651, auc for val: 0.6429284662403014\n",
      "--------------------------------------------------------------\n",
      "Epoch 16: iteration 1000, the loss is [ 0.12547171]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413769283485223, auc for val: 0.6429154778596197\n",
      "--------------------------------------------------------------\n",
      "Epoch 16: iteration 1100, the loss is [ 0.15598007]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.641379189096571, auc for val: 0.6429797099690917\n",
      "--------------------------------------------------------------\n",
      "Epoch 17: iteration 0, the loss is [ 0.17360206]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413665525614423, auc for val: 0.6429424754799917\n",
      "--------------------------------------------------------------\n",
      "Epoch 17: iteration 100, the loss is [ 0.19009309]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413772119868849, auc for val: 0.6429865081548437\n",
      "--------------------------------------------------------------\n",
      "Epoch 17: iteration 200, the loss is [ 0.15701148]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413864139982076, auc for val: 0.6430074583874326\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: iteration 300, the loss is [ 0.16728979]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413570314670084, auc for val: 0.64297369651568\n",
      "--------------------------------------------------------------\n",
      "Epoch 17: iteration 400, the loss is [ 0.15872625]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413737079801192, auc for val: 0.6430296641917661\n",
      "--------------------------------------------------------------\n",
      "Epoch 17: iteration 500, the loss is [ 0.14915104]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413733823063743, auc for val: 0.6430367706147255\n",
      "--------------------------------------------------------------\n",
      "Epoch 17: iteration 600, the loss is [ 0.14776921]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413693034275799, auc for val: 0.6430210901072402\n",
      "--------------------------------------------------------------\n",
      "Epoch 17: iteration 700, the loss is [ 0.1493316]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413630745153427, auc for val: 0.6430390781519859\n",
      "--------------------------------------------------------------\n",
      "Epoch 17: iteration 800, the loss is [ 0.1320221]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413638986660285, auc for val: 0.6430786512848533\n",
      "--------------------------------------------------------------\n",
      "Epoch 17: iteration 900, the loss is [ 0.14332069]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6414008409351969, auc for val: 0.643130631672291\n",
      "--------------------------------------------------------------\n",
      "Epoch 17: iteration 1000, the loss is [ 0.22482663]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413732363479535, auc for val: 0.6430534748090837\n",
      "--------------------------------------------------------------\n",
      "Epoch 17: iteration 1100, the loss is [ 0.19768728]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413575939213189, auc for val: 0.643045781604284\n",
      "--------------------------------------------------------------\n",
      "Epoch 18: iteration 0, the loss is [ 0.17069238]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413725592809185, auc for val: 0.6431029355694395\n",
      "--------------------------------------------------------------\n",
      "Epoch 18: iteration 100, the loss is [ 0.15069374]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413809683666909, auc for val: 0.6430786272480068\n",
      "--------------------------------------------------------------\n",
      "Epoch 18: iteration 200, the loss is [ 0.20449838]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413778204952567, auc for val: 0.6430271488064806\n",
      "--------------------------------------------------------------\n",
      "Epoch 18: iteration 300, the loss is [ 0.15366112]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6411228615959309, auc for val: 0.642832642644919\n",
      "--------------------------------------------------------------\n",
      "Epoch 18: iteration 400, the loss is [ 0.11862424]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413633633378012, auc for val: 0.6431476257227389\n",
      "--------------------------------------------------------------\n",
      "Epoch 18: iteration 500, the loss is [ 0.1417243]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.64134731964607, auc for val: 0.6430641542385731\n",
      "--------------------------------------------------------------\n",
      "Epoch 18: iteration 600, the loss is [ 0.15898305]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413662354474838, auc for val: 0.6431758336690268\n",
      "--------------------------------------------------------------\n",
      "Epoch 18: iteration 700, the loss is [ 0.16885316]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413641243550403, auc for val: 0.6429805144964821\n",
      "--------------------------------------------------------------\n",
      "Epoch 18: iteration 800, the loss is [ 0.18681607]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413722441762993, auc for val: 0.6431785229679661\n",
      "--------------------------------------------------------------\n",
      "Epoch 18: iteration 900, the loss is [ 0.16019332]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413666488489906, auc for val: 0.6431852546989072\n",
      "--------------------------------------------------------------\n",
      "Epoch 18: iteration 1000, the loss is [ 0.20076753]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413566265047272, auc for val: 0.6431462231020513\n",
      "--------------------------------------------------------------\n",
      "Epoch 18: iteration 1100, the loss is [ 0.16140573]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413271919316353, auc for val: 0.6431865145124482\n",
      "--------------------------------------------------------------\n",
      "Epoch 19: iteration 0, the loss is [ 0.16413875]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6412721577706184, auc for val: 0.6429039189643363\n",
      "--------------------------------------------------------------\n",
      "Epoch 19: iteration 100, the loss is [ 0.15084115]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6412824799485206, auc for val: 0.6431967386557871\n",
      "--------------------------------------------------------------\n",
      "Epoch 19: iteration 200, the loss is [ 0.14395642]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413265622046386, auc for val: 0.6430116436265811\n",
      "--------------------------------------------------------------\n",
      "Epoch 19: iteration 300, the loss is [ 0.16838005]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413172664375354, auc for val: 0.6432667848542382\n",
      "--------------------------------------------------------------\n",
      "Epoch 19: iteration 400, the loss is [ 0.19642937]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6412828282072414, auc for val: 0.6431023897916317\n",
      "--------------------------------------------------------------\n",
      "Epoch 19: iteration 500, the loss is [ 0.19096009]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413458934570858, auc for val: 0.6428928549453036\n",
      "--------------------------------------------------------------\n",
      "Epoch 19: iteration 600, the loss is [ 0.15742241]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413118259499279, auc for val: 0.6431452291077535\n",
      "--------------------------------------------------------------\n",
      "Epoch 19: iteration 700, the loss is [ 0.14903025]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413616578908143, auc for val: 0.6431190303590433\n",
      "--------------------------------------------------------------\n",
      "Epoch 19: iteration 800, the loss is [ 0.15776484]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.641379364270788, auc for val: 0.6430992027885774\n",
      "--------------------------------------------------------------\n",
      "Epoch 19: iteration 900, the loss is [ 0.2190809]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413539925419613, auc for val: 0.643156652265551\n",
      "--------------------------------------------------------------\n",
      "Epoch 19: iteration 1000, the loss is [ 0.17162521]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413753343796904, auc for val: 0.643101319444998\n",
      "--------------------------------------------------------------\n",
      "Epoch 19: iteration 1100, the loss is [ 0.16336721]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413389447190689, auc for val: 0.6431070854602843\n",
      "--------------------------------------------------------------\n",
      "Epoch 20: iteration 0, the loss is [ 0.16701989]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.641391234765329, auc for val: 0.6431016658583735\n",
      "--------------------------------------------------------------\n",
      "Epoch 20: iteration 100, the loss is [ 0.16118996]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413597560509854, auc for val: 0.6432290342799054\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: iteration 200, the loss is [ 0.16090731]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413325211018349, auc for val: 0.6430941720180063\n",
      "--------------------------------------------------------------\n",
      "Epoch 20: iteration 300, the loss is [ 0.12473518]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413668506268625, auc for val: 0.6431271802639256\n",
      "--------------------------------------------------------------\n",
      "Epoch 20: iteration 400, the loss is [ 0.15205887]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413657236687152, auc for val: 0.6431702769156978\n",
      "--------------------------------------------------------------\n",
      "Epoch 20: iteration 500, the loss is [ 0.17975491]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413571043256583, auc for val: 0.6431576688827632\n",
      "--------------------------------------------------------------\n",
      "Epoch 20: iteration 600, the loss is [ 0.13237743]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413308210398778, auc for val: 0.6431398660631285\n",
      "--------------------------------------------------------------\n",
      "Epoch 20: iteration 700, the loss is [ 0.15444212]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413340277045858, auc for val: 0.6431597558466087\n",
      "--------------------------------------------------------------\n",
      "Epoch 20: iteration 800, the loss is [ 0.16240731]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413566962288075, auc for val: 0.6431985965626252\n",
      "--------------------------------------------------------------\n",
      "Epoch 20: iteration 900, the loss is [ 0.16123576]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413585388333083, auc for val: 0.643082457590187\n",
      "--------------------------------------------------------------\n",
      "Epoch 20: iteration 1000, the loss is [ 0.18733907]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.641362217853544, auc for val: 0.6431670050767149\n",
      "--------------------------------------------------------------\n",
      "Epoch 20: iteration 1100, the loss is [ 0.14730786]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413536774373423, auc for val: 0.6431555550542066\n",
      "--------------------------------------------------------------\n",
      "Epoch 21: iteration 0, the loss is [ 0.18379726]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413712837120226, auc for val: 0.6431734653326843\n",
      "--------------------------------------------------------------\n",
      "Epoch 21: iteration 100, the loss is [ 0.18433249]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413619811935386, auc for val: 0.6431060080439901\n",
      "--------------------------------------------------------------\n",
      "Epoch 21: iteration 200, the loss is [ 0.17622578]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413563123244045, auc for val: 0.6430032307303197\n",
      "--------------------------------------------------------------\n",
      "Epoch 21: iteration 300, the loss is [ 0.16454138]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413635642315636, auc for val: 0.6432161731531167\n",
      "--------------------------------------------------------------\n",
      "Epoch 21: iteration 400, the loss is [ 0.16494255]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413612899807533, auc for val: 0.6429927365259414\n",
      "--------------------------------------------------------------\n",
      "Epoch 21: iteration 500, the loss is [ 0.17185135]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413372254478263, auc for val: 0.6431277345253263\n",
      "--------------------------------------------------------------\n",
      "Epoch 21: iteration 600, the loss is [ 0.12862952]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413869004592986, auc for val: 0.6431686622051886\n",
      "--------------------------------------------------------------\n",
      "Epoch 21: iteration 700, the loss is [ 0.14150171]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.641362660711968, auc for val: 0.6432644645915887\n",
      "--------------------------------------------------------------\n",
      "Epoch 21: iteration 800, the loss is [ 0.14515482]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413670761551271, auc for val: 0.6431632185664312\n",
      "--------------------------------------------------------------\n",
      "Epoch 21: iteration 900, the loss is [ 0.1254335]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413576865920563, auc for val: 0.6431304365496551\n",
      "--------------------------------------------------------------\n",
      "Epoch 21: iteration 1000, the loss is [ 0.14746998]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413466899592601, auc for val: 0.6429575225458762\n",
      "--------------------------------------------------------------\n",
      "Epoch 21: iteration 1100, the loss is [ 0.12616731]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413783256030181, auc for val: 0.6431457423651221\n",
      "--------------------------------------------------------------\n",
      "Epoch 22: iteration 0, the loss is [ 0.16416767]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413469036324217, auc for val: 0.6431182625938887\n",
      "--------------------------------------------------------------\n",
      "Epoch 22: iteration 100, the loss is [ 0.15125848]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413764406416409, auc for val: 0.643041141078985\n",
      "--------------------------------------------------------------\n",
      "Epoch 22: iteration 200, the loss is [ 0.14355543]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413600244987415, auc for val: 0.6429971013344722\n",
      "--------------------------------------------------------------\n",
      "Epoch 22: iteration 300, the loss is [ 0.14805134]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413822821934109, auc for val: 0.6431135527859144\n",
      "--------------------------------------------------------------\n",
      "Epoch 22: iteration 400, the loss is [ 0.15540795]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413487795115843, auc for val: 0.6430881953268305\n",
      "--------------------------------------------------------------\n",
      "Epoch 22: iteration 500, the loss is [ 0.1469271]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.641376669706343, auc for val: 0.6431161925972286\n",
      "--------------------------------------------------------------\n",
      "Epoch 22: iteration 600, the loss is [ 0.17429791]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.64134423844452, auc for val: 0.6430967538581027\n",
      "--------------------------------------------------------------\n",
      "Epoch 22: iteration 700, the loss is [ 0.16786957]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.64135657739647, auc for val: 0.6430911561007415\n",
      "--------------------------------------------------------------\n",
      "Epoch 22: iteration 800, the loss is [ 0.12683314]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413580930412486, auc for val: 0.6431474376697637\n",
      "--------------------------------------------------------------\n",
      "Epoch 22: iteration 900, the loss is [ 0.17852187]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413658423403057, auc for val: 0.6431353400663331\n",
      "--------------------------------------------------------------\n",
      "Epoch 22: iteration 1000, the loss is [ 0.12614349]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413806530209512, auc for val: 0.6430383216982885\n",
      "--------------------------------------------------------------\n",
      "Epoch 22: iteration 1100, the loss is [ 0.17380819]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413774449497057, auc for val: 0.6430211805988975\n",
      "--------------------------------------------------------------\n",
      "Epoch 23: iteration 0, the loss is [ 0.17931208]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413811482427623, auc for val: 0.6430049288628257\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: iteration 100, the loss is [ 0.1515031]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.641388879779258, auc for val: 0.643050508379444\n",
      "--------------------------------------------------------------\n",
      "Epoch 23: iteration 200, the loss is [ 0.13341773]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413893762468595, auc for val: 0.6430788280263714\n",
      "--------------------------------------------------------------\n",
      "Epoch 23: iteration 300, the loss is [ 0.15375741]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413701379866443, auc for val: 0.6430500714744113\n",
      "--------------------------------------------------------------\n",
      "Epoch 23: iteration 400, the loss is [ 0.22994156]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413833569087315, auc for val: 0.6430591828531521\n",
      "--------------------------------------------------------------\n",
      "Epoch 23: iteration 500, the loss is [ 0.16674183]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413794278462894, auc for val: 0.6429938280815571\n",
      "--------------------------------------------------------------\n",
      "Epoch 23: iteration 600, the loss is [ 0.16090946]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413755418640863, auc for val: 0.6430689319152902\n",
      "--------------------------------------------------------------\n",
      "Epoch 23: iteration 700, the loss is [ 0.16978879]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.641386716805669, auc for val: 0.6430604101462537\n",
      "--------------------------------------------------------------\n",
      "Epoch 23: iteration 800, the loss is [ 0.15745696]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413832228857873, auc for val: 0.6430772288691157\n",
      "--------------------------------------------------------------\n",
      "Epoch 23: iteration 900, the loss is [ 0.14670172]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413784040476317, auc for val: 0.6430626017410781\n",
      "--------------------------------------------------------------\n",
      "Epoch 23: iteration 1000, the loss is [ 0.13489576]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413829087456511, auc for val: 0.6429856456562354\n",
      "--------------------------------------------------------------\n",
      "Epoch 23: iteration 1100, the loss is [ 0.11502669]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413860464096409, auc for val: 0.6430074824242791\n",
      "--------------------------------------------------------------\n",
      "Epoch 24: iteration 0, the loss is [ 0.17718206]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413809497200204, auc for val: 0.6430896983367003\n",
      "--------------------------------------------------------------\n",
      "Epoch 24: iteration 100, the loss is [ 0.17366447]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413848985945498, auc for val: 0.6430352053918413\n",
      "--------------------------------------------------------------\n",
      "Epoch 24: iteration 200, the loss is [ 0.15560476]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413692083858219, auc for val: 0.6430326023427628\n",
      "--------------------------------------------------------------\n",
      "Epoch 24: iteration 300, the loss is [ 0.14310691]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.641376588408467, auc for val: 0.6429805922627501\n",
      "--------------------------------------------------------------\n",
      "Epoch 24: iteration 400, the loss is [ 0.15776749]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413659630614222, auc for val: 0.6429493669852653\n",
      "--------------------------------------------------------------\n",
      "Epoch 24: iteration 500, the loss is [ 0.16181409]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413650977192769, auc for val: 0.6431045941118453\n",
      "--------------------------------------------------------------\n",
      "Epoch 24: iteration 600, the loss is [ 0.16990402]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413580516488551, auc for val: 0.6431206832457206\n",
      "--------------------------------------------------------------\n",
      "Epoch 24: iteration 700, the loss is [ 0.16386235]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413594311809765, auc for val: 0.6430591771974234\n",
      "--------------------------------------------------------------\n",
      "Epoch 24: iteration 800, the loss is [ 0.10961026]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413270032144702, auc for val: 0.6430890465139816\n",
      "--------------------------------------------------------------\n",
      "Epoch 24: iteration 900, the loss is [ 0.18043119]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413413606283134, auc for val: 0.6431291230066926\n",
      "--------------------------------------------------------------\n",
      "Epoch 24: iteration 1000, the loss is [ 0.14646031]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413830717432707, auc for val: 0.6430885587073916\n",
      "--------------------------------------------------------------\n",
      "Epoch 24: iteration 1100, the loss is [ 0.15046786]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413372944887312, auc for val: 0.6429802628165603\n",
      "--------------------------------------------------------------\n",
      "Epoch 25: iteration 0, the loss is [ 0.14983207]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413626138541708, auc for val: 0.643128727105692\n",
      "--------------------------------------------------------------\n",
      "Epoch 25: iteration 100, the loss is [ 0.13560151]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413787136064739, auc for val: 0.6431062752771654\n",
      "--------------------------------------------------------------\n",
      "Epoch 25: iteration 200, the loss is [ 0.17087375]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.641355499144712, auc for val: 0.6431473231412599\n",
      "--------------------------------------------------------------\n",
      "Epoch 25: iteration 300, the loss is [ 0.14235206]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413400722800181, auc for val: 0.6430893490954606\n",
      "--------------------------------------------------------------\n",
      "Epoch 25: iteration 400, the loss is [ 0.17511898]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413785048762874, auc for val: 0.6430736615183145\n",
      "--------------------------------------------------------------\n",
      "Epoch 25: iteration 500, the loss is [ 0.14109388]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413767084062215, auc for val: 0.6431243099816718\n",
      "--------------------------------------------------------------\n",
      "Epoch 25: iteration 600, the loss is [ 0.19042942]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413827354200267, auc for val: 0.6431264690560569\n",
      "--------------------------------------------------------------\n",
      "Epoch 25: iteration 700, the loss is [ 0.15868202]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413805693118679, auc for val: 0.6431134792614429\n",
      "--------------------------------------------------------------\n",
      "Epoch 25: iteration 800, the loss is [ 0.170966]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413721466831475, auc for val: 0.6431425765710499\n",
      "--------------------------------------------------------------\n",
      "Epoch 25: iteration 900, the loss is [ 0.18822262]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.641385946183787, auc for val: 0.643122749000584\n",
      "--------------------------------------------------------------\n",
      "Epoch 25: iteration 1000, the loss is [ 0.17480424]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413760923427334, auc for val: 0.6430990557396342\n",
      "--------------------------------------------------------------\n",
      "Epoch 25: iteration 1100, the loss is [ 0.16056862]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413805828950029, auc for val: 0.6430118528885386\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: iteration 0, the loss is [ 0.17229192]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.641376170988281, auc for val: 0.6431100575456526\n",
      "--------------------------------------------------------------\n",
      "Epoch 26: iteration 100, the loss is [ 0.20694239]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413861193888512, auc for val: 0.6430990769486165\n",
      "--------------------------------------------------------------\n",
      "Epoch 26: iteration 200, the loss is [ 0.125379]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413963914940135, auc for val: 0.6430917245014637\n",
      "--------------------------------------------------------------\n",
      "Epoch 26: iteration 300, the loss is [ 0.13139428]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413878593161039, auc for val: 0.6430441372011997\n",
      "--------------------------------------------------------------\n",
      "Epoch 26: iteration 400, the loss is [ 0.17662229]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413808926547789, auc for val: 0.6430776940527914\n",
      "--------------------------------------------------------------\n",
      "Epoch 26: iteration 500, the loss is [ 0.19032836]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413784857071887, auc for val: 0.6430750881758485\n",
      "--------------------------------------------------------------\n",
      "Epoch 26: iteration 600, the loss is [ 0.16207239]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413886109698209, auc for val: 0.6430558219364441\n",
      "--------------------------------------------------------------\n",
      "Epoch 26: iteration 700, the loss is [ 0.12124337]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413749726583953, auc for val: 0.6430348999824981\n",
      "--------------------------------------------------------------\n",
      "Epoch 26: iteration 800, the loss is [ 0.15627694]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413936792071941, auc for val: 0.6430625522534531\n",
      "--------------------------------------------------------------\n",
      "Epoch 26: iteration 900, the loss is [ 0.15027145]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413773812134569, auc for val: 0.6430753257164488\n",
      "--------------------------------------------------------------\n",
      "Epoch 26: iteration 1000, the loss is [ 0.19641651]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413863130891784, auc for val: 0.643029604806616\n",
      "--------------------------------------------------------------\n",
      "Epoch 26: iteration 1100, the loss is [ 0.14783192]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413762474637422, auc for val: 0.6430552337406719\n",
      "--------------------------------------------------------------\n",
      "Epoch 27: iteration 0, the loss is [ 0.13498823]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413847495015595, auc for val: 0.643015588497265\n",
      "--------------------------------------------------------------\n",
      "Epoch 27: iteration 100, the loss is [ 0.16800654]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413893752020031, auc for val: 0.6430531750554691\n",
      "--------------------------------------------------------------\n",
      "Epoch 27: iteration 200, the loss is [ 0.1599797]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413928218622199, auc for val: 0.6430665720625406\n",
      "--------------------------------------------------------------\n",
      "Epoch 27: iteration 300, the loss is [ 0.19239244]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413813333029295, auc for val: 0.6430500417818363\n",
      "--------------------------------------------------------------\n",
      "Epoch 27: iteration 400, the loss is [ 0.13185149]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413828843924566, auc for val: 0.6430329431004097\n",
      "--------------------------------------------------------------\n",
      "Epoch 27: iteration 500, the loss is [ 0.13994285]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.641382285247607, auc for val: 0.642982407751624\n",
      "--------------------------------------------------------------\n",
      "Epoch 27: iteration 600, the loss is [ 0.16730605]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413906535437877, auc for val: 0.6430355433216239\n",
      "--------------------------------------------------------------\n",
      "Epoch 27: iteration 700, the loss is [ 0.15479898]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413858715167315, auc for val: 0.6430251565760885\n",
      "--------------------------------------------------------------\n",
      "Epoch 27: iteration 800, the loss is [ 0.1946076]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413759133105847, auc for val: 0.6430326928344201\n",
      "--------------------------------------------------------------\n",
      "Epoch 27: iteration 900, the loss is [ 0.16171306]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413871510239331, auc for val: 0.6429981023984308\n",
      "--------------------------------------------------------------\n",
      "Epoch 27: iteration 1000, the loss is [ 0.17637053]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413834238599233, auc for val: 0.6429546890258582\n",
      "--------------------------------------------------------------\n",
      "Epoch 27: iteration 1100, the loss is [ 0.15151548]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413946193771423, auc for val: 0.6430217489996197\n",
      "--------------------------------------------------------------\n",
      "Epoch 28: iteration 0, the loss is [ 0.11228851]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413883811817566, auc for val: 0.6430267302825656\n",
      "--------------------------------------------------------------\n",
      "Epoch 28: iteration 100, the loss is [ 0.13918744]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413912595203914, auc for val: 0.6430159660171476\n",
      "--------------------------------------------------------------\n",
      "Epoch 28: iteration 200, the loss is [ 0.18072566]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413729816842665, auc for val: 0.6430224375845741\n",
      "--------------------------------------------------------------\n",
      "Epoch 28: iteration 300, the loss is [ 0.15656234]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.641388434429253, auc for val: 0.6429996449984008\n",
      "--------------------------------------------------------------\n",
      "Epoch 28: iteration 400, the loss is [ 0.15781166]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413815952404255, auc for val: 0.6429688834906594\n",
      "--------------------------------------------------------------\n",
      "Epoch 28: iteration 500, the loss is [ 0.18999913]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413859457015457, auc for val: 0.6430060190045092\n",
      "--------------------------------------------------------------\n",
      "Epoch 28: iteration 600, the loss is [ 0.17093666]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413737035193855, auc for val: 0.643011653524106\n",
      "--------------------------------------------------------------\n",
      "Epoch 28: iteration 700, the loss is [ 0.14193211]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.641381043475801, auc for val: 0.6430363902669787\n",
      "--------------------------------------------------------------\n",
      "Epoch 28: iteration 800, the loss is [ 0.11541405]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413772402381982, auc for val: 0.643029351712762\n",
      "--------------------------------------------------------------\n",
      "Epoch 28: iteration 900, the loss is [ 0.1706198]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413818866348386, auc for val: 0.643015178456943\n",
      "--------------------------------------------------------------\n",
      "Epoch 28: iteration 1000, the loss is [ 0.14454988]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413861851344393, auc for val: 0.6430154598294398\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: iteration 1100, the loss is [ 0.15401116]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413693516517277, auc for val: 0.6429226846717616\n",
      "--------------------------------------------------------------\n",
      "Epoch 29: iteration 0, the loss is [ 0.13656069]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413722006138193, auc for val: 0.6429973586701226\n",
      "--------------------------------------------------------------\n",
      "Epoch 29: iteration 100, the loss is [ 0.18440996]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413798333309989, auc for val: 0.6430208879149435\n",
      "--------------------------------------------------------------\n",
      "Epoch 29: iteration 200, the loss is [ 0.12946518]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413760391354237, auc for val: 0.6429961554138675\n",
      "--------------------------------------------------------------\n",
      "Epoch 29: iteration 300, the loss is [ 0.13413565]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413803192294747, auc for val: 0.6429995530928113\n",
      "--------------------------------------------------------------\n",
      "Epoch 29: iteration 400, the loss is [ 0.15315343]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413623448839864, auc for val: 0.6430302636989953\n",
      "--------------------------------------------------------------\n",
      "Epoch 29: iteration 500, the loss is [ 0.19616708]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.641371966445395, auc for val: 0.6429638060603275\n",
      "--------------------------------------------------------------\n",
      "Epoch 29: iteration 600, the loss is [ 0.15751655]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413719556351484, auc for val: 0.6430038783112421\n",
      "--------------------------------------------------------------\n",
      "Epoch 29: iteration 700, the loss is [ 0.16484055]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413747921393352, auc for val: 0.6429939369543323\n",
      "--------------------------------------------------------------\n",
      "Epoch 29: iteration 800, the loss is [ 0.18768616]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413846456588945, auc for val: 0.6430152222888394\n",
      "--------------------------------------------------------------\n",
      "Epoch 29: iteration 900, the loss is [ 0.15248054]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413832467567405, auc for val: 0.6430447692288686\n",
      "--------------------------------------------------------------\n",
      "Epoch 29: iteration 1000, the loss is [ 0.16061991]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413835557931543, auc for val: 0.6430471531184645\n",
      "--------------------------------------------------------------\n",
      "Epoch 29: iteration 1100, the loss is [ 0.16375941]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413729619525527, auc for val: 0.6430258494028395\n",
      "--------------------------------------------------------------\n",
      "Epoch 30: iteration 0, the loss is [ 0.13838032]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413826963986538, auc for val: 0.6430104842022224\n",
      "--------------------------------------------------------------\n",
      "Epoch 30: iteration 100, the loss is [ 0.13870953]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413744755478048, auc for val: 0.6430365217626681\n",
      "--------------------------------------------------------------\n",
      "Epoch 30: iteration 200, the loss is [ 0.182486]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.641380925889254, auc for val: 0.6430283775135144\n",
      "--------------------------------------------------------------\n",
      "Epoch 30: iteration 300, the loss is [ 0.15358572]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413773115697501, auc for val: 0.6430226284654137\n",
      "--------------------------------------------------------------\n",
      "Epoch 30: iteration 400, the loss is [ 0.15382336]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413866139676737, auc for val: 0.6430470937333145\n",
      "--------------------------------------------------------------\n",
      "Epoch 30: iteration 500, the loss is [ 0.15606897]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413826652538916, auc for val: 0.6430385620667529\n",
      "--------------------------------------------------------------\n",
      "Epoch 30: iteration 600, the loss is [ 0.13944274]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413837226487057, auc for val: 0.6430446235938576\n",
      "--------------------------------------------------------------\n",
      "Epoch 30: iteration 700, the loss is [ 0.17033584]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413741409525927, auc for val: 0.6430338409473216\n",
      "--------------------------------------------------------------\n",
      "Epoch 30: iteration 800, the loss is [ 0.12017922]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413793070447992, auc for val: 0.6430255567188854\n",
      "--------------------------------------------------------------\n",
      "Epoch 30: iteration 900, the loss is [ 0.19262424]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413730833568447, auc for val: 0.6430250773958885\n",
      "--------------------------------------------------------------\n",
      "Epoch 30: iteration 1000, the loss is [ 0.17040624]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413764936078299, auc for val: 0.6430067853557317\n",
      "--------------------------------------------------------------\n",
      "Epoch 30: iteration 1100, the loss is [ 0.17732066]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413793003336054, auc for val: 0.6430263881109866\n",
      "--------------------------------------------------------------\n",
      "Epoch 31: iteration 0, the loss is [ 0.16228662]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413760546073378, auc for val: 0.6429753013286644\n",
      "--------------------------------------------------------------\n",
      "Epoch 31: iteration 100, the loss is [ 0.15150918]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.641381364166384, auc for val: 0.6430265408156582\n",
      "--------------------------------------------------------------\n",
      "Epoch 31: iteration 200, the loss is [ 0.17400029]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413835001746373, auc for val: 0.6430203364814071\n",
      "--------------------------------------------------------------\n",
      "Epoch 31: iteration 300, the loss is [ 0.15345027]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413765562188485, auc for val: 0.6429870765555659\n",
      "--------------------------------------------------------------\n",
      "Epoch 31: iteration 400, the loss is [ 0.16748609]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413766634773905, auc for val: 0.6430272491956628\n",
      "--------------------------------------------------------------\n",
      "Epoch 31: iteration 500, the loss is [ 0.15415975]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413762411946028, auc for val: 0.6430227995512032\n",
      "--------------------------------------------------------------\n",
      "Epoch 31: iteration 600, the loss is [ 0.14126894]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413758734452892, auc for val: 0.6430090363357062\n",
      "--------------------------------------------------------------\n",
      "Epoch 31: iteration 700, the loss is [ 0.17280427]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413745143280573, auc for val: 0.642999814670258\n",
      "--------------------------------------------------------------\n",
      "Epoch 31: iteration 800, the loss is [ 0.14273348]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.641378221197738, auc for val: 0.6429912207906823\n",
      "--------------------------------------------------------------\n",
      "Epoch 31: iteration 900, the loss is [ 0.19146694]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413793045130314, auc for val: 0.6430182862797972\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31: iteration 1000, the loss is [ 0.13189209]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.641376368144672, auc for val: 0.6430075375676327\n",
      "--------------------------------------------------------------\n",
      "Epoch 31: iteration 1100, the loss is [ 0.15926169]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413768923009718, auc for val: 0.6430015382535426\n",
      "--------------------------------------------------------------\n",
      "Epoch 32: iteration 0, the loss is [ 0.15928867]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.641378079700051, auc for val: 0.6430037977171098\n",
      "--------------------------------------------------------------\n",
      "Epoch 32: iteration 100, the loss is [ 0.14911841]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413744900552358, auc for val: 0.6430056188617123\n",
      "--------------------------------------------------------------\n",
      "Epoch 32: iteration 200, the loss is [ 0.12691364]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413723824590433, auc for val: 0.642981921358966\n",
      "--------------------------------------------------------------\n",
      "Epoch 32: iteration 300, the loss is [ 0.18894663]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413760679091652, auc for val: 0.6430008751193665\n",
      "--------------------------------------------------------------\n",
      "Epoch 32: iteration 400, the loss is [ 0.16920736]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413771774262458, auc for val: 0.6430071049043965\n",
      "--------------------------------------------------------------\n",
      "Epoch 32: iteration 500, the loss is [ 0.12872806]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413786467758422, auc for val: 0.6430153665099182\n",
      "--------------------------------------------------------------\n",
      "Epoch 32: iteration 600, the loss is [ 0.1265212]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413741891767406, auc for val: 0.6430101604117613\n",
      "--------------------------------------------------------------\n",
      "Epoch 32: iteration 700, the loss is [ 0.17025822]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.641369207582086, auc for val: 0.6429882882954141\n",
      "--------------------------------------------------------------\n",
      "Epoch 32: iteration 800, the loss is [ 0.16634132]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413695330147102, auc for val: 0.6430019779864395\n",
      "--------------------------------------------------------------\n",
      "Epoch 32: iteration 900, the loss is [ 0.15804979]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413722952537092, auc for val: 0.6430118585442671\n",
      "--------------------------------------------------------------\n",
      "Epoch 32: iteration 1000, the loss is [ 0.17858259]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.641377224766284, auc for val: 0.6430177447437857\n",
      "--------------------------------------------------------------\n",
      "Epoch 32: iteration 1100, the loss is [ 0.11639128]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413706864961387, auc for val: 0.6429857941191106\n",
      "--------------------------------------------------------------\n",
      "Epoch 33: iteration 0, the loss is [ 0.12093195]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413678925497623, auc for val: 0.6430014548315459\n",
      "--------------------------------------------------------------\n",
      "Epoch 33: iteration 100, the loss is [ 0.19685341]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413778498717999, auc for val: 0.6430259172715824\n",
      "--------------------------------------------------------------\n",
      "Epoch 33: iteration 200, the loss is [ 0.19007415]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413771298852735, auc for val: 0.6430162290085266\n",
      "--------------------------------------------------------------\n",
      "Epoch 33: iteration 300, the loss is [ 0.12679563]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413739071056627, auc for val: 0.6429992448556037\n",
      "--------------------------------------------------------------\n",
      "Epoch 33: iteration 400, the loss is [ 0.15345722]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413742480503877, auc for val: 0.6429997764940902\n",
      "--------------------------------------------------------------\n",
      "Epoch 33: iteration 500, the loss is [ 0.18928589]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413724262224574, auc for val: 0.6430061179797593\n",
      "--------------------------------------------------------------\n",
      "Epoch 33: iteration 600, the loss is [ 0.18099985]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413752668256967, auc for val: 0.6430182693126114\n",
      "--------------------------------------------------------------\n",
      "Epoch 33: iteration 700, the loss is [ 0.18496613]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413759133507715, auc for val: 0.6430297051957983\n",
      "--------------------------------------------------------------\n",
      "Epoch 33: iteration 800, the loss is [ 0.15108161]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413707915042205, auc for val: 0.6430037086393847\n",
      "--------------------------------------------------------------\n",
      "Epoch 33: iteration 900, the loss is [ 0.12050349]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413783457767863, auc for val: 0.6430180077351646\n",
      "--------------------------------------------------------------\n",
      "Epoch 33: iteration 1000, the loss is [ 0.18318014]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413780982663478, auc for val: 0.6430300388837844\n",
      "--------------------------------------------------------------\n",
      "Epoch 33: iteration 1100, the loss is [ 0.15640667]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413701254483659, auc for val: 0.6430206800669183\n",
      "--------------------------------------------------------------\n",
      "Epoch 34: iteration 0, the loss is [ 0.14297782]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.641372984256221, auc for val: 0.6430288681479686\n",
      "--------------------------------------------------------------\n",
      "Epoch 34: iteration 100, the loss is [ 0.20326312]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.641377216206498, auc for val: 0.6430435348661061\n",
      "--------------------------------------------------------------\n",
      "Epoch 34: iteration 200, the loss is [ 0.13366228]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413720362900356, auc for val: 0.6430149310188178\n",
      "--------------------------------------------------------------\n",
      "Epoch 34: iteration 300, the loss is [ 0.14426757]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413735194638881, auc for val: 0.6430026213255655\n",
      "--------------------------------------------------------------\n",
      "Epoch 34: iteration 400, the loss is [ 0.1527869]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.641365720252838, auc for val: 0.6430315828976865\n",
      "--------------------------------------------------------------\n",
      "Epoch 34: iteration 500, the loss is [ 0.11967214]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413786902981357, auc for val: 0.6430385733782101\n",
      "--------------------------------------------------------------\n",
      "Epoch 34: iteration 600, the loss is [ 0.20799088]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413740145651388, auc for val: 0.6429996068222328\n",
      "--------------------------------------------------------------\n",
      "Epoch 34: iteration 700, the loss is [ 0.16484]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413844237072542, auc for val: 0.6430247083595987\n",
      "--------------------------------------------------------------\n",
      "Epoch 34: iteration 800, the loss is [ 0.20607221]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413767119024721, auc for val: 0.6430171325111671\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34: iteration 900, the loss is [ 0.18678483]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.641378786625871, auc for val: 0.6430053997022298\n",
      "--------------------------------------------------------------\n",
      "Epoch 34: iteration 1000, the loss is [ 0.16108371]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413741229087242, auc for val: 0.6430255652024783\n",
      "--------------------------------------------------------------\n",
      "Epoch 34: iteration 1100, the loss is [ 0.16019197]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413773526808362, auc for val: 0.6430446221799254\n",
      "--------------------------------------------------------------\n",
      "Epoch 35: iteration 0, the loss is [ 0.1538405]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413741887346859, auc for val: 0.6430252739324566\n",
      "--------------------------------------------------------------\n",
      "Epoch 35: iteration 100, the loss is [ 0.20819062]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413777557543382, auc for val: 0.6430289261191864\n",
      "--------------------------------------------------------------\n",
      "Epoch 35: iteration 200, the loss is [ 0.13028891]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413761792265731, auc for val: 0.6430277044818133\n",
      "--------------------------------------------------------------\n",
      "Epoch 35: iteration 300, the loss is [ 0.15030925]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413741385815722, auc for val: 0.6430104276449367\n",
      "--------------------------------------------------------------\n",
      "Epoch 35: iteration 400, the loss is [ 0.1224248]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413782724358951, auc for val: 0.6430249642813168\n",
      "--------------------------------------------------------------\n",
      "Epoch 35: iteration 500, the loss is [ 0.17712626]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413747203657284, auc for val: 0.6430237355742832\n",
      "--------------------------------------------------------------\n",
      "Epoch 35: iteration 600, the loss is [ 0.17593019]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413737974761001, auc for val: 0.6430025223503153\n",
      "--------------------------------------------------------------\n",
      "Epoch 35: iteration 700, the loss is [ 0.16129796]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413751565933322, auc for val: 0.6430193071388057\n",
      "--------------------------------------------------------------\n",
      "Epoch 35: iteration 800, the loss is [ 0.19176443]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413787840941032, auc for val: 0.6430345351880047\n",
      "--------------------------------------------------------------\n",
      "Epoch 35: iteration 900, the loss is [ 0.16375864]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.641373711998798, auc for val: 0.6430036153198632\n",
      "--------------------------------------------------------------\n",
      "Epoch 35: iteration 1000, the loss is [ 0.1358702]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413741532497506, auc for val: 0.6430243647740875\n",
      "--------------------------------------------------------------\n",
      "Epoch 35: iteration 1100, the loss is [ 0.18556896]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413783327562664, auc for val: 0.6430326207238808\n",
      "--------------------------------------------------------------\n",
      "Epoch 36: iteration 0, the loss is [ 0.13858435]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413773386556466, auc for val: 0.6430372598352476\n",
      "--------------------------------------------------------------\n",
      "Epoch 36: iteration 100, the loss is [ 0.18057042]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413773288902564, auc for val: 0.643022973464857\n",
      "--------------------------------------------------------------\n",
      "Epoch 36: iteration 200, the loss is [ 0.17008997]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413767003688636, auc for val: 0.6430358840792708\n",
      "--------------------------------------------------------------\n",
      "Epoch 36: iteration 300, the loss is [ 0.15343978]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413799612857378, auc for val: 0.6430297702366768\n",
      "--------------------------------------------------------------\n",
      "Epoch 36: iteration 400, the loss is [ 0.14201947]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413764700181842, auc for val: 0.6430223527486455\n",
      "--------------------------------------------------------------\n",
      "Epoch 36: iteration 500, the loss is [ 0.15012729]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413748099822698, auc for val: 0.6429994908797969\n",
      "--------------------------------------------------------------\n",
      "Epoch 36: iteration 600, the loss is [ 0.19791222]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413775481091951, auc for val: 0.6430041625116032\n",
      "--------------------------------------------------------------\n",
      "Epoch 36: iteration 700, the loss is [ 0.1197037]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413793179354194, auc for val: 0.6430245584827913\n",
      "--------------------------------------------------------------\n",
      "Epoch 36: iteration 800, the loss is [ 0.10600694]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.641382974571613, auc for val: 0.6430317582252725\n",
      "--------------------------------------------------------------\n",
      "Epoch 36: iteration 900, the loss is [ 0.18527186]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413790244111064, auc for val: 0.6430154796244898\n",
      "--------------------------------------------------------------\n",
      "Epoch 36: iteration 1000, the loss is [ 0.15940417]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413820495922713, auc for val: 0.6430329134078346\n",
      "--------------------------------------------------------------\n",
      "Epoch 36: iteration 1100, the loss is [ 0.15131705]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.641377757924425, auc for val: 0.64302251393691\n",
      "--------------------------------------------------------------\n",
      "Epoch 37: iteration 0, the loss is [ 0.15539603]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413806122715462, auc for val: 0.6430373588104978\n",
      "--------------------------------------------------------------\n",
      "Epoch 37: iteration 100, the loss is [ 0.17930163]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413781625250248, auc for val: 0.6430282149113177\n",
      "--------------------------------------------------------------\n",
      "Epoch 37: iteration 200, the loss is [ 0.14203553]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413808549193832, auc for val: 0.6430142127412881\n",
      "--------------------------------------------------------------\n",
      "Epoch 37: iteration 300, the loss is [ 0.14427076]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413766384812072, auc for val: 0.643030249559674\n",
      "--------------------------------------------------------------\n",
      "Epoch 37: iteration 400, the loss is [ 0.17028402]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.641380268312812, auc for val: 0.6430354768668132\n",
      "--------------------------------------------------------------\n",
      "Epoch 37: iteration 500, the loss is [ 0.183396]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.641380669859216, auc for val: 0.643042997571891\n",
      "--------------------------------------------------------------\n",
      "Epoch 37: iteration 600, the loss is [ 0.15594395]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413790162933748, auc for val: 0.6430345111511583\n",
      "--------------------------------------------------------------\n",
      "Epoch 37: iteration 700, the loss is [ 0.12025601]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413810043740547, auc for val: 0.6430366617419504\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37: iteration 800, the loss is [ 0.11982491]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413810387739468, auc for val: 0.6430401923305159\n",
      "--------------------------------------------------------------\n",
      "Epoch 37: iteration 900, the loss is [ 0.16394164]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413789165899492, auc for val: 0.6430288695619006\n",
      "--------------------------------------------------------------\n",
      "Epoch 37: iteration 1000, the loss is [ 0.14524147]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.641377143146914, auc for val: 0.6430212922995369\n",
      "--------------------------------------------------------------\n",
      "Epoch 37: iteration 1100, the loss is [ 0.17445102]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413821933404187, auc for val: 0.6430335963370606\n",
      "--------------------------------------------------------------\n",
      "Epoch 38: iteration 0, the loss is [ 0.14634621]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413804048675239, auc for val: 0.643029790031727\n",
      "--------------------------------------------------------------\n",
      "Epoch 38: iteration 100, the loss is [ 0.14778136]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413804637813579, auc for val: 0.6430353750636987\n",
      "--------------------------------------------------------------\n",
      "Epoch 38: iteration 200, the loss is [ 0.22736821]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413799538913684, auc for val: 0.6430267783562587\n",
      "--------------------------------------------------------------\n",
      "Epoch 38: iteration 300, the loss is [ 0.14885801]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413820767183545, auc for val: 0.6430278416332313\n",
      "--------------------------------------------------------------\n",
      "Epoch 38: iteration 400, the loss is [ 0.17244151]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.641382362285683, auc for val: 0.6430387685008462\n",
      "--------------------------------------------------------------\n",
      "Epoch 38: iteration 500, the loss is [ 0.14359671]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413734985667572, auc for val: 0.6430250378057883\n",
      "--------------------------------------------------------------\n",
      "Epoch 38: iteration 600, the loss is [ 0.17466368]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413781174756334, auc for val: 0.6430390852216465\n",
      "--------------------------------------------------------------\n",
      "Epoch 38: iteration 700, the loss is [ 0.16618134]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413767497584284, auc for val: 0.6430250307361276\n",
      "--------------------------------------------------------------\n",
      "Epoch 38: iteration 800, the loss is [ 0.14447039]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413740799490458, auc for val: 0.6429946905801653\n",
      "--------------------------------------------------------------\n",
      "Epoch 38: iteration 900, the loss is [ 0.14672446]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413784393316332, auc for val: 0.6430253630101816\n",
      "--------------------------------------------------------------\n",
      "Epoch 38: iteration 1000, the loss is [ 0.19929913]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413793516521359, auc for val: 0.6430406051987021\n",
      "--------------------------------------------------------------\n",
      "Epoch 38: iteration 1100, the loss is [ 0.14849927]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413812680395827, auc for val: 0.6430326051706271\n",
      "--------------------------------------------------------------\n",
      "Epoch 39: iteration 0, the loss is [ 0.1691912]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.641378960835605, auc for val: 0.6430112463116484\n",
      "--------------------------------------------------------------\n",
      "Epoch 39: iteration 100, the loss is [ 0.17480542]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413767228332792, auc for val: 0.6430210321360224\n",
      "--------------------------------------------------------------\n",
      "Epoch 39: iteration 200, the loss is [ 0.16489406]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413755459229519, auc for val: 0.6430204326287929\n",
      "--------------------------------------------------------------\n",
      "Epoch 39: iteration 300, the loss is [ 0.16893896]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413764024240038, auc for val: 0.6430246093843486\n",
      "--------------------------------------------------------------\n",
      "Epoch 39: iteration 400, the loss is [ 0.15599404]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413747493002173, auc for val: 0.6430301406868988\n",
      "--------------------------------------------------------------\n",
      "Epoch 39: iteration 500, the loss is [ 0.16659203]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413770511191653, auc for val: 0.6430298918348414\n",
      "--------------------------------------------------------------\n",
      "Epoch 39: iteration 600, the loss is [ 0.17269246]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413757927100284, auc for val: 0.6430234499599898\n",
      "--------------------------------------------------------------\n",
      "Epoch 39: iteration 700, the loss is [ 0.1180582]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413765948785404, auc for val: 0.6430162247667301\n",
      "--------------------------------------------------------------\n",
      "Epoch 39: iteration 800, the loss is [ 0.1673253]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413825718999788, auc for val: 0.643035308608888\n",
      "--------------------------------------------------------------\n",
      "Epoch 39: iteration 900, the loss is [ 0.18918209]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413748076112492, auc for val: 0.6430168596222632\n",
      "--------------------------------------------------------------\n",
      "Epoch 39: iteration 1000, the loss is [ 0.12816672]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413756651571574, auc for val: 0.643027407556063\n",
      "--------------------------------------------------------------\n",
      "Epoch 39: iteration 1100, the loss is [ 0.14162457]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.641374546276555, auc for val: 0.6430192958273486\n",
      "--------------------------------------------------------------\n",
      "Epoch 40: iteration 0, the loss is [ 0.15196124]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413773696396614, auc for val: 0.6430246998760057\n",
      "--------------------------------------------------------------\n",
      "Epoch 40: iteration 100, the loss is [ 0.16418065]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413793905127618, auc for val: 0.6430286758531969\n",
      "--------------------------------------------------------------\n",
      "Epoch 40: iteration 200, the loss is [ 0.18853122]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413740953405862, auc for val: 0.643005020768415\n",
      "--------------------------------------------------------------\n",
      "Epoch 40: iteration 300, the loss is [ 0.13195463]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413732142452193, auc for val: 0.643003946179985\n",
      "--------------------------------------------------------------\n",
      "Epoch 40: iteration 400, the loss is [ 0.17723106]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413754785297054, auc for val: 0.6430154739687612\n",
      "--------------------------------------------------------------\n",
      "Epoch 40: iteration 500, the loss is [ 0.16298382]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413790626689303, auc for val: 0.6430335638166214\n",
      "--------------------------------------------------------------\n",
      "Epoch 40: iteration 600, the loss is [ 0.17367345]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413766065327092, auc for val: 0.6430223555765098\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40: iteration 700, the loss is [ 0.19934449]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413766717960561, auc for val: 0.6430212286725904\n",
      "--------------------------------------------------------------\n",
      "Epoch 40: iteration 800, the loss is [ 0.14050472]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.641379177442402, auc for val: 0.6430344178316367\n",
      "--------------------------------------------------------------\n",
      "Epoch 40: iteration 900, the loss is [ 0.17136577]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413795565243907, auc for val: 0.6430299738429057\n",
      "--------------------------------------------------------------\n",
      "Epoch 40: iteration 1000, the loss is [ 0.18101215]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413750464413412, auc for val: 0.6430179016902537\n",
      "--------------------------------------------------------------\n",
      "Epoch 40: iteration 1100, the loss is [ 0.11651552]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413763480512772, auc for val: 0.6430151431086394\n",
      "--------------------------------------------------------------\n",
      "Epoch 41: iteration 0, the loss is [ 0.13360934]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413772186578921, auc for val: 0.6430228037929997\n",
      "--------------------------------------------------------------\n",
      "Epoch 41: iteration 100, the loss is [ 0.15367918]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413764948938072, auc for val: 0.6430168143764344\n",
      "--------------------------------------------------------------\n",
      "Epoch 41: iteration 200, the loss is [ 0.16970026]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413802118503722, auc for val: 0.6430277921456063\n",
      "--------------------------------------------------------------\n",
      "Epoch 41: iteration 300, the loss is [ 0.17119242]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413781847483196, auc for val: 0.643024790367663\n",
      "--------------------------------------------------------------\n",
      "Epoch 41: iteration 400, the loss is [ 0.12695394]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413766105915751, auc for val: 0.6430203930386929\n",
      "--------------------------------------------------------------\n",
      "Epoch 41: iteration 500, the loss is [ 0.15687451]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413785671658118, auc for val: 0.6430247055317343\n",
      "--------------------------------------------------------------\n",
      "Epoch 41: iteration 600, the loss is [ 0.14812072]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413747813290888, auc for val: 0.6430084679349839\n",
      "--------------------------------------------------------------\n",
      "Epoch 41: iteration 700, the loss is [ 0.14505284]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413790477194444, auc for val: 0.6430290463034187\n",
      "--------------------------------------------------------------\n",
      "Epoch 41: iteration 800, the loss is [ 0.20202875]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413767880564392, auc for val: 0.6430170222244598\n",
      "--------------------------------------------------------------\n",
      "Epoch 41: iteration 900, the loss is [ 0.15297909]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413732604600275, auc for val: 0.6430016994418071\n",
      "--------------------------------------------------------------\n",
      "Epoch 41: iteration 1000, the loss is [ 0.16229263]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413789306955124, auc for val: 0.6430303216702133\n",
      "--------------------------------------------------------------\n",
      "Epoch 41: iteration 1100, the loss is [ 0.13669272]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413806174154554, auc for val: 0.6430329727929848\n",
      "--------------------------------------------------------------\n",
      "Epoch 42: iteration 0, the loss is [ 0.12859908]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413757424765412, auc for val: 0.6430189578975659\n",
      "--------------------------------------------------------------\n",
      "Epoch 42: iteration 100, the loss is [ 0.17392726]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413797165080009, auc for val: 0.6430243463929696\n",
      "--------------------------------------------------------------\n",
      "Epoch 42: iteration 200, the loss is [ 0.12370034]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.64137474037875, auc for val: 0.6430074668710255\n",
      "--------------------------------------------------------------\n",
      "Epoch 42: iteration 300, the loss is [ 0.17687654]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413763345887024, auc for val: 0.6430165273482091\n",
      "--------------------------------------------------------------\n",
      "Epoch 42: iteration 400, the loss is [ 0.16226019]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413793384708688, auc for val: 0.6430248285438309\n",
      "--------------------------------------------------------------\n",
      "Epoch 42: iteration 500, the loss is [ 0.19143952]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413765167956076, auc for val: 0.6430238090987546\n",
      "--------------------------------------------------------------\n",
      "Epoch 42: iteration 600, the loss is [ 0.16960487]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413753337367016, auc for val: 0.6430205627105503\n",
      "--------------------------------------------------------------\n",
      "Epoch 42: iteration 700, the loss is [ 0.14333461]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413769547110566, auc for val: 0.6430281017967461\n",
      "--------------------------------------------------------------\n",
      "Epoch 42: iteration 800, the loss is [ 0.14257084]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413804892195957, auc for val: 0.6430287861399041\n",
      "--------------------------------------------------------------\n",
      "Epoch 42: iteration 900, the loss is [ 0.16924205]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413781708035037, auc for val: 0.6430256217597642\n",
      "--------------------------------------------------------------\n",
      "Epoch 42: iteration 1000, the loss is [ 0.14115521]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413761177407846, auc for val: 0.6430142240527453\n",
      "--------------------------------------------------------------\n",
      "Epoch 42: iteration 1100, the loss is [ 0.16026045]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413759995514356, auc for val: 0.6430220148188628\n",
      "--------------------------------------------------------------\n",
      "Epoch 43: iteration 0, the loss is [ 0.16093591]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413759011339873, auc for val: 0.6430220869294023\n",
      "--------------------------------------------------------------\n",
      "Epoch 43: iteration 100, the loss is [ 0.20119692]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413740917639619, auc for val: 0.64300555806263\n",
      "--------------------------------------------------------------\n",
      "Epoch 43: iteration 200, the loss is [ 0.17132984]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413763959137437, auc for val: 0.6430234160256184\n",
      "--------------------------------------------------------------\n",
      "Epoch 43: iteration 300, the loss is [ 0.15950154]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.641379014565343, auc for val: 0.6430314344348111\n",
      "--------------------------------------------------------------\n",
      "Epoch 43: iteration 400, the loss is [ 0.1455747]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413766374363506, auc for val: 0.6430257419439964\n",
      "--------------------------------------------------------------\n",
      "Epoch 43: iteration 500, the loss is [ 0.1640843]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413773024473488, auc for val: 0.6430227217849352\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43: iteration 600, the loss is [ 0.18498687]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413791253201357, auc for val: 0.6430240028074585\n",
      "--------------------------------------------------------------\n",
      "Epoch 43: iteration 700, the loss is [ 0.1706806]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413761624284948, auc for val: 0.643017952591811\n",
      "--------------------------------------------------------------\n",
      "Epoch 43: iteration 800, the loss is [ 0.14810188]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.641378178197873, auc for val: 0.6430299653593128\n",
      "--------------------------------------------------------------\n",
      "Epoch 43: iteration 900, the loss is [ 0.17856021]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413799757529821, auc for val: 0.6430301477565596\n",
      "--------------------------------------------------------------\n",
      "Epoch 43: iteration 1000, the loss is [ 0.18040711]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413702444012638, auc for val: 0.642994178736729\n",
      "--------------------------------------------------------------\n",
      "Epoch 43: iteration 1100, the loss is [ 0.16002193]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.641376614811188, auc for val: 0.6430116747330883\n",
      "--------------------------------------------------------------\n",
      "Epoch 44: iteration 0, the loss is [ 0.18343323]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.641378607834843, auc for val: 0.6430173530845816\n",
      "--------------------------------------------------------------\n",
      "Epoch 44: iteration 100, the loss is [ 0.17470165]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413764179762914, auc for val: 0.6430187330823549\n",
      "--------------------------------------------------------------\n",
      "Epoch 44: iteration 200, the loss is [ 0.16376731]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413797316182339, auc for val: 0.6430315503772471\n",
      "--------------------------------------------------------------\n",
      "Epoch 44: iteration 300, the loss is [ 0.13386714]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.641376832060974, auc for val: 0.6430223499207812\n",
      "--------------------------------------------------------------\n",
      "Epoch 44: iteration 400, the loss is [ 0.12928666]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413752812125675, auc for val: 0.6430039688028993\n",
      "--------------------------------------------------------------\n",
      "Epoch 44: iteration 500, the loss is [ 0.2874918]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413772805053616, auc for val: 0.6430227656168317\n",
      "--------------------------------------------------------------\n",
      "Epoch 44: iteration 600, the loss is [ 0.16386615]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413804380216256, auc for val: 0.6430328724038025\n",
      "--------------------------------------------------------------\n",
      "Epoch 44: iteration 700, the loss is [ 0.17400946]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413771335020844, auc for val: 0.6430207196570183\n",
      "--------------------------------------------------------------\n",
      "Epoch 44: iteration 800, the loss is [ 0.14106639]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413779074594698, auc for val: 0.6430263782134616\n",
      "--------------------------------------------------------------\n",
      "Epoch 44: iteration 900, the loss is [ 0.13354482]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413770983788301, auc for val: 0.6430169642532417\n",
      "--------------------------------------------------------------\n",
      "Epoch 44: iteration 1000, the loss is [ 0.13661021]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413807964074173, auc for val: 0.643033483222489\n",
      "--------------------------------------------------------------\n",
      "Epoch 44: iteration 1100, the loss is [ 0.13952754]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413801594066117, auc for val: 0.6430403761416947\n",
      "--------------------------------------------------------------\n",
      "Epoch 45: iteration 0, the loss is [ 0.16636643]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.641376311722419, auc for val: 0.6430154244811362\n",
      "--------------------------------------------------------------\n",
      "Epoch 45: iteration 100, the loss is [ 0.11346664]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413774390422474, auc for val: 0.643027739830117\n",
      "--------------------------------------------------------------\n",
      "Epoch 45: iteration 200, the loss is [ 0.17300814]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413790279475439, auc for val: 0.6430300940271381\n",
      "--------------------------------------------------------------\n",
      "Epoch 45: iteration 300, the loss is [ 0.11469667]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413774306432084, auc for val: 0.6430390074553786\n",
      "--------------------------------------------------------------\n",
      "Epoch 45: iteration 400, the loss is [ 0.12922806]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413781339924041, auc for val: 0.643021586397423\n",
      "--------------------------------------------------------------\n",
      "Epoch 45: iteration 500, the loss is [ 0.19818592]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413798577243804, auc for val: 0.6430370703683402\n",
      "--------------------------------------------------------------\n",
      "Epoch 45: iteration 600, the loss is [ 0.13817385]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413817579165508, auc for val: 0.6430440042915782\n",
      "--------------------------------------------------------------\n",
      "Epoch 45: iteration 700, the loss is [ 0.12767422]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413775881352377, auc for val: 0.6430254803665497\n",
      "--------------------------------------------------------------\n",
      "Epoch 45: iteration 800, the loss is [ 0.13859122]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.641376432604283, auc for val: 0.643018460193451\n",
      "--------------------------------------------------------------\n",
      "Epoch 45: iteration 900, the loss is [ 0.16429062]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413758084230632, auc for val: 0.6430133855909834\n",
      "--------------------------------------------------------------\n",
      "Epoch 45: iteration 1000, the loss is [ 0.15390088]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413756227199074, auc for val: 0.643015843005051\n",
      "--------------------------------------------------------------\n",
      "Epoch 45: iteration 1100, the loss is [ 0.12679358]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413813360758178, auc for val: 0.6430408059770667\n",
      "--------------------------------------------------------------\n",
      "Epoch 46: iteration 0, the loss is [ 0.15799925]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413759942065929, auc for val: 0.6430099568055324\n",
      "--------------------------------------------------------------\n",
      "Epoch 46: iteration 100, the loss is [ 0.11953662]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413801947307998, auc for val: 0.6430338197383394\n",
      "--------------------------------------------------------------\n",
      "Epoch 46: iteration 200, the loss is [ 0.18827216]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413819901560092, auc for val: 0.6430233029110468\n",
      "--------------------------------------------------------------\n",
      "Epoch 46: iteration 300, the loss is [ 0.19895892]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413760367242163, auc for val: 0.6430244015363232\n",
      "--------------------------------------------------------------\n",
      "Epoch 46: iteration 400, the loss is [ 0.19750841]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.641381731674577, auc for val: 0.6430424206875761\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46: iteration 500, the loss is [ 0.14536588]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413795532692605, auc for val: 0.6430347656589444\n",
      "--------------------------------------------------------------\n",
      "Epoch 46: iteration 600, the loss is [ 0.13224834]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413770639387513, auc for val: 0.6430225803917208\n",
      "--------------------------------------------------------------\n",
      "Epoch 46: iteration 700, the loss is [ 0.1821373]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413804757570211, auc for val: 0.6430392237869968\n",
      "--------------------------------------------------------------\n",
      "Epoch 46: iteration 800, the loss is [ 0.1300853]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413775001261681, auc for val: 0.6430218847371056\n",
      "--------------------------------------------------------------\n",
      "Epoch 46: iteration 900, the loss is [ 0.20690905]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413775719399615, auc for val: 0.6430266341351798\n",
      "--------------------------------------------------------------\n",
      "Epoch 46: iteration 1000, the loss is [ 0.18395923]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413804470234665, auc for val: 0.6430377928876663\n",
      "--------------------------------------------------------------\n",
      "Epoch 46: iteration 1100, the loss is [ 0.13417006]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413804843569941, auc for val: 0.6430352039779093\n",
      "--------------------------------------------------------------\n",
      "Epoch 47: iteration 0, the loss is [ 0.15024927]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413762998673163, auc for val: 0.6430234683411076\n",
      "--------------------------------------------------------------\n",
      "Epoch 47: iteration 100, the loss is [ 0.16660593]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413807263216557, auc for val: 0.6430369473562436\n",
      "--------------------------------------------------------------\n",
      "Epoch 47: iteration 200, the loss is [ 0.14895812]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413792618346607, auc for val: 0.6430311700295002\n",
      "--------------------------------------------------------------\n",
      "Epoch 47: iteration 300, the loss is [ 0.13469033]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.641384433110963, auc for val: 0.643033551091232\n",
      "--------------------------------------------------------------\n",
      "Epoch 47: iteration 400, the loss is [ 0.12802096]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413787771819753, auc for val: 0.6430323336956554\n",
      "--------------------------------------------------------------\n",
      "Epoch 47: iteration 500, the loss is [ 0.12090705]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413761990788472, auc for val: 0.6430016230894712\n",
      "--------------------------------------------------------------\n",
      "Epoch 47: iteration 600, the loss is [ 0.1521737]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413789411440779, auc for val: 0.6430292555653763\n",
      "--------------------------------------------------------------\n",
      "Epoch 47: iteration 700, the loss is [ 0.18610661]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.641375243236051, auc for val: 0.6430069564415213\n",
      "--------------------------------------------------------------\n",
      "Epoch 47: iteration 800, the loss is [ 0.18950114]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413786032937355, auc for val: 0.6430369473562437\n",
      "--------------------------------------------------------------\n",
      "Epoch 47: iteration 900, the loss is [ 0.13781293]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413788332827339, auc for val: 0.6430331877106708\n",
      "--------------------------------------------------------------\n",
      "Epoch 47: iteration 1000, the loss is [ 0.16504878]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413807234683936, auc for val: 0.6430367536475399\n",
      "--------------------------------------------------------------\n",
      "Epoch 47: iteration 1100, the loss is [ 0.14991459]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.64137924531789, auc for val: 0.6430352039779093\n",
      "--------------------------------------------------------------\n",
      "Epoch 48: iteration 0, the loss is [ 0.16041391]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413778027728824, auc for val: 0.6430279787846495\n",
      "--------------------------------------------------------------\n",
      "Epoch 48: iteration 100, the loss is [ 0.15056244]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.641381311561876, auc for val: 0.6430391318814073\n",
      "--------------------------------------------------------------\n",
      "Epoch 48: iteration 200, the loss is [ 0.1555797]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413786942766279, auc for val: 0.6430329119939024\n",
      "--------------------------------------------------------------\n",
      "Epoch 48: iteration 300, the loss is [ 0.12168694]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413809351322156, auc for val: 0.6430404044203376\n",
      "--------------------------------------------------------------\n",
      "Epoch 48: iteration 400, the loss is [ 0.14508212]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.641377343920116, auc for val: 0.6430277087236098\n",
      "--------------------------------------------------------------\n",
      "Epoch 48: iteration 500, the loss is [ 0.13424726]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413730009337388, auc for val: 0.6430060727339308\n",
      "--------------------------------------------------------------\n",
      "Epoch 48: iteration 600, the loss is [ 0.13189888]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413752019240312, auc for val: 0.6430189027542124\n",
      "--------------------------------------------------------------\n",
      "Epoch 48: iteration 700, the loss is [ 0.12682581]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413808075391579, auc for val: 0.6430343895529937\n",
      "--------------------------------------------------------------\n",
      "Epoch 48: iteration 800, the loss is [ 0.13358489]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413798184216999, auc for val: 0.6430398855072403\n",
      "--------------------------------------------------------------\n",
      "Epoch 48: iteration 900, the loss is [ 0.15987696]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413792842588895, auc for val: 0.6430328200883131\n",
      "--------------------------------------------------------------\n",
      "Epoch 48: iteration 1000, the loss is [ 0.12479864]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413764634275507, auc for val: 0.6430226256375495\n",
      "--------------------------------------------------------------\n",
      "Epoch 48: iteration 1100, the loss is [ 0.14941785]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413779353892888, auc for val: 0.6430332994113102\n",
      "--------------------------------------------------------------\n",
      "Epoch 49: iteration 0, the loss is [ 0.14936237]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.641377525564406, auc for val: 0.6430351658017412\n",
      "--------------------------------------------------------------\n",
      "Epoch 49: iteration 100, the loss is [ 0.156501]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413795500141306, auc for val: 0.6430374549578837\n",
      "--------------------------------------------------------------\n",
      "Epoch 49: iteration 200, the loss is [ 0.16805227]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413811952613064, auc for val: 0.6430384814726207\n",
      "--------------------------------------------------------------\n",
      "Epoch 49: iteration 300, the loss is [ 0.19879101]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413782827237131, auc for val: 0.643019056872816\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: iteration 400, the loss is [ 0.19317256]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413797472107083, auc for val: 0.6430359858823852\n",
      "--------------------------------------------------------------\n",
      "Epoch 49: iteration 500, the loss is [ 0.14067934]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413760116878463, auc for val: 0.6430133375172905\n",
      "--------------------------------------------------------------\n",
      "Epoch 49: iteration 600, the loss is [ 0.17800969]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413777801075329, auc for val: 0.6430315546190436\n",
      "--------------------------------------------------------------\n",
      "Epoch 49: iteration 700, the loss is [ 0.1569825]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413803310042041, auc for val: 0.6430373390154478\n",
      "--------------------------------------------------------------\n",
      "Epoch 49: iteration 800, the loss is [ 0.17061266]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413755914947719, auc for val: 0.6430064912578457\n",
      "--------------------------------------------------------------\n",
      "Epoch 49: iteration 900, the loss is [ 0.13173723]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413773273229717, auc for val: 0.6430284001364286\n",
      "--------------------------------------------------------------\n",
      "Epoch 49: iteration 1000, the loss is [ 0.16879275]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413764224772119, auc for val: 0.643019974514778\n",
      "--------------------------------------------------------------\n",
      "Epoch 49: iteration 1100, the loss is [ 0.14031462]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413763716409226, auc for val: 0.6430191968520984\n",
      "--------------------------------------------------------------\n",
      "Epoch 50: iteration 0, the loss is [ 0.17508867]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413797469294006, auc for val: 0.6430350738961519\n",
      "--------------------------------------------------------------\n",
      "Epoch 50: iteration 100, the loss is [ 0.14369926]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413778931127858, auc for val: 0.6430273764495559\n",
      "--------------------------------------------------------------\n",
      "Epoch 50: iteration 200, the loss is [ 0.16095865]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413781312998893, auc for val: 0.6430299229413485\n",
      "--------------------------------------------------------------\n",
      "Epoch 50: iteration 300, the loss is [ 0.1246604]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413773980517218, auc for val: 0.6430225888753137\n",
      "--------------------------------------------------------------\n",
      "Epoch 50: iteration 400, the loss is [ 0.16754496]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413787765791734, auc for val: 0.6430306440467424\n",
      "--------------------------------------------------------------\n",
      "Epoch 50: iteration 500, the loss is [ 0.16493708]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413799274484607, auc for val: 0.6430357186492098\n",
      "--------------------------------------------------------------\n",
      "Epoch 50: iteration 600, the loss is [ 0.21306355]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.641375478248398, auc for val: 0.6430164736187876\n",
      "--------------------------------------------------------------\n",
      "Epoch 50: iteration 700, the loss is [ 0.12058046]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413755958349452, auc for val: 0.6430223230560704\n",
      "--------------------------------------------------------------\n",
      "Epoch 50: iteration 800, the loss is [ 0.12837914]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413765463730849, auc for val: 0.6430221590399416\n",
      "--------------------------------------------------------------\n",
      "Epoch 50: iteration 900, the loss is [ 0.13165586]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413803879488853, auc for val: 0.6430385026816029\n",
      "--------------------------------------------------------------\n",
      "Epoch 50: iteration 1000, the loss is [ 0.20034604]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413784949903371, auc for val: 0.6430327691867559\n",
      "--------------------------------------------------------------\n",
      "Epoch 50: iteration 1100, the loss is [ 0.1909108]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413763077841137, auc for val: 0.6430187288405584\n",
      "--------------------------------------------------------------\n",
      "Epoch 51: iteration 0, the loss is [ 0.20320286]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413783415973604, auc for val: 0.6430284609355108\n",
      "--------------------------------------------------------------\n",
      "Epoch 51: iteration 100, the loss is [ 0.14781474]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413805107195283, auc for val: 0.6430414238654141\n",
      "--------------------------------------------------------------\n",
      "Epoch 51: iteration 200, the loss is [ 0.16593592]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413787746502077, auc for val: 0.6430298579004698\n",
      "--------------------------------------------------------------\n",
      "Epoch 51: iteration 300, the loss is [ 0.1584989]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413804168833741, auc for val: 0.6430401866747872\n",
      "--------------------------------------------------------------\n",
      "Epoch 51: iteration 400, the loss is [ 0.12639098]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413745334569692, auc for val: 0.643017278146178\n",
      "--------------------------------------------------------------\n",
      "Epoch 51: iteration 500, the loss is [ 0.15732546]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413789709626758, auc for val: 0.643029634499191\n",
      "--------------------------------------------------------------\n",
      "Epoch 51: iteration 600, the loss is [ 0.14476226]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413787238942918, auc for val: 0.643031871339844\n",
      "--------------------------------------------------------------\n",
      "Epoch 51: iteration 700, the loss is [ 0.14182939]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.641377850796096, auc for val: 0.6430274160396559\n",
      "--------------------------------------------------------------\n",
      "Epoch 51: iteration 800, the loss is [ 0.20013748]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.641380109574992, auc for val: 0.6430342608851687\n",
      "--------------------------------------------------------------\n",
      "Epoch 51: iteration 900, the loss is [ 0.13218841]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413811398839099, auc for val: 0.6430415737422213\n",
      "--------------------------------------------------------------\n",
      "Epoch 51: iteration 1000, the loss is [ 0.13937415]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413796847604368, auc for val: 0.6430390739101894\n",
      "--------------------------------------------------------------\n",
      "Epoch 51: iteration 1100, the loss is [ 0.15572523]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413786985364276, auc for val: 0.6430325556830021\n",
      "--------------------------------------------------------------\n",
      "Epoch 52: iteration 0, the loss is [ 0.18667507]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413772492802259, auc for val: 0.6430247394661058\n",
      "--------------------------------------------------------------\n",
      "Epoch 52: iteration 100, the loss is [ 0.17552446]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413812248387837, auc for val: 0.6430456571782555\n",
      "--------------------------------------------------------------\n",
      "Epoch 52: iteration 200, the loss is [ 0.19638775]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413768715645883, auc for val: 0.6430254704690247\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52: iteration 300, the loss is [ 0.14237863]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413770198136559, auc for val: 0.6430148956705141\n",
      "--------------------------------------------------------------\n",
      "Epoch 52: iteration 400, the loss is [ 0.16675253]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413779728031901, auc for val: 0.6430315319961293\n",
      "--------------------------------------------------------------\n",
      "Epoch 52: iteration 500, the loss is [ 0.15748021]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413801491187934, auc for val: 0.6430385196487886\n",
      "--------------------------------------------------------------\n",
      "Epoch 52: iteration 600, the loss is [ 0.15715325]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.641380304480923, auc for val: 0.6430415002177499\n",
      "--------------------------------------------------------------\n",
      "Epoch 52: iteration 700, the loss is [ 0.12366015]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413801975438751, auc for val: 0.6430395475774578\n",
      "--------------------------------------------------------------\n",
      "Epoch 52: iteration 800, the loss is [ 0.15234664]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413804270506318, auc for val: 0.6430426766092943\n",
      "--------------------------------------------------------------\n",
      "Epoch 52: iteration 900, the loss is [ 0.17948151]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413765526020374, auc for val: 0.6430169091098881\n",
      "--------------------------------------------------------------\n",
      "Epoch 52: iteration 1000, the loss is [ 0.15312392]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413793279821168, auc for val: 0.6430363110867785\n",
      "--------------------------------------------------------------\n",
      "Epoch 52: iteration 1100, the loss is [ 0.16697332]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413766809184572, auc for val: 0.6430182523454258\n",
      "--------------------------------------------------------------\n",
      "Epoch 53: iteration 0, the loss is [ 0.16519201]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413766333372981, auc for val: 0.64301942873697\n",
      "--------------------------------------------------------------\n",
      "Epoch 53: iteration 100, the loss is [ 0.18391536]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413778321494257, auc for val: 0.6430340346560255\n",
      "--------------------------------------------------------------\n",
      "Epoch 53: iteration 200, the loss is [ 0.14679936]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413771663346916, auc for val: 0.6430276337852061\n",
      "--------------------------------------------------------------\n",
      "Epoch 53: iteration 300, the loss is [ 0.19967176]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.641375798537113, auc for val: 0.6430204835303501\n",
      "--------------------------------------------------------------\n",
      "Epoch 53: iteration 400, the loss is [ 0.15462483]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413807341982665, auc for val: 0.6430416515084892\n",
      "--------------------------------------------------------------\n",
      "Epoch 53: iteration 500, the loss is [ 0.16022208]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413784493381438, auc for val: 0.6430326277935414\n",
      "--------------------------------------------------------------\n",
      "Epoch 53: iteration 600, the loss is [ 0.1651489]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413796513652145, auc for val: 0.6430361258616676\n",
      "--------------------------------------------------------------\n",
      "Epoch 53: iteration 700, the loss is [ 0.17424984]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413804239964358, auc for val: 0.6430421110364364\n",
      "--------------------------------------------------------------\n",
      "Epoch 53: iteration 800, the loss is [ 0.19182651]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413802624455408, auc for val: 0.6430424715891333\n",
      "--------------------------------------------------------------\n",
      "Epoch 53: iteration 900, the loss is [ 0.19560771]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413792316543816, auc for val: 0.6430273665520307\n",
      "--------------------------------------------------------------\n",
      "Epoch 53: iteration 1000, the loss is [ 0.15748887]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413788489957689, auc for val: 0.6430339823405361\n",
      "--------------------------------------------------------------\n",
      "Epoch 53: iteration 1100, the loss is [ 0.17056134]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413772204261108, auc for val: 0.643027759625167\n",
      "--------------------------------------------------------------\n",
      "Epoch 54: iteration 0, the loss is [ 0.1399781]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413812436060145, auc for val: 0.6430428335557623\n",
      "--------------------------------------------------------------\n",
      "Epoch 54: iteration 100, the loss is [ 0.13496062]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413795527870192, auc for val: 0.6430325146789698\n",
      "--------------------------------------------------------------\n",
      "Epoch 54: iteration 200, the loss is [ 0.16148712]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.641376272540299, auc for val: 0.6430137249346981\n",
      "--------------------------------------------------------------\n",
      "Epoch 54: iteration 300, the loss is [ 0.16599666]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413806290696243, auc for val: 0.6430396917985366\n",
      "--------------------------------------------------------------\n",
      "Epoch 54: iteration 400, the loss is [ 0.17916977]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413758580939354, auc for val: 0.6430240480532871\n",
      "--------------------------------------------------------------\n",
      "Epoch 54: iteration 500, the loss is [ 0.17231457]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413791856806939, auc for val: 0.643033580783807\n",
      "--------------------------------------------------------------\n",
      "Epoch 54: iteration 600, the loss is [ 0.10706247]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413800101126873, auc for val: 0.6430378466170877\n",
      "--------------------------------------------------------------\n",
      "Epoch 54: iteration 700, the loss is [ 0.18087231]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413756032695012, auc for val: 0.6430107839558371\n",
      "--------------------------------------------------------------\n",
      "Epoch 54: iteration 800, the loss is [ 0.13893974]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413779096697433, auc for val: 0.6430309607675427\n",
      "--------------------------------------------------------------\n",
      "Epoch 54: iteration 900, the loss is [ 0.14842124]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413792577356082, auc for val: 0.6430357243049385\n",
      "--------------------------------------------------------------\n",
      "Epoch 54: iteration 1000, the loss is [ 0.18370146]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413848513750716, auc for val: 0.643044687220804\n",
      "--------------------------------------------------------------\n",
      "Epoch 54: iteration 1100, the loss is [ 0.15770951]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413785943722684, auc for val: 0.643031109230418\n",
      "--------------------------------------------------------------\n",
      "Epoch 55: iteration 0, the loss is [ 0.15626565]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413760397784123, auc for val: 0.6430158571443725\n",
      "--------------------------------------------------------------\n",
      "Epoch 55: iteration 100, the loss is [ 0.18553694]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413771776271796, auc for val: 0.643021206049676\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55: iteration 200, the loss is [ 0.14767593]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413828550561, auc for val: 0.6430414521440568\n",
      "--------------------------------------------------------------\n",
      "Epoch 55: iteration 300, the loss is [ 0.14351703]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413783647851381, auc for val: 0.6430293135365941\n",
      "--------------------------------------------------------------\n",
      "Epoch 55: iteration 400, the loss is [ 0.11995388]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413759323189364, auc for val: 0.6430093389171851\n",
      "--------------------------------------------------------------\n",
      "Epoch 55: iteration 500, the loss is [ 0.13109368]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413771854636037, auc for val: 0.6430229268050962\n",
      "--------------------------------------------------------------\n",
      "Epoch 55: iteration 600, the loss is [ 0.16525359]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413764717060294, auc for val: 0.6430218720117162\n",
      "--------------------------------------------------------------\n",
      "Epoch 55: iteration 700, the loss is [ 0.16801837]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413764186594669, auc for val: 0.6430185846194797\n",
      "--------------------------------------------------------------\n",
      "Epoch 55: iteration 800, the loss is [ 0.19276762]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413766998464353, auc for val: 0.6430177277766\n",
      "--------------------------------------------------------------\n",
      "Epoch 55: iteration 900, the loss is [ 0.162432]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413768626029341, auc for val: 0.6430211127301546\n",
      "--------------------------------------------------------------\n",
      "Epoch 55: iteration 1000, the loss is [ 0.12807293]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413787235727977, auc for val: 0.6430258762675503\n",
      "--------------------------------------------------------------\n",
      "Epoch 55: iteration 1100, the loss is [ 0.14853965]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413793094560067, auc for val: 0.6430318359915403\n",
      "--------------------------------------------------------------\n",
      "Epoch 56: iteration 0, the loss is [ 0.18724518]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413793507680265, auc for val: 0.6430351149001841\n",
      "--------------------------------------------------------------\n",
      "Epoch 56: iteration 100, the loss is [ 0.16966863]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413756400002271, auc for val: 0.6430208073208112\n",
      "--------------------------------------------------------------\n",
      "Epoch 56: iteration 200, the loss is [ 0.12368611]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413817209045174, auc for val: 0.6430429367728088\n",
      "--------------------------------------------------------------\n",
      "Epoch 56: iteration 300, the loss is [ 0.16413184]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413776783143942, auc for val: 0.6430230427475321\n",
      "--------------------------------------------------------------\n",
      "Epoch 56: iteration 400, the loss is [ 0.16968358]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413794149865167, auc for val: 0.6430186440046297\n",
      "--------------------------------------------------------------\n",
      "Epoch 56: iteration 500, the loss is [ 0.1752889]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413771922551712, auc for val: 0.6430260855295077\n",
      "--------------------------------------------------------------\n",
      "Epoch 56: iteration 600, the loss is [ 0.17460515]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413784767455346, auc for val: 0.6430314641273863\n",
      "--------------------------------------------------------------\n",
      "Epoch 56: iteration 700, the loss is [ 0.12901866]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413783428431508, auc for val: 0.6430324666052769\n",
      "--------------------------------------------------------------\n",
      "Epoch 56: iteration 800, the loss is [ 0.13057235]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.641380807740092, auc for val: 0.6430427600312908\n",
      "--------------------------------------------------------------\n",
      "Epoch 56: iteration 900, the loss is [ 0.18058187]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413777941327223, auc for val: 0.6430186524882227\n",
      "--------------------------------------------------------------\n",
      "Epoch 56: iteration 1000, the loss is [ 0.14679894]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413806561957075, auc for val: 0.6430379201415592\n",
      "--------------------------------------------------------------\n",
      "Epoch 56: iteration 1100, the loss is [ 0.1086122]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413826269960676, auc for val: 0.6430388278859962\n",
      "--------------------------------------------------------------\n",
      "Epoch 57: iteration 0, the loss is [ 0.15884344]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413752843069503, auc for val: 0.6430173544985138\n",
      "--------------------------------------------------------------\n",
      "Epoch 57: iteration 100, the loss is [ 0.10020303]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.641377925583712, auc for val: 0.643028677267129\n",
      "--------------------------------------------------------------\n",
      "Epoch 57: iteration 200, the loss is [ 0.20147429]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.641378644364635, auc for val: 0.6430326221378129\n",
      "--------------------------------------------------------------\n",
      "Epoch 57: iteration 300, the loss is [ 0.18683808]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413820911454119, auc for val: 0.6430404001785412\n",
      "--------------------------------------------------------------\n",
      "Epoch 57: iteration 400, the loss is [ 0.18513259]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.641380547329694, auc for val: 0.6430397172493152\n",
      "--------------------------------------------------------------\n",
      "Epoch 57: iteration 500, the loss is [ 0.13349617]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413823511539424, auc for val: 0.6430422849500901\n",
      "--------------------------------------------------------------\n",
      "Epoch 57: iteration 600, the loss is [ 0.14775674]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413813223319358, auc for val: 0.6430428872851837\n",
      "--------------------------------------------------------------\n",
      "Epoch 57: iteration 700, the loss is [ 0.13941887]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413744947169037, auc for val: 0.6430145916751029\n",
      "--------------------------------------------------------------\n",
      "Epoch 57: iteration 800, the loss is [ 0.21032201]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.641379079547382, auc for val: 0.6430281922884034\n",
      "--------------------------------------------------------------\n",
      "Epoch 57: iteration 900, the loss is [ 0.13818152]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.641377340303305, auc for val: 0.6430247465357666\n",
      "--------------------------------------------------------------\n",
      "Epoch 57: iteration 1000, the loss is [ 0.15631646]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.641380066936808, auc for val: 0.6430382835221204\n",
      "--------------------------------------------------------------\n",
      "Epoch 57: iteration 1100, the loss is [ 0.1876965]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413760833408927, auc for val: 0.6430079589194119\n",
      "--------------------------------------------------------------\n",
      "Epoch 58: iteration 0, the loss is [ 0.13755289]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413783076797097, auc for val: 0.6430292654629012\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58: iteration 100, the loss is [ 0.17305626]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413814694557733, auc for val: 0.6430423005033438\n",
      "--------------------------------------------------------------\n",
      "Epoch 58: iteration 200, the loss is [ 0.19034794]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413845388424072, auc for val: 0.643033990824129\n",
      "--------------------------------------------------------------\n",
      "Epoch 58: iteration 300, the loss is [ 0.15255673]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413827461097128, auc for val: 0.6430381491985667\n",
      "--------------------------------------------------------------\n",
      "Epoch 58: iteration 400, the loss is [ 0.12074572]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413785860134161, auc for val: 0.6430270441755018\n",
      "--------------------------------------------------------------\n",
      "Epoch 58: iteration 500, the loss is [ 0.14917812]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.641379964661428, auc for val: 0.6430357342024635\n",
      "--------------------------------------------------------------\n",
      "Epoch 58: iteration 600, the loss is [ 0.16037221]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413794536462084, auc for val: 0.6430334294930675\n",
      "--------------------------------------------------------------\n",
      "Epoch 58: iteration 700, the loss is [ 0.16159984]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413767530939318, auc for val: 0.6430189267910588\n",
      "--------------------------------------------------------------\n",
      "Epoch 58: iteration 800, the loss is [ 0.19002686]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413807985373171, auc for val: 0.643038914135857\n",
      "--------------------------------------------------------------\n",
      "Epoch 58: iteration 900, the loss is [ 0.22132182]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413771414590689, auc for val: 0.6430189338607195\n",
      "--------------------------------------------------------------\n",
      "Epoch 58: iteration 1000, the loss is [ 0.12422789]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413808140092312, auc for val: 0.643042810932848\n",
      "--------------------------------------------------------------\n",
      "Epoch 58: iteration 1100, the loss is [ 0.16736914]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413814156858485, auc for val: 0.6430364538939252\n",
      "--------------------------------------------------------------\n",
      "Epoch 59: iteration 0, the loss is [ 0.16682197]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.641379474422779, auc for val: 0.6430354287931201\n",
      "--------------------------------------------------------------\n",
      "Epoch 59: iteration 100, the loss is [ 0.1655803]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413792511851614, auc for val: 0.643032008491262\n",
      "--------------------------------------------------------------\n",
      "Epoch 59: iteration 200, the loss is [ 0.13801302]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413785153248527, auc for val: 0.6430255185427175\n",
      "--------------------------------------------------------------\n",
      "Epoch 59: iteration 300, the loss is [ 0.17220689]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.641379440183634, auc for val: 0.643035640882942\n",
      "--------------------------------------------------------------\n",
      "Epoch 59: iteration 400, the loss is [ 0.17869768]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413845568059022, auc for val: 0.6430320834296657\n",
      "--------------------------------------------------------------\n",
      "Epoch 59: iteration 500, the loss is [ 0.16073538]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413769314830919, auc for val: 0.643023831721669\n",
      "--------------------------------------------------------------\n",
      "Epoch 59: iteration 600, the loss is [ 0.16519333]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413762501562571, auc for val: 0.643010939488373\n",
      "--------------------------------------------------------------\n",
      "Epoch 59: iteration 700, the loss is [ 0.17161085]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413780702561553, auc for val: 0.6430297857899305\n",
      "--------------------------------------------------------------\n",
      "Epoch 59: iteration 800, the loss is [ 0.14259674]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413800770638792, auc for val: 0.6430386271076316\n",
      "--------------------------------------------------------------\n",
      "Epoch 59: iteration 900, the loss is [ 0.15673012]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.641376501805935, auc for val: 0.643019056872816\n",
      "--------------------------------------------------------------\n",
      "Epoch 59: iteration 1000, the loss is [ 0.14696257]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413787535119562, auc for val: 0.6430301802769989\n",
      "--------------------------------------------------------------\n",
      "Epoch 59: iteration 1100, the loss is [ 0.16004182]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413803186668596, auc for val: 0.6430344574217368\n",
      "--------------------------------------------------------------\n",
      "Learning rate is 0.0004130048057886514. Weight decay is 0.015377794660046963. dropout is 0.5\n",
      " Val aus is 0.6430344574217368. Train auc is 0.6413803186668596\n",
      "BLOWINGUP: Learning rate is 0.0004130048057886514. Weight decay is 0.015377794660046963. dropout is 0.5\n",
      " Val aus is 0.6430344574217368. Train auc is 0.6413803186668596\n",
      "WARNING: YOUR TRAINING IS BLOWING UP THIS TIME!\n",
      "Epoch 0: iteration 0, the loss is [ 0.90558106]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.37113222301538906, auc for val: 0.3705456991084131\n",
      "--------------------------------------------------------------\n",
      "Epoch 0: iteration 100, the loss is [ 0.15369411]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6407448869544362, auc for val: 0.6421543383910469\n",
      "--------------------------------------------------------------\n",
      "Epoch 0: iteration 200, the loss is [ 0.13683602]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.641157875542176, auc for val: 0.6427864622071392\n",
      "--------------------------------------------------------------\n",
      "Epoch 0: iteration 300, the loss is [ 0.14918469]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413220792878593, auc for val: 0.6426913540614221\n",
      "--------------------------------------------------------------\n",
      "Epoch 0: iteration 400, the loss is [ 0.16498516]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406194690067184, auc for val: 0.6416250936917391\n",
      "--------------------------------------------------------------\n",
      "Epoch 0: iteration 500, the loss is [ 0.1293838]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413095910017981, auc for val: 0.6429345009026952\n",
      "--------------------------------------------------------------\n",
      "Epoch 0: iteration 600, the loss is [ 0.17406173]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.638922242227674, auc for val: 0.64125998813332\n",
      "--------------------------------------------------------------\n",
      "Epoch 0: iteration 700, the loss is [ 0.14070536]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6409624168421951, auc for val: 0.6427312820912578\n",
      "--------------------------------------------------------------\n",
      "Epoch 0: iteration 800, the loss is [ 0.15911444]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6411920561751472, auc for val: 0.6427200455725035\n",
      "--------------------------------------------------------------\n",
      "Epoch 0: iteration 900, the loss is [ 0.15699387]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6412650275088718, auc for val: 0.6430827545159374\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: iteration 1000, the loss is [ 0.15908463]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6408776873761937, auc for val: 0.6426719379452104\n",
      "--------------------------------------------------------------\n",
      "Epoch 0: iteration 1100, the loss is [ 0.16619278]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.641040586859613, auc for val: 0.6424170370860177\n",
      "--------------------------------------------------------------\n",
      "Epoch 1: iteration 0, the loss is [ 0.20237914]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.525291567272532, auc for val: 0.5266543600652127\n",
      "--------------------------------------------------------------\n",
      "Epoch 1: iteration 100, the loss is [ 0.17061432]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6412548897079615, auc for val: 0.6425766770948216\n",
      "--------------------------------------------------------------\n",
      "Epoch 1: iteration 200, the loss is [ 0.2160631]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6088589391733049, auc for val: 0.6116600298906952\n",
      "--------------------------------------------------------------\n",
      "Epoch 1: iteration 300, the loss is [ 0.16238989]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6408805681662229, auc for val: 0.6420805509281405\n",
      "--------------------------------------------------------------\n",
      "Epoch 1: iteration 400, the loss is [ 0.18661435]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6411251726578406, auc for val: 0.6425667527050974\n",
      "--------------------------------------------------------------\n",
      "Epoch 1: iteration 500, the loss is [ 0.15569276]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6412791422346911, auc for val: 0.643004306732682\n",
      "--------------------------------------------------------------\n",
      "Epoch 1: iteration 600, the loss is [ 0.19130659]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.640780992253333, auc for val: 0.6427700832171743\n",
      "--------------------------------------------------------------\n",
      "Epoch 1: iteration 700, the loss is [ 0.17127527]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.37061983863195164, auc for val: 0.37057447545542316\n",
      "--------------------------------------------------------------\n",
      "Epoch 1: iteration 800, the loss is [ 0.18049175]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.640970666386412, auc for val: 0.6427677431594748\n",
      "--------------------------------------------------------------\n",
      "Epoch 1: iteration 900, the loss is [ 0.15745477]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6193704816097385, auc for val: 0.6221837048567769\n",
      "--------------------------------------------------------------\n",
      "Epoch 1: iteration 1000, the loss is [ 0.18522003]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6400583406191688, auc for val: 0.641476046862564\n",
      "--------------------------------------------------------------\n",
      "Epoch 1: iteration 1100, the loss is [ 0.1897108]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6383725395874624, auc for val: 0.639890570814276\n",
      "--------------------------------------------------------------\n",
      "Epoch 2: iteration 0, the loss is [ 0.13346834]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413128048599416, auc for val: 0.6428743366760037\n",
      "--------------------------------------------------------------\n",
      "Epoch 2: iteration 100, the loss is [ 0.12838294]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.5310224605373304, auc for val: 0.5330937633633378\n",
      "--------------------------------------------------------------\n",
      "Epoch 2: iteration 200, the loss is [ 0.15303805]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.5158615854939909, auc for val: 0.5154765209087662\n",
      "--------------------------------------------------------------\n",
      "Epoch 2: iteration 300, the loss is [ 0.13943386]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6240409913689086, auc for val: 0.624065033480966\n",
      "--------------------------------------------------------------\n",
      "Epoch 2: iteration 400, the loss is [ 0.13820608]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6380791109190871, auc for val: 0.6393404239560724\n",
      "--------------------------------------------------------------\n",
      "Epoch 2: iteration 500, the loss is [ 0.11575011]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6272193703234588, auc for val: 0.6284675360288786\n",
      "--------------------------------------------------------------\n",
      "Epoch 2: iteration 600, the loss is [ 0.15631752]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.5720371213877988, auc for val: 0.5718763073835206\n",
      "--------------------------------------------------------------\n",
      "Epoch 2: iteration 700, the loss is [ 0.14519219]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6255621229704111, auc for val: 0.6259681691806234\n",
      "--------------------------------------------------------------\n",
      "Epoch 2: iteration 800, the loss is [ 0.16157646]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6412887811567929, auc for val: 0.6426823727644386\n",
      "--------------------------------------------------------------\n",
      "Epoch 2: iteration 900, the loss is [ 0.18012269]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6367958044183177, auc for val: 0.6386297816601175\n",
      "--------------------------------------------------------------\n",
      "Epoch 2: iteration 1000, the loss is [ 0.17246135]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6287806095468176, auc for val: 0.629837661727927\n",
      "--------------------------------------------------------------\n",
      "Epoch 2: iteration 1100, the loss is [ 0.14390215]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6386431971347418, auc for val: 0.6400264468655229\n",
      "--------------------------------------------------------------\n",
      "Epoch 3: iteration 0, the loss is [ 0.15703379]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6072666621248737, auc for val: 0.6107676082364136\n",
      "--------------------------------------------------------------\n",
      "Epoch 3: iteration 100, the loss is [ 0.13604586]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6391900782604394, auc for val: 0.6410188999772495\n",
      "--------------------------------------------------------------\n",
      "Epoch 3: iteration 200, the loss is [ 0.19287883]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6410921074494079, auc for val: 0.642614315968515\n",
      "--------------------------------------------------------------\n",
      "Epoch 3: iteration 300, the loss is [ 0.12949905]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6392639959930645, auc for val: 0.6411580224167008\n",
      "--------------------------------------------------------------\n",
      "Epoch 3: iteration 400, the loss is [ 0.1404445]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6342650036334179, auc for val: 0.6347044670717388\n",
      "--------------------------------------------------------------\n",
      "Epoch 3: iteration 500, the loss is [ 0.19413872]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6391272020090676, auc for val: 0.6414243945073836\n",
      "--------------------------------------------------------------\n",
      "Epoch 3: iteration 600, the loss is [ 0.19349289]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.5809397379915756, auc for val: 0.5813836634772044\n",
      "--------------------------------------------------------------\n",
      "Epoch 3: iteration 700, the loss is [ 0.18921569]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405610921888967, auc for val: 0.6416840617318363\n",
      "--------------------------------------------------------------\n",
      "Epoch 3: iteration 800, the loss is [ 0.20160913]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.5488193289449537, auc for val: 0.5476365390369597\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: iteration 900, the loss is [ 0.15595786]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6189265089266643, auc for val: 0.6195295152665351\n",
      "--------------------------------------------------------------\n",
      "Epoch 3: iteration 1000, the loss is [ 0.1621576]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.640734475119806, auc for val: 0.641826245334381\n",
      "--------------------------------------------------------------\n",
      "Epoch 3: iteration 1100, the loss is [ 0.12423261]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.5190007799574338, auc for val: 0.5187083314727166\n",
      "--------------------------------------------------------------\n",
      "Epoch 4: iteration 0, the loss is [ 0.1893075]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6410261080414779, auc for val: 0.6428672443923658\n",
      "--------------------------------------------------------------\n",
      "Epoch 4: iteration 100, the loss is [ 0.10812939]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6298484982971282, auc for val: 0.630736730277221\n",
      "--------------------------------------------------------------\n",
      "Epoch 4: iteration 200, the loss is [ 0.18664524]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6307899655552048, auc for val: 0.6328988502721052\n",
      "--------------------------------------------------------------\n",
      "Epoch 4: iteration 300, the loss is [ 0.18321146]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.637984149615087, auc for val: 0.6401218066911618\n",
      "--------------------------------------------------------------\n",
      "Epoch 4: iteration 400, the loss is [ 0.14033335]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.5941650107166772, auc for val: 0.5957564152243515\n",
      "--------------------------------------------------------------\n",
      "Epoch 4: iteration 500, the loss is [ 0.14882945]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.5162148682063956, auc for val: 0.517634222365698\n",
      "--------------------------------------------------------------\n",
      "Epoch 4: iteration 600, the loss is [ 0.15476324]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6338083444630352, auc for val: 0.6358098608413922\n",
      "--------------------------------------------------------------\n",
      "Epoch 4: iteration 700, the loss is [ 0.20139545]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.5913528571526254, auc for val: 0.5939279775599015\n",
      "--------------------------------------------------------------\n",
      "Epoch 4: iteration 800, the loss is [ 0.17686588]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405816038479215, auc for val: 0.6421158624695237\n",
      "--------------------------------------------------------------\n",
      "Epoch 4: iteration 900, the loss is [ 0.17507024]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6400513411654233, auc for val: 0.6421303835526504\n",
      "--------------------------------------------------------------\n",
      "Epoch 4: iteration 1000, the loss is [ 0.15465046]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.5750446217316221, auc for val: 0.5771978375468826\n",
      "--------------------------------------------------------------\n",
      "Epoch 4: iteration 1100, the loss is [ 0.18070649]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6394283098907386, auc for val: 0.6414453051498725\n",
      "--------------------------------------------------------------\n",
      "Epoch 5: iteration 0, the loss is [ 0.17785186]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6307624498611404, auc for val: 0.6324941616832934\n",
      "--------------------------------------------------------------\n",
      "Epoch 5: iteration 100, the loss is [ 0.14295043]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6356450089140457, auc for val: 0.6377004634465187\n",
      "--------------------------------------------------------------\n",
      "Epoch 5: iteration 200, the loss is [ 0.17165475]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.5070623916835608, auc for val: 0.507656456703182\n",
      "--------------------------------------------------------------\n",
      "Epoch 5: iteration 300, the loss is [ 0.1629723]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6046932491166277, auc for val: 0.6059701265717273\n",
      "--------------------------------------------------------------\n",
      "Epoch 5: iteration 400, the loss is [ 0.19696167]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6327672692230103, auc for val: 0.6350746288515078\n",
      "--------------------------------------------------------------\n",
      "Epoch 5: iteration 500, the loss is [ 0.11090232]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.5631542027007399, auc for val: 0.5624447804707602\n",
      "--------------------------------------------------------------\n",
      "Epoch 5: iteration 600, the loss is [ 0.19417532]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6391598072796905, auc for val: 0.640373256141989\n",
      "--------------------------------------------------------------\n",
      "Epoch 5: iteration 700, the loss is [ 0.17503802]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.5771362877603288, auc for val: 0.5773317510603126\n",
      "--------------------------------------------------------------\n",
      "Epoch 5: iteration 800, the loss is [ 0.11608489]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6141482242251826, auc for val: 0.6149248161801254\n",
      "--------------------------------------------------------------\n",
      "Epoch 5: iteration 900, the loss is [ 0.19295321]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6094912732477812, auc for val: 0.6109462840136822\n",
      "--------------------------------------------------------------\n",
      "Epoch 5: iteration 1000, the loss is [ 0.14178066]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.624004996021069, auc for val: 0.6248030141549398\n",
      "--------------------------------------------------------------\n",
      "Epoch 5: iteration 1100, the loss is [ 0.17299928]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6357071140211072, auc for val: 0.6358150768370743\n",
      "--------------------------------------------------------------\n",
      "Epoch 6: iteration 0, the loss is [ 0.18761261]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6207847637706991, auc for val: 0.6211057116781566\n",
      "--------------------------------------------------------------\n",
      "Epoch 6: iteration 100, the loss is [ 0.1598161]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6030856834058727, auc for val: 0.6051747685313192\n",
      "--------------------------------------------------------------\n",
      "Epoch 6: iteration 200, the loss is [ 0.15134756]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6352527987452417, auc for val: 0.6359053309537384\n",
      "--------------------------------------------------------------\n",
      "Epoch 6: iteration 300, the loss is [ 0.18795197]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6109005871962881, auc for val: 0.6116392153955923\n",
      "--------------------------------------------------------------\n",
      "Epoch 6: iteration 400, the loss is [ 0.16948508]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.506120257284586, auc for val: 0.5063352007408529\n",
      "--------------------------------------------------------------\n",
      "Epoch 6: iteration 500, the loss is [ 0.16376573]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6397110119815459, auc for val: 0.6416568901978106\n",
      "--------------------------------------------------------------\n",
      "Epoch 6: iteration 600, the loss is [ 0.1499531]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6409746725271168, auc for val: 0.6424793066576728\n",
      "--------------------------------------------------------------\n",
      "Epoch 6: iteration 700, the loss is [ 0.17142229]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6126265599477807, auc for val: 0.6139062632949389\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: iteration 800, the loss is [ 0.10943824]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6379576024628443, auc for val: 0.6388790550693739\n",
      "--------------------------------------------------------------\n",
      "Epoch 6: iteration 900, the loss is [ 0.15277018]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.575206883731994, auc for val: 0.5755451728570083\n",
      "--------------------------------------------------------------\n",
      "Epoch 6: iteration 1000, the loss is [ 0.14108901]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.5473930754607962, auc for val: 0.546650335505313\n",
      "--------------------------------------------------------------\n",
      "Epoch 6: iteration 1100, the loss is [ 0.14443119]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.639552869810307, auc for val: 0.6416468597631756\n",
      "--------------------------------------------------------------\n",
      "Epoch 7: iteration 0, the loss is [ 0.15499501]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6277270922355399, auc for val: 0.6292221883767715\n",
      "--------------------------------------------------------------\n",
      "Epoch 7: iteration 100, the loss is [ 0.14532025]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.5654560994503812, auc for val: 0.5645193314061231\n",
      "--------------------------------------------------------------\n",
      "Epoch 7: iteration 200, the loss is [ 0.15510648]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6214294104345912, auc for val: 0.6227797295712814\n",
      "--------------------------------------------------------------\n",
      "Epoch 7: iteration 300, the loss is [ 0.15270799]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.640498196647866, auc for val: 0.6419301184454634\n",
      "--------------------------------------------------------------\n",
      "Epoch 7: iteration 400, the loss is [ 0.15487386]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6228908805269747, auc for val: 0.6244212326808053\n",
      "--------------------------------------------------------------\n",
      "Epoch 7: iteration 500, the loss is [ 0.14960589]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6351412332643523, auc for val: 0.6378158827274951\n",
      "--------------------------------------------------------------\n",
      "Epoch 7: iteration 600, the loss is [ 0.15188685]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6351164876055244, auc for val: 0.6377074977589388\n",
      "--------------------------------------------------------------\n",
      "Epoch 7: iteration 700, the loss is [ 0.17337927]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.633798630512197, auc for val: 0.6357466962506895\n",
      "--------------------------------------------------------------\n",
      "Epoch 7: iteration 800, the loss is [ 0.13229471]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6403722446578979, auc for val: 0.6420291954987107\n",
      "--------------------------------------------------------------\n",
      "Epoch 7: iteration 900, the loss is [ 0.19896303]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.5232210080692076, auc for val: 0.5254309383097757\n",
      "--------------------------------------------------------------\n",
      "Epoch 7: iteration 1000, the loss is [ 0.16079213]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.5223466691061741, auc for val: 0.5239873036924536\n",
      "--------------------------------------------------------------\n",
      "Epoch 7: iteration 1100, the loss is [ 0.21443997]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.629677376435762, auc for val: 0.6316215308372422\n",
      "--------------------------------------------------------------\n",
      "Epoch 8: iteration 0, the loss is [ 0.12976171]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.5841209563839482, auc for val: 0.5844040332142956\n",
      "--------------------------------------------------------------\n",
      "Epoch 8: iteration 100, the loss is [ 0.10203518]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6331697161727884, auc for val: 0.6354773054148775\n",
      "--------------------------------------------------------------\n",
      "Epoch 8: iteration 200, the loss is [ 0.15549295]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6269510610510562, auc for val: 0.6277348802233619\n",
      "--------------------------------------------------------------\n",
      "Epoch 8: iteration 300, the loss is [ 0.15303767]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.5328153313939341, auc for val: 0.5343984056229661\n",
      "--------------------------------------------------------------\n",
      "Epoch 8: iteration 400, the loss is [ 0.18175435]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6347479105242718, auc for val: 0.6372806288165722\n",
      "--------------------------------------------------------------\n",
      "Epoch 8: iteration 500, the loss is [ 0.15308774]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6328355272490895, auc for val: 0.6350869159218456\n",
      "--------------------------------------------------------------\n",
      "Epoch 8: iteration 600, the loss is [ 0.13325405]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6234334507353534, auc for val: 0.6248516661461088\n",
      "--------------------------------------------------------------\n",
      "Epoch 8: iteration 700, the loss is [ 0.15480264]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6395643674518118, auc for val: 0.641283098854226\n",
      "--------------------------------------------------------------\n",
      "Epoch 8: iteration 800, the loss is [ 0.19001222]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6337781374596559, auc for val: 0.6359635114336308\n",
      "--------------------------------------------------------------\n",
      "Epoch 8: iteration 900, the loss is [ 0.12613779]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6326180944526152, auc for val: 0.6358890636644129\n",
      "--------------------------------------------------------------\n",
      "Epoch 8: iteration 1000, the loss is [ 0.18679795]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6368646007866335, auc for val: 0.639125903585002\n",
      "--------------------------------------------------------------\n",
      "Epoch 8: iteration 1100, the loss is [ 0.20727883]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6043524612809176, auc for val: 0.6054588784007582\n",
      "--------------------------------------------------------------\n",
      "Epoch 9: iteration 0, the loss is [ 0.17864613]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6405768973318436, auc for val: 0.6415819023065132\n",
      "--------------------------------------------------------------\n",
      "Epoch 9: iteration 100, the loss is [ 0.11183017]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6364062123384978, auc for val: 0.6386871307479088\n",
      "--------------------------------------------------------------\n",
      "Epoch 9: iteration 200, the loss is [ 0.14542018]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6330138380449151, auc for val: 0.6351941244988575\n",
      "--------------------------------------------------------------\n",
      "Epoch 9: iteration 300, the loss is [ 0.18647392]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6412093324359094, auc for val: 0.6428584044885965\n",
      "--------------------------------------------------------------\n",
      "Epoch 9: iteration 400, the loss is [ 0.17098513]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6333405419780738, auc for val: 0.6356567277483185\n",
      "--------------------------------------------------------------\n",
      "Epoch 9: iteration 500, the loss is [ 0.13727327]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6410686187128058, auc for val: 0.6424277320687607\n",
      "--------------------------------------------------------------\n",
      "Epoch 9: iteration 600, the loss is [ 0.16815598]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6369910858945557, auc for val: 0.6391754661484722\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: iteration 700, the loss is [ 0.17215151]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6071677433463711, auc for val: 0.6102320644694013\n",
      "--------------------------------------------------------------\n",
      "Epoch 9: iteration 800, the loss is [ 0.16671154]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6207713485361435, auc for val: 0.6217488160912854\n",
      "--------------------------------------------------------------\n",
      "Epoch 9: iteration 900, the loss is [ 0.16707864]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6042916885266864, auc for val: 0.6076061436245295\n",
      "--------------------------------------------------------------\n",
      "Epoch 9: iteration 1000, the loss is [ 0.1660666]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6022933064455588, auc for val: 0.6052083267968427\n",
      "--------------------------------------------------------------\n",
      "Epoch 9: iteration 1100, the loss is [ 0.14786676]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.636402354969289, auc for val: 0.6373983895691805\n",
      "--------------------------------------------------------------\n",
      "Epoch 10: iteration 0, the loss is [ 0.15303925]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6248165982210783, auc for val: 0.6249350344132278\n",
      "--------------------------------------------------------------\n",
      "Epoch 10: iteration 100, the loss is [ 0.14215367]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.5902156133170601, auc for val: 0.592713069089908\n",
      "--------------------------------------------------------------\n",
      "Epoch 10: iteration 200, the loss is [ 0.14636698]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.623921373982435, auc for val: 0.6242493847818635\n",
      "--------------------------------------------------------------\n",
      "Epoch 10: iteration 300, the loss is [ 0.12139269]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6409278060900123, auc for val: 0.642094670454538\n",
      "--------------------------------------------------------------\n",
      "Epoch 10: iteration 400, the loss is [ 0.16809572]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6411359446866686, auc for val: 0.6427983562043409\n",
      "--------------------------------------------------------------\n",
      "Epoch 10: iteration 500, the loss is [ 0.13299061]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6347814998086306, auc for val: 0.6354103217934516\n",
      "--------------------------------------------------------------\n",
      "Epoch 10: iteration 600, the loss is [ 0.14506486]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6377406451966539, auc for val: 0.6381322415612918\n",
      "--------------------------------------------------------------\n",
      "Epoch 10: iteration 700, the loss is [ 0.20994404]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6224771243715495, auc for val: 0.6235798652884067\n",
      "--------------------------------------------------------------\n",
      "Epoch 10: iteration 800, the loss is [ 0.139818]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6354226706768835, auc for val: 0.6356752290504326\n",
      "--------------------------------------------------------------\n",
      "Epoch 10: iteration 900, the loss is [ 0.17339662]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6333888058304209, auc for val: 0.6339567415774233\n",
      "--------------------------------------------------------------\n",
      "Epoch 10: iteration 1000, the loss is [ 0.15467303]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6377700257988871, auc for val: 0.6386136458664815\n",
      "--------------------------------------------------------------\n",
      "Epoch 10: iteration 1100, the loss is [ 0.18271656]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.637461026196176, auc for val: 0.6379952767822934\n",
      "--------------------------------------------------------------\n",
      "Epoch 11: iteration 0, the loss is [ 0.20037608]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6274904101274829, auc for val: 0.6279712189813581\n",
      "--------------------------------------------------------------\n",
      "Epoch 11: iteration 100, the loss is [ 0.16439627]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6283992805541624, auc for val: 0.628084865191425\n",
      "--------------------------------------------------------------\n",
      "Epoch 11: iteration 200, the loss is [ 0.14169519]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6370720993684748, auc for val: 0.6375786616758407\n",
      "--------------------------------------------------------------\n",
      "Epoch 11: iteration 300, the loss is [ 0.18319257]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6092298666191476, auc for val: 0.6121692801755437\n",
      "--------------------------------------------------------------\n",
      "Epoch 11: iteration 400, the loss is [ 0.12840904]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6272158672811758, auc for val: 0.6298485999069988\n",
      "--------------------------------------------------------------\n",
      "Epoch 11: iteration 500, the loss is [ 0.18835926]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6403314195812827, auc for val: 0.6418153750240521\n",
      "--------------------------------------------------------------\n",
      "Epoch 11: iteration 600, the loss is [ 0.13139684]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6385992921425536, auc for val: 0.6396199512714302\n",
      "--------------------------------------------------------------\n",
      "Epoch 11: iteration 700, the loss is [ 0.15233675]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.5760258529743154, auc for val: 0.5772770940993249\n",
      "--------------------------------------------------------------\n",
      "Epoch 11: iteration 800, the loss is [ 0.18025467]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6231497434121854, auc for val: 0.6239860116412598\n",
      "--------------------------------------------------------------\n",
      "Epoch 11: iteration 900, the loss is [ 0.15139169]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6341453787722015, auc for val: 0.6352189588030481\n",
      "--------------------------------------------------------------\n",
      "Epoch 11: iteration 1000, the loss is [ 0.16303362]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6326597029305114, auc for val: 0.6334368500392531\n",
      "--------------------------------------------------------------\n",
      "Epoch 11: iteration 1100, the loss is [ 0.13873205]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6383198237219774, auc for val: 0.6390591108444159\n",
      "--------------------------------------------------------------\n",
      "Epoch 12: iteration 0, the loss is [ 0.18290262]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6397809540752397, auc for val: 0.6410535116222211\n",
      "--------------------------------------------------------------\n",
      "Epoch 12: iteration 100, the loss is [ 0.18648146]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6108967963362166, auc for val: 0.6124309862901494\n",
      "--------------------------------------------------------------\n",
      "Epoch 12: iteration 200, the loss is [ 0.11640407]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.5942982356716822, auc for val: 0.5967980335846157\n",
      "--------------------------------------------------------------\n",
      "Epoch 12: iteration 300, the loss is [ 0.16885962]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.5, auc for val: 0.5\n",
      "--------------------------------------------------------------\n",
      "Epoch 12: iteration 400, the loss is [ 0.164581]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.5, auc for val: 0.5\n",
      "--------------------------------------------------------------\n",
      "Epoch 12: iteration 500, the loss is [ 0.14667886]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.5, auc for val: 0.5\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: iteration 600, the loss is [ 0.14273137]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.5, auc for val: 0.5\n",
      "--------------------------------------------------------------\n",
      "Epoch 12: iteration 700, the loss is [ 0.11679047]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.5, auc for val: 0.5\n",
      "--------------------------------------------------------------\n",
      "BLOWINGUP: Learning rate is 0.025221516643856644. Weight decay is 0.015984258287073554. dropout is 0.5\n",
      " Val aus is 0.5. Train auc is 0.5\n",
      "WARNING: YOUR TRAINING IS BLOWING UP THIS TIME!\n",
      "Epoch 0: iteration 0, the loss is [ 1.17364573]\n",
      "  acc for train: 0.03798814540029435, acc for val: 0.03827\n",
      "  auc for train: 0.35931559677640257, auc for val: 0.3572875161366067\n",
      "--------------------------------------------------------------\n",
      "Epoch 0: iteration 100, the loss is [ 0.26455334]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.49541520372056713, auc for val: 0.49717576951609743\n",
      "--------------------------------------------------------------\n",
      "Epoch 0: iteration 200, the loss is [ 0.2169254]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6409720089466866, auc for val: 0.642747009258504\n",
      "--------------------------------------------------------------\n",
      "Epoch 0: iteration 300, the loss is [ 0.20562884]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6412683379358711, auc for val: 0.6428645763024085\n",
      "--------------------------------------------------------------\n",
      "Epoch 0: iteration 400, the loss is [ 0.15386061]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6412681484149704, auc for val: 0.6429116333781182\n",
      "--------------------------------------------------------------\n",
      "Epoch 0: iteration 500, the loss is [ 0.18462963]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6410356100471863, auc for val: 0.6424924717798727\n",
      "--------------------------------------------------------------\n",
      "Epoch 0: iteration 600, the loss is [ 0.19153385]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6412803734373711, auc for val: 0.6429340625837303\n",
      "--------------------------------------------------------------\n",
      "Epoch 0: iteration 700, the loss is [ 0.15470406]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6411463065289323, auc for val: 0.6424056648297775\n",
      "--------------------------------------------------------------\n",
      "Epoch 0: iteration 800, the loss is [ 0.17513001]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6410476737196262, auc for val: 0.6426661266840954\n",
      "--------------------------------------------------------------\n",
      "Epoch 0: iteration 900, the loss is [ 0.17406233]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6412763050071423, auc for val: 0.6426442771906624\n",
      "--------------------------------------------------------------\n",
      "Epoch 0: iteration 1000, the loss is [ 0.18328619]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413051508035763, auc for val: 0.6429435401708968\n",
      "--------------------------------------------------------------\n",
      "Epoch 0: iteration 1100, the loss is [ 0.18025948]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6409286090220733, auc for val: 0.6428256889266312\n",
      "--------------------------------------------------------------\n",
      "Epoch 1: iteration 0, the loss is [ 0.14252989]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.641186718485348, auc for val: 0.6425045001006282\n",
      "--------------------------------------------------------------\n",
      "Epoch 1: iteration 100, the loss is [ 0.15740532]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6409307146089265, auc for val: 0.6425026930953471\n",
      "--------------------------------------------------------------\n",
      "Epoch 1: iteration 200, the loss is [ 0.18880889]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6406745308162469, auc for val: 0.6418003364417604\n",
      "--------------------------------------------------------------\n",
      "Epoch 1: iteration 300, the loss is [ 0.19607434]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413023092760408, auc for val: 0.6429530587620953\n",
      "--------------------------------------------------------------\n",
      "Epoch 1: iteration 400, the loss is [ 0.15160185]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404703703501031, auc for val: 0.6417285595903639\n",
      "--------------------------------------------------------------\n",
      "Epoch 1: iteration 500, the loss is [ 0.16278738]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6410048350437197, auc for val: 0.6428493864293774\n",
      "--------------------------------------------------------------\n",
      "Epoch 1: iteration 600, the loss is [ 0.17977433]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6408541005034507, auc for val: 0.6427694681566914\n",
      "--------------------------------------------------------------\n",
      "Epoch 1: iteration 700, the loss is [ 0.15586896]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6410625408627116, auc for val: 0.6420902816091607\n",
      "--------------------------------------------------------------\n",
      "Epoch 1: iteration 800, the loss is [ 0.14181344]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6404412813030301, auc for val: 0.6426138267479928\n",
      "--------------------------------------------------------------\n",
      "Epoch 1: iteration 900, the loss is [ 0.14134169]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6379123115488352, auc for val: 0.6401644282617335\n",
      "--------------------------------------------------------------\n",
      "Epoch 1: iteration 1000, the loss is [ 0.14708138]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6412075343583719, auc for val: 0.6426803975012324\n",
      "--------------------------------------------------------------\n",
      "Epoch 1: iteration 1100, the loss is [ 0.17027242]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.641264350602584, auc for val: 0.6428473093630566\n",
      "--------------------------------------------------------------\n",
      "Epoch 2: iteration 0, the loss is [ 0.14957769]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6396692190060974, auc for val: 0.6407640896097142\n",
      "--------------------------------------------------------------\n",
      "Epoch 2: iteration 100, the loss is [ 0.18130285]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6412821039207282, auc for val: 0.6430232138333216\n",
      "--------------------------------------------------------------\n",
      "Epoch 2: iteration 200, the loss is [ 0.14788744]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6410628368786054, auc for val: 0.6426329261434043\n",
      "--------------------------------------------------------------\n",
      "Epoch 2: iteration 300, the loss is [ 0.14651465]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6384829557663495, auc for val: 0.6405840932198223\n",
      "--------------------------------------------------------------\n",
      "Epoch 2: iteration 400, the loss is [ 0.18210599]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6412101545772559, auc for val: 0.6426919705358372\n",
      "--------------------------------------------------------------\n",
      "Epoch 2: iteration 500, the loss is [ 0.11600333]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.641324712125395, auc for val: 0.6428327769684729\n",
      "--------------------------------------------------------------\n",
      "Epoch 2: iteration 600, the loss is [ 0.15605943]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6408193529152486, auc for val: 0.6426166828909254\n",
      "--------------------------------------------------------------\n",
      "Epoch 2: iteration 700, the loss is [ 0.17507739]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6410689891948211, auc for val: 0.6430221194498418\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: iteration 800, the loss is [ 0.16032268]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6412827487177711, auc for val: 0.6429308628552868\n",
      "--------------------------------------------------------------\n",
      "Epoch 2: iteration 900, the loss is [ 0.16457665]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6413210135341928, auc for val: 0.642952498844966\n",
      "--------------------------------------------------------------\n",
      "Epoch 2: iteration 1000, the loss is [ 0.16741574]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6410162308518993, auc for val: 0.6427903787991802\n",
      "--------------------------------------------------------------\n",
      "Epoch 2: iteration 1100, the loss is [ 0.13771659]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6412875080794777, auc for val: 0.6427155195757082\n",
      "--------------------------------------------------------------\n",
      "Epoch 3: iteration 0, the loss is [ 0.16815175]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6412599206921803, auc for val: 0.6426884258079503\n",
      "--------------------------------------------------------------\n",
      "Epoch 3: iteration 100, the loss is [ 0.14405228]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6401826042013489, auc for val: 0.6424956248485556\n",
      "--------------------------------------------------------------\n",
      "Epoch 3: iteration 200, the loss is [ 0.16110681]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6411922117783975, auc for val: 0.6426469339691624\n",
      "--------------------------------------------------------------\n",
      "Epoch 3: iteration 300, the loss is [ 0.17108068]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6409717949520307, auc for val: 0.6421377600366496\n",
      "--------------------------------------------------------------\n",
      "Epoch 3: iteration 400, the loss is [ 0.17260239]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6350243048563602, auc for val: 0.6385763420947064\n",
      "--------------------------------------------------------------\n",
      "Epoch 3: iteration 500, the loss is [ 0.13551673]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.639200478239967, auc for val: 0.6412524688421741\n",
      "--------------------------------------------------------------\n",
      "Epoch 3: iteration 600, the loss is [ 0.1418893]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.5970777697389873, auc for val: 0.6004651599215651\n",
      "--------------------------------------------------------------\n",
      "Epoch 3: iteration 700, the loss is [ 0.15236187]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6110362462249667, auc for val: 0.6134597152449816\n",
      "--------------------------------------------------------------\n",
      "Epoch 3: iteration 800, the loss is [ 0.18998492]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6323589271926583, auc for val: 0.6332296764596068\n",
      "--------------------------------------------------------------\n",
      "Epoch 3: iteration 900, the loss is [ 0.16271585]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6153198217782225, auc for val: 0.6159458420326334\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-1357:\n",
      "Process Process-1358:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLOWINGUP: Learning rate is 0.0055353144032044485. Weight decay is 0.020228773910792964. dropout is 0.5\n",
      " Val aus is 0.6159458420326334. Train auc is 0.6153198217782225\n",
      "WARNING: YOUR TRAINING IS BLOWING UP THIS TIME!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 343, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: iteration 0, the loss is [ 0.88721603]\n",
      "  acc for train: 0.9635541622144714, acc for val: 0.96327\n",
      "  auc for train: 0.6129201870987113, auc for val: 0.6098416438698622\n",
      "--------------------------------------------------------------\n",
      "Epoch 0: iteration 100, the loss is [ 0.20654897]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6410678220498842, auc for val: 0.6427860747897316\n",
      "--------------------------------------------------------------\n",
      "Epoch 0: iteration 200, the loss is [ 0.18505025]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6330309870743006, auc for val: 0.6340475471276239\n",
      "--------------------------------------------------------------\n",
      "Epoch 0: iteration 300, the loss is [ 0.16718414]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6411116265349107, auc for val: 0.6426830613493931\n",
      "--------------------------------------------------------------\n",
      "Epoch 0: iteration 400, the loss is [ 0.13806607]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6412364196584205, auc for val: 0.6428931631825111\n",
      "--------------------------------------------------------------\n",
      "Epoch 0: iteration 500, the loss is [ 0.15474124]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6393576302890778, auc for val: 0.6399000879915424\n",
      "--------------------------------------------------------------\n",
      "Epoch 0: iteration 600, the loss is [ 0.16872679]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6273787944532931, auc for val: 0.6289755067051365\n",
      "--------------------------------------------------------------\n",
      "Epoch 0: iteration 700, the loss is [ 0.14535865]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6328899924053495, auc for val: 0.633641774052857\n",
      "--------------------------------------------------------------\n",
      "Epoch 0: iteration 800, the loss is [ 0.16237962]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6395186357690921, auc for val: 0.640694746135539\n",
      "--------------------------------------------------------------\n",
      "Epoch 0: iteration 900, the loss is [ 0.16684674]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6385100317767459, auc for val: 0.6395706997730319\n",
      "--------------------------------------------------------------\n",
      "Epoch 0: iteration 1000, the loss is [ 0.15975299]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6401804214156723, auc for val: 0.6420997917167663\n",
      "--------------------------------------------------------------\n",
      "Epoch 0: iteration 1100, the loss is [ 0.19541648]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.5998844055362346, auc for val: 0.6038736611020528\n",
      "--------------------------------------------------------------\n",
      "Epoch 1: iteration 0, the loss is [ 0.0972138]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6054462235724996, auc for val: 0.6065422360379354\n",
      "--------------------------------------------------------------\n",
      "Epoch 1: iteration 100, the loss is [ 0.16573595]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6142012883493884, auc for val: 0.6148771610111186\n",
      "--------------------------------------------------------------\n",
      "Epoch 1: iteration 200, the loss is [ 0.21837141]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6156381985664733, auc for val: 0.6162217100916005\n",
      "--------------------------------------------------------------\n",
      "Epoch 1: iteration 300, the loss is [ 0.16750164]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.589596119404177, auc for val: 0.5934564029109821\n",
      "--------------------------------------------------------------\n",
      "Epoch 1: iteration 400, the loss is [ 0.1730945]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6245912304625777, auc for val: 0.6254593529729432\n",
      "--------------------------------------------------------------\n",
      "Epoch 1: iteration 500, the loss is [ 0.15924348]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6339800885364684, auc for val: 0.635971092937791\n",
      "--------------------------------------------------------------\n",
      "Epoch 1: iteration 600, the loss is [ 0.17095032]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.5650980198016213, auc for val: 0.5642953532429364\n",
      "--------------------------------------------------------------\n",
      "Epoch 1: iteration 700, the loss is [ 0.16562371]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6233688642949535, auc for val: 0.6235932566397497\n",
      "--------------------------------------------------------------\n",
      "Epoch 1: iteration 800, the loss is [ 0.08997198]\n",
      "  acc for train: 0.963552482140817, acc for val: 0.96329\n",
      "  auc for train: 0.6212915788678677, auc for val: 0.6229894779213635\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-1361:\n",
      "Traceback (most recent call last):\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLOWINGUP: Learning rate is 0.020576266888179925. Weight decay is 0.0038547135436206204. dropout is 0.5\n",
      " Val aus is 0.6229894779213635. Train auc is 0.6212915788678677\n",
      "WARNING: YOUR TRAINING IS BLOWING UP THIS TIME!\n",
      "This is round you consume 0:22:37.933934 time to run this model.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-001d756696b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'This is round you consume {} time to run this model.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoc\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'submission11.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   2329\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2330\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2331\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2333\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   2395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2396\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2397\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2398\u001b[0m         \u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[0;34m(self, key, value, broadcast)\u001b[0m\n\u001b[1;32m   2571\u001b[0m                     \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_convert_platform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2572\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2573\u001b[0;31m                     \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_asarray_tuplesafe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2574\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2575\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/common.py\u001b[0m in \u001b[0;36m_asarray_tuplesafe\u001b[0;34m(values, dtype)\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__array__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(i)\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnelement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_range\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:tornado.general:Uncaught exception, closing connection.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 421, in execute_request\n",
      "    self._abort_queues()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 637, in _abort_queues\n",
      "    self._abort_queue(stream)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 662, in _abort_queue\n",
      "    poller.poll(50)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/zmq/sugar/poll.py\", line 99, in poll\n",
      "    return zmq_poll(self.sockets, timeout=timeout)\n",
      "  File \"zmq/backend/cython/_poll.pyx\", line 116, in zmq.backend.cython._poll.zmq_poll (zmq/backend/cython/_poll.c:2036)\n",
      "  File \"zmq/backend/cython/checkrc.pxd\", line 12, in zmq.backend.cython.checkrc._check_rc (zmq/backend/cython/_poll.c:2418)\n",
      "KeyboardInterrupt\n",
      "ERROR:tornado.general:Uncaught exception, closing connection.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 421, in execute_request\n",
      "    self._abort_queues()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 637, in _abort_queues\n",
      "    self._abort_queue(stream)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 662, in _abort_queue\n",
      "    poller.poll(50)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/zmq/sugar/poll.py\", line 99, in poll\n",
      "    return zmq_poll(self.sockets, timeout=timeout)\n",
      "  File \"zmq/backend/cython/_poll.pyx\", line 116, in zmq.backend.cython._poll.zmq_poll (zmq/backend/cython/_poll.c:2036)\n",
      "  File \"zmq/backend/cython/checkrc.pxd\", line 12, in zmq.backend.cython.checkrc._check_rc (zmq/backend/cython/_poll.c:2418)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "from Stacker import Ensemble\n",
    "\n",
    "tic= datetime.now()\n",
    "stack_model = Ensemble(n_splits=5)\n",
    "y_pred= stack_model.fit_predict(X_train,y_train, X_test, META = 'nn')\n",
    "toc = datetime.now()\n",
    "print('This is round you consume {} time to run this model.'.format(toc-tic))\n",
    "\n",
    "result['target'] = y_pred\n",
    "result.to_csv('submission11.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate is 0.0006241303598307956. Weight decay is 0.0026706282114709093. dropout is 0.5\n",
      " Val aus is 0.6431840995163449. Train auc is 0.6409540445249023\n",
      "Learning rate is 0.0004130048057886514. Weight decay is 0.015377794660046963. dropout is 0.5\n",
      " Val aus is 0.6429664246626596. Train auc is 0.6408428527176809\n",
      "Learning rate is 0.025221516643856644. Weight decay is 0.015984258287073554. dropout is 0.5\n",
      " Val aus is 0.6429169441072538. Train auc is 0.6408618952542339\n",
      "Learning rate is 0.0055353144032044485. Weight decay is 0.020228773910792964. dropout is 0.5\n",
      " Val aus is 0.6428881663463115. Train auc is 0.6406036645498829\n",
      "Learning rate is 0.020576266888179925. Weight decay is 0.0038547135436206204. dropout is 0.5\n",
      " Val aus is 0.6428809043908161. Train auc is 0.6410302614713866\n",
      "Learning rate is 0.001341993635972462. Weight decay is 0.044122990686213025. dropout is 0.5\n",
      " Val aus is 0.6428361774752811. Train auc is 0.6406958356800573\n",
      "Learning rate is 0.0005067940145389166. Weight decay is 0.013264664506374056. dropout is 0.5\n",
      " Val aus is 0.642824201470015. Train auc is 0.6410139986384483\n",
      "Learning rate is 0.000571127228978929. Weight decay is 0.014334536037264735. dropout is 0.5\n",
      " Val aus is 0.6428158324056501. Train auc is 0.6409400958346113\n",
      "Learning rate is 0.0012978605744697971. Weight decay is 0.02952488177779784. dropout is 0.5\n",
      " Val aus is 0.6428117546253447. Train auc is 0.6409781632922112\n",
      "Learning rate is 0.0004759266035686457. Weight decay is 0.05802839526364841. dropout is 0.5\n",
      " Val aus is 0.6428066969900628. Train auc is 0.6410517268667467\n",
      "Learning rate is 0.06734258529604265. Weight decay is 0.03802998340125783. dropout is 0.5\n",
      " Val aus is 0.6427661397604226. Train auc is 0.640993897718102\n",
      "Learning rate is 0.0008070632879716268. Weight decay is 0.0032552678119929087. dropout is 0.5\n",
      " Val aus is 0.642713917590588. Train auc is 0.6409754883514415\n",
      "Learning rate is 0.0012286904544815118. Weight decay is 0.014664056969049483. dropout is 0.5\n",
      " Val aus is 0.642709373212675. Train auc is 0.6409845864987487\n",
      "Learning rate is 0.0978971530433665. Weight decay is 0.004453531922927502. dropout is 0.5\n",
      " Val aus is 0.6426688456756098. Train auc is 0.6409545365963347\n",
      "Learning rate is 0.03399081893537368. Weight decay is 0.05155575213670047. dropout is 0.5\n",
      " Val aus is 0.6426656699840126. Train auc is 0.6410155797318644\n",
      "Learning rate is 0.0377868078515243. Weight decay is 0.03327522927544063. dropout is 0.5\n",
      " Val aus is 0.6426220077593825. Train auc is 0.6407623851437504\n",
      "Learning rate is 0.00037979908793919835. Weight decay is 0.003412555085338429. dropout is 0.5\n",
      " Val aus is 0.6426205019216483. Train auc is 0.6408800331211791\n",
      "Learning rate is 0.0016195466494312544. Weight decay is 0.08957339121694476. dropout is 0.5\n",
      " Val aus is 0.6425936258994407. Train auc is 0.6409874550356751\n",
      "Learning rate is 0.0010842340084321331. Weight decay is 0.004542297055840149. dropout is 0.5\n",
      " Val aus is 0.6425477819775113. Train auc is 0.6408787507355868\n",
      "Learning rate is 0.0011389723006059798. Weight decay is 0.0021456358256778615. dropout is 0.5\n",
      " Val aus is 0.6425344924292827. Train auc is 0.6407432955162403\n",
      "Learning rate is 0.0007020441293177336. Weight decay is 0.050241154755061204. dropout is 0.5\n",
      " Val aus is 0.6425323163877119. Train auc is 0.6409795792765928\n",
      "Learning rate is 0.04012093434805108. Weight decay is 0.021727264678335766. dropout is 0.5\n",
      " Val aus is 0.6425224938011023. Train auc is 0.6404461450079674\n",
      "Learning rate is 0.07311886952792655. Weight decay is 0.005721143387397193. dropout is 0.5\n",
      " Val aus is 0.6425196334163734. Train auc is 0.6407986555871756\n",
      "Learning rate is 0.004969291167032587. Weight decay is 0.0013882711526632375. dropout is 0.5\n",
      " Val aus is 0.642514295822527. Train auc is 0.6408481778328651\n",
      "Learning rate is 0.0011204694813848122. Weight decay is 0.002569213361077419. dropout is 0.5\n",
      " Val aus is 0.6425088691509554. Train auc is 0.6407273580760013\n",
      "Learning rate is 0.0003122093294305291. Weight decay is 0.0033706967938083904. dropout is 0.5\n",
      " Val aus is 0.6425037945484878. Train auc is 0.640402409136293\n",
      "Learning rate is 0.019644118868499674. Weight decay is 0.01496094290536108. dropout is 0.5\n",
      " Val aus is 0.6425036064955126. Train auc is 0.6409068849662416\n",
      "Learning rate is 0.0010413858546388572. Weight decay is 0.058419356234414584. dropout is 0.5\n",
      " Val aus is 0.6424665374364736. Train auc is 0.6408968380233563\n",
      "Learning rate is 0.0007076144785983324. Weight decay is 0.005035906755622979. dropout is 0.5\n",
      " Val aus is 0.6424454104623666. Train auc is 0.6408645221157179\n",
      "Learning rate is 0.051616014024124655. Weight decay is 0.05132407747520662. dropout is 0.5\n",
      " Val aus is 0.642425243548186. Train auc is 0.6408818672267497\n",
      "Learning rate is 0.0061561096152121875. Weight decay is 0.015378275759637334. dropout is 0.5\n",
      " Val aus is 0.6423133435443177. Train auc is 0.6407296707070871\n",
      "Learning rate is 0.000311629825374842. Weight decay is 0.006512083213197469. dropout is 0.5\n",
      " Val aus is 0.64224811885448. Train auc is 0.6407448748655471\n",
      "Learning rate is 0.04899676595854559. Weight decay is 0.005037861757003686. dropout is 0.5\n",
      " Val aus is 0.6422363747340857. Train auc is 0.6401192934191844\n",
      "Learning rate is 0.00019604617578732056. Weight decay is 0.046554369085378695. dropout is 0.5\n",
      " Val aus is 0.6422309508903784. Train auc is 0.6407388639664998\n",
      "Learning rate is 0.0020247793774923425. Weight decay is 0.005669001057014794. dropout is 0.5\n",
      " Val aus is 0.6422120013717744. Train auc is 0.6404104682012856\n",
      "Learning rate is 0.0019195844619994903. Weight decay is 0.08557207985341482. dropout is 0.5\n",
      " Val aus is 0.6421329102493931. Train auc is 0.6405385193269009\n",
      "Learning rate is 0.008098800781390428. Weight decay is 0.0012551609484882885. dropout is 0.5\n",
      " Val aus is 0.6421254333762116. Train auc is 0.6400360411418584\n",
      "Learning rate is 0.0002533888071157335. Weight decay is 0.03816120739496742. dropout is 0.5\n",
      " Val aus is 0.641994443874457. Train auc is 0.6408586921971018\n",
      "Learning rate is 0.0010113095604303522. Weight decay is 0.007093482903096122. dropout is 0.5\n",
      " Val aus is 0.641834919808089. Train auc is 0.6405407166723471\n",
      "Learning rate is 0.027679479827521343. Weight decay is 0.021309127401443757. dropout is 0.5\n",
      " Val aus is 0.6417749620154906. Train auc is 0.6405076608553139\n",
      "Learning rate is 0.0005670518606959731. Weight decay is 0.0015157292423794877. dropout is 0.5\n",
      " Val aus is 0.6416369169923337. Train auc is 0.6397260031176629\n",
      "Learning rate is 0.000786467681182744. Weight decay is 0.019196855450328943. dropout is 0.5\n",
      " Val aus is 0.6408668004685738. Train auc is 0.6396172163211945\n",
      "Learning rate is 0.0001021782647080577. Weight decay is 0.019000151577823313. dropout is 0.5\n",
      " Val aus is 0.6408345755410625. Train auc is 0.6393622830884377\n",
      "Learning rate is 0.0008671429104670833. Weight decay is 0.004281851787174051. dropout is 0.5\n",
      " Val aus is 0.6407613762739284. Train auc is 0.6393455204518481\n",
      "Learning rate is 0.0016011655321867264. Weight decay is 0.0023166978347582716. dropout is 0.5\n",
      " Val aus is 0.6407502613533386. Train auc is 0.6389236916350851\n",
      "Learning rate is 0.00514730046500468. Weight decay is 0.013720568559086387. dropout is 0.5\n",
      " Val aus is 0.6404251856278697. Train auc is 0.6389630740349888\n",
      "Learning rate is 0.016046125235968923. Weight decay is 0.06631123116859995. dropout is 0.5\n",
      " Val aus is 0.6404213581135537. Train auc is 0.6386092861281055\n",
      "Learning rate is 0.00015731462414654926. Weight decay is 0.02465295490284764. dropout is 0.5\n",
      " Val aus is 0.6399004810646787. Train auc is 0.6379775914034671\n",
      "Learning rate is 0.001439047118952334. Weight decay is 0.003068816915060797. dropout is 0.5\n",
      " Val aus is 0.6396245988663901. Train auc is 0.6382068877378526\n",
      "Learning rate is 0.0035812500254717495. Weight decay is 0.08816662783236347. dropout is 0.5\n",
      " Val aus is 0.639518210370022. Train auc is 0.638396006125846\n",
      "Learning rate is 0.024003608207932514. Weight decay is 0.0037230694060965655. dropout is 0.5\n",
      " Val aus is 0.6390720511514045. Train auc is 0.6383969078886045\n",
      "Learning rate is 0.060109321304377. Weight decay is 0.007567516526163304. dropout is 0.5\n",
      " Val aus is 0.6386492486778864. Train auc is 0.6375780175404436\n",
      "Learning rate is 0.0004468288130009619. Weight decay is 0.01713080496282197. dropout is 0.5\n",
      " Val aus is 0.6385009370934265. Train auc is 0.6376620900724765\n",
      "Learning rate is 0.0002594216878652695. Weight decay is 0.0015662428348281324. dropout is 0.5\n",
      " Val aus is 0.637362448827993. Train auc is 0.6358247561553185\n",
      "Learning rate is 0.0006568066306425819. Weight decay is 0.0026801676924690836. dropout is 0.5\n",
      " Val aus is 0.6367654301191906. Train auc is 0.6357630011324199\n",
      "Learning rate is 0.008043449910137675. Weight decay is 0.0018105706506953104. dropout is 0.5\n",
      " Val aus is 0.6363943351442062. Train auc is 0.6354751794590067\n",
      "Learning rate is 0.0002524336032616717. Weight decay is 0.007494961458744887. dropout is 0.5\n",
      " Val aus is 0.6363568221104736. Train auc is 0.631233328313635\n",
      "Learning rate is 0.0049375195611817335. Weight decay is 0.0533959326205545. dropout is 0.5\n",
      " Val aus is 0.6342646408551977. Train auc is 0.631690655805361\n",
      "Learning rate is 0.0010990013098533078. Weight decay is 0.04246959081710752. dropout is 0.5\n",
      " Val aus is 0.634096406966818. Train auc is 0.6313291285287601\n",
      "Learning rate is 0.0047327366492346. Weight decay is 0.00491168540960169. dropout is 0.5\n",
      " Val aus is 0.633977184208372. Train auc is 0.6311786829206436\n",
      "Learning rate is 0.002902582403673544. Weight decay is 0.031328128443716824. dropout is 0.5\n",
      " Val aus is 0.6334032592532901. Train auc is 0.6328062831915646\n",
      "Learning rate is 0.0004608079115868425. Weight decay is 0.003932863804058899. dropout is 0.5\n",
      " Val aus is 0.6332695111699211. Train auc is 0.6329156599040097\n",
      "Learning rate is 0.005717292218781895. Weight decay is 0.05725979843051256. dropout is 0.5\n",
      " Val aus is 0.6327869658214906. Train auc is 0.6301551998634647\n",
      "Learning rate is 0.0001625724442380248. Weight decay is 0.012781889320801524. dropout is 0.5\n",
      " Val aus is 0.6302480838115857. Train auc is 0.6284595631592117\n",
      "Learning rate is 0.00026572263187676914. Weight decay is 0.0022712213779617803. dropout is 0.5\n",
      " Val aus is 0.6297061957310397. Train auc is 0.6267268170948547\n",
      "Learning rate is 0.0026364285485249877. Weight decay is 0.03724924456746956. dropout is 0.5\n",
      " Val aus is 0.6279387734804324. Train auc is 0.626654835195105\n",
      "Learning rate is 0.00015960871793062724. Weight decay is 0.001555208011305216. dropout is 0.5\n",
      " Val aus is 0.6270182852730775. Train auc is 0.6222383886291588\n",
      "Learning rate is 0.003517812040063327. Weight decay is 0.019480465268198358. dropout is 0.5\n",
      " Val aus is 0.6269028150905438. Train auc is 0.626401651536744\n",
      "Learning rate is 0.0006415627840618995. Weight decay is 0.001359774443538536. dropout is 0.5\n",
      " Val aus is 0.626379957122734. Train auc is 0.6256109631488243\n",
      "Learning rate is 0.0016494020613418597. Weight decay is 0.030223968119957003. dropout is 0.5\n",
      " Val aus is 0.624499428784139. Train auc is 0.6241854200005021\n",
      "Learning rate is 0.021377444228367577. Weight decay is 0.002649414768196066. dropout is 0.5\n",
      " Val aus is 0.6238027009944204. Train auc is 0.6217804347695675\n",
      "Learning rate is 0.0037907596965365527. Weight decay is 0.023034005504118698. dropout is 0.5\n",
      " Val aus is 0.6227852368369854. Train auc is 0.621871278225439\n",
      "Learning rate is 0.0027781798194518796. Weight decay is 0.027102593419491703. dropout is 0.5\n",
      " Val aus is 0.6202650752886771. Train auc is 0.6193625234197528\n",
      "Learning rate is 0.004614363261642551. Weight decay is 0.02570284810696751. dropout is 0.5\n",
      " Val aus is 0.6185460377960634. Train auc is 0.6176418643250492\n",
      "Learning rate is 0.002150057369245043. Weight decay is 0.025604068372570316. dropout is 0.5\n",
      " Val aus is 0.6176167492750395. Train auc is 0.6179265403225336\n",
      "Learning rate is 0.00017814055701279675. Weight decay is 0.04357194306673782. dropout is 0.5\n",
      " Val aus is 0.6156934098989655. Train auc is 0.6166157853352617\n",
      "Learning rate is 0.050110983142781444. Weight decay is 0.04663425238167551. dropout is 0.5\n",
      " Val aus is 0.614978465007494. Train auc is 0.6114204224230071\n",
      "Learning rate is 0.03133300608068672. Weight decay is 0.03588580653534281. dropout is 0.5\n",
      " Val aus is 0.6140252018530237. Train auc is 0.6119506942722521\n",
      "Learning rate is 0.006807085148645252. Weight decay is 0.005480638493554583. dropout is 0.5\n",
      " Val aus is 0.6120520340942364. Train auc is 0.6116148487757742\n",
      "Learning rate is 0.0667069145756801. Weight decay is 0.029055663603671674. dropout is 0.5\n",
      " Val aus is 0.6071672378778146. Train auc is 0.6047103559141077\n",
      "Learning rate is 0.010022860532118712. Weight decay is 0.01434244553883881. dropout is 0.5\n",
      " Val aus is 0.601773314388641. Train auc is 0.5999054134874345\n",
      "Learning rate is 0.012638407011355584. Weight decay is 0.07091808865416985. dropout is 0.5\n",
      " Val aus is 0.6006338901001276. Train auc is 0.6001330871023234\n",
      "Learning rate is 0.008575905633824366. Weight decay is 0.021717820977378587. dropout is 0.5\n",
      " Val aus is 0.5950762657890303. Train auc is 0.5949914578207403\n",
      "Learning rate is 0.042526809559462676. Weight decay is 0.012524707041264235. dropout is 0.5\n",
      " Val aus is 0.5583122074668064. Train auc is 0.5575824273190463\n",
      "Learning rate is 0.05424413517988623. Weight decay is 0.050749326997818325. dropout is 0.5\n",
      " Val aus is 0.5477349501281672. Train auc is 0.5443767847736506\n",
      "Learning rate is 0.0001393717627862959. Weight decay is 0.015715766228226194. dropout is 0.5\n",
      " Val aus is 0.5469218274442924. Train auc is 0.5527947217523408\n",
      "Learning rate is 0.00014625032672550782. Weight decay is 0.006384768787944084. dropout is 0.5\n",
      " Val aus is 0.532791526883803. Train auc is 0.5284162022362017\n",
      "Learning rate is 0.014942698363647252. Weight decay is 0.03853690757515586. dropout is 0.5\n",
      " Val aus is 0.5016712112378173. Train auc is 0.5041368167423969\n",
      "BLOWINGUP: Learning rate is 0.025319830533220636. Weight decay is 0.0022548018902921775. dropout is 0.5\n",
      " Val aus is 0.5. Train auc is 0.5\n",
      "Learning rate is 0.0586038859959561. Weight decay is 0.023707391204394322. dropout is 0.5\n",
      " Val aus is 0.4886278865668639. Train auc is 0.4896369002889054\n",
      "Learning rate is 0.00010868614521629454. Weight decay is 0.005475470634222938. dropout is 0.5\n",
      " Val aus is 0.47980748657784617. Train auc is 0.47764126005853963\n"
     ]
    }
   ],
   "source": [
    "#! ls -a\n",
    "# ! rm \n",
    "# X_train.describe()\n",
    "for i in sorted(stack_model.train_hist, key =lambda x:x[0], reverse=True):\n",
    "    print(stack_model.train_hist[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ps_ind_01</th>\n",
       "      <th>ps_ind_03</th>\n",
       "      <th>ps_ind_14</th>\n",
       "      <th>ps_ind_15</th>\n",
       "      <th>ps_reg_01</th>\n",
       "      <th>ps_reg_02</th>\n",
       "      <th>ps_reg_03</th>\n",
       "      <th>ps_car_11</th>\n",
       "      <th>ps_car_12</th>\n",
       "      <th>ps_car_13</th>\n",
       "      <th>...</th>\n",
       "      <th>ps_calc_16_bin_0</th>\n",
       "      <th>ps_calc_16_bin_1</th>\n",
       "      <th>ps_calc_17_bin_0</th>\n",
       "      <th>ps_calc_17_bin_1</th>\n",
       "      <th>ps_calc_18_bin_0</th>\n",
       "      <th>ps_calc_18_bin_1</th>\n",
       "      <th>ps_calc_19_bin_0</th>\n",
       "      <th>ps_calc_19_bin_1</th>\n",
       "      <th>ps_calc_20_bin_0</th>\n",
       "      <th>ps_calc_20_bin_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.957955</td>\n",
       "      <td>1.324746</td>\n",
       "      <td>-0.097621</td>\n",
       "      <td>1.325445</td>\n",
       "      <td>-0.385866</td>\n",
       "      <td>-0.344291</td>\n",
       "      <td>-9.076669e-01</td>\n",
       "      <td>-1.616955</td>\n",
       "      <td>-1.092966</td>\n",
       "      <td>-0.639875</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.058390</td>\n",
       "      <td>0.213594</td>\n",
       "      <td>-0.097621</td>\n",
       "      <td>-0.648589</td>\n",
       "      <td>1.004750</td>\n",
       "      <td>0.150435</td>\n",
       "      <td>-3.924898e-01</td>\n",
       "      <td>-1.616955</td>\n",
       "      <td>-1.092966</td>\n",
       "      <td>-0.921442</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.562477</td>\n",
       "      <td>-0.527174</td>\n",
       "      <td>-0.097621</td>\n",
       "      <td>0.761435</td>\n",
       "      <td>-0.733520</td>\n",
       "      <td>-1.086380</td>\n",
       "      <td>7.078733e-02</td>\n",
       "      <td>0.785475</td>\n",
       "      <td>0.343963</td>\n",
       "      <td>0.369450</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.957955</td>\n",
       "      <td>0.583978</td>\n",
       "      <td>-0.097621</td>\n",
       "      <td>-0.930594</td>\n",
       "      <td>-1.776482</td>\n",
       "      <td>-0.591654</td>\n",
       "      <td>-2.450736e-14</td>\n",
       "      <td>-0.415740</td>\n",
       "      <td>-0.099167</td>\n",
       "      <td>-0.717556</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.562477</td>\n",
       "      <td>0.954362</td>\n",
       "      <td>-0.097621</td>\n",
       "      <td>-0.930594</td>\n",
       "      <td>1.004750</td>\n",
       "      <td>-0.096928</td>\n",
       "      <td>-2.440197e-01</td>\n",
       "      <td>0.785475</td>\n",
       "      <td>-0.099167</td>\n",
       "      <td>-0.001560</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.957955</td>\n",
       "      <td>0.583978</td>\n",
       "      <td>-0.097621</td>\n",
       "      <td>0.761435</td>\n",
       "      <td>1.004750</td>\n",
       "      <td>0.150435</td>\n",
       "      <td>-4.290506e-01</td>\n",
       "      <td>-0.415740</td>\n",
       "      <td>-1.092966</td>\n",
       "      <td>-0.277586</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.957955</td>\n",
       "      <td>-0.527174</td>\n",
       "      <td>-0.097621</td>\n",
       "      <td>1.043440</td>\n",
       "      <td>-1.776482</td>\n",
       "      <td>-0.839017</td>\n",
       "      <td>-2.450736e-14</td>\n",
       "      <td>-0.415740</td>\n",
       "      <td>-1.092966</td>\n",
       "      <td>-0.394927</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.957955</td>\n",
       "      <td>-1.638327</td>\n",
       "      <td>-0.097621</td>\n",
       "      <td>-0.084579</td>\n",
       "      <td>1.004750</td>\n",
       "      <td>1.634614</td>\n",
       "      <td>8.057727e-01</td>\n",
       "      <td>0.785475</td>\n",
       "      <td>-1.092966</td>\n",
       "      <td>-1.009201</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.957955</td>\n",
       "      <td>0.954362</td>\n",
       "      <td>-0.097621</td>\n",
       "      <td>-0.366584</td>\n",
       "      <td>-0.733520</td>\n",
       "      <td>-1.086380</td>\n",
       "      <td>-9.490565e-01</td>\n",
       "      <td>-0.415740</td>\n",
       "      <td>1.153809</td>\n",
       "      <td>3.681776</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.453868</td>\n",
       "      <td>0.583978</td>\n",
       "      <td>-0.097621</td>\n",
       "      <td>-0.084579</td>\n",
       "      <td>1.004750</td>\n",
       "      <td>1.387251</td>\n",
       "      <td>-2.450736e-14</td>\n",
       "      <td>-0.415740</td>\n",
       "      <td>1.153809</td>\n",
       "      <td>1.015509</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 237 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ps_ind_01  ps_ind_03  ps_ind_14  ps_ind_15  ps_reg_01  ps_reg_02  \\\n",
       "0  -0.957955   1.324746  -0.097621   1.325445  -0.385866  -0.344291   \n",
       "1   1.058390   0.213594  -0.097621  -0.648589   1.004750   0.150435   \n",
       "2   1.562477  -0.527174  -0.097621   0.761435  -0.733520  -1.086380   \n",
       "3  -0.957955   0.583978  -0.097621  -0.930594  -1.776482  -0.591654   \n",
       "4   1.562477   0.954362  -0.097621  -0.930594   1.004750  -0.096928   \n",
       "5  -0.957955   0.583978  -0.097621   0.761435   1.004750   0.150435   \n",
       "6  -0.957955  -0.527174  -0.097621   1.043440  -1.776482  -0.839017   \n",
       "7  -0.957955  -1.638327  -0.097621  -0.084579   1.004750   1.634614   \n",
       "8  -0.957955   0.954362  -0.097621  -0.366584  -0.733520  -1.086380   \n",
       "9  -0.453868   0.583978  -0.097621  -0.084579   1.004750   1.387251   \n",
       "\n",
       "      ps_reg_03  ps_car_11  ps_car_12  ps_car_13        ...         \\\n",
       "0 -9.076669e-01  -1.616955  -1.092966  -0.639875        ...          \n",
       "1 -3.924898e-01  -1.616955  -1.092966  -0.921442        ...          \n",
       "2  7.078733e-02   0.785475   0.343963   0.369450        ...          \n",
       "3 -2.450736e-14  -0.415740  -0.099167  -0.717556        ...          \n",
       "4 -2.440197e-01   0.785475  -0.099167  -0.001560        ...          \n",
       "5 -4.290506e-01  -0.415740  -1.092966  -0.277586        ...          \n",
       "6 -2.450736e-14  -0.415740  -1.092966  -0.394927        ...          \n",
       "7  8.057727e-01   0.785475  -1.092966  -1.009201        ...          \n",
       "8 -9.490565e-01  -0.415740   1.153809   3.681776        ...          \n",
       "9 -2.450736e-14  -0.415740   1.153809   1.015509        ...          \n",
       "\n",
       "   ps_calc_16_bin_0  ps_calc_16_bin_1  ps_calc_17_bin_0  ps_calc_17_bin_1  \\\n",
       "0                 0                 1                 0                 1   \n",
       "1                 1                 0                 0                 1   \n",
       "2                 1                 0                 1                 0   \n",
       "3                 1                 0                 0                 1   \n",
       "4                 0                 1                 0                 1   \n",
       "5                 1                 0                 0                 1   \n",
       "6                 0                 1                 1                 0   \n",
       "7                 0                 1                 1                 0   \n",
       "8                 1                 0                 0                 1   \n",
       "9                 0                 1                 0                 1   \n",
       "\n",
       "   ps_calc_18_bin_0  ps_calc_18_bin_1  ps_calc_19_bin_0  ps_calc_19_bin_1  \\\n",
       "0                 1                 0                 1                 0   \n",
       "1                 0                 1                 1                 0   \n",
       "2                 1                 0                 1                 0   \n",
       "3                 1                 0                 1                 0   \n",
       "4                 1                 0                 1                 0   \n",
       "5                 1                 0                 0                 1   \n",
       "6                 1                 0                 1                 0   \n",
       "7                 1                 0                 1                 0   \n",
       "8                 1                 0                 1                 0   \n",
       "9                 1                 0                 1                 0   \n",
       "\n",
       "   ps_calc_20_bin_0  ps_calc_20_bin_1  \n",
       "0                 0                 1  \n",
       "1                 0                 1  \n",
       "2                 1                 0  \n",
       "3                 1                 0  \n",
       "4                 0                 1  \n",
       "5                 1                 0  \n",
       "6                 1                 0  \n",
       "7                 1                 0  \n",
       "8                 1                 0  \n",
       "9                 1                 0  \n",
       "\n",
       "[10 rows x 237 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#result.to_csv('submission04.csv', index=False)\n",
    "# train_data.shape\n",
    "# X_train.shape\n",
    "# np.bincount(y_train)\n",
    "# ! pwd\n",
    "miscellany\n",
    "X_test.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only include LB score in this chunk\n",
    "\n",
    "#### 10.28\n",
    "1. No oversample, No variable dropping: 2.75\n",
    "2. 0.2 SMOTE oversample, No variable dropping: 2.76\n",
    "3. Take fold outside(ensemble), only fold one, 0.2 SMOTE oversample, No variable dropping: 2.76 (no improvement)\n",
    "---\n",
    "#### 10.29\n",
    "4. 0.4 SMOTE oversample, No variable dropping: 2.7\n",
    "5. META: Logistic Version, 0.3: 2.56(Don't know what is wrong here)\n",
    "6. 0.1 SMOTE oversample, No variable dropping: 2.75\n",
    "7. 0.15 SMOTE oversample, add L2:0.034, No variable dropping: 2.75\n",
    "\n",
    "#### 10.30\n",
    "###### (correct the mistake in stacker, forget to take mean for S_test) \n",
    "8. 0.22 SMOTE oversample, no META: 0.278\n",
    "9. 0.20 SMOTE oversample, no META: 0.279 Stacker score: 0.94613\n",
    "10. 0.25 SMOTE oversample, no META: 0.279 Stacker score: ***\n",
    "11. 0.18 SMOTE oversample, no META: 0.2 Stacker score: 0.9\n",
    "###### (won't normalize binary and categorical anymore  \n",
    "12. 0.18 SMOTE oversample, no META: 0.2 Stacker score: 0.9\n",
    "\n",
    "###### () \n",
    "9. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only include offline test score(auc) in this chunk\n",
    "\n",
    "#### 10.29\n",
    "\n",
    "all based on rso66 with 0.1 test set splitted from train_data,ensemble: three lgb in one.\n",
    "0. As I rememberred, 0.2 SMOTE oversample, No variable dropping: 0.6424\n",
    "1. 0.4 SMOTE oversample, No variable dropping: 0.641637\n",
    "2. 0.6 SMOTE oversample, No variable dropping: 0.639938\n",
    "3. 0.1 SMOTE oversample, No variable dropping: 0.6431368\n",
    "4. 0.0 SMOTE oversample, No variable dropping: 0.4736901 #what's wrong with it here??\n",
    "5. 0.3 SMOTE oversample, No variable dropping: 0.641200\n",
    "6. META: Logistic Version: (0.1, 0.2, 0.3, 0.5, 0.7, 1)\n",
    "        (0.633452, 0.632539, 0.645008, 0.628007, 0.627307, 0.625253)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_util\n",
    "import data_preprocess\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "import numpy as np \n",
    "from lgb_stacker import lgb_ensemble\n",
    "from Stacker import Ensemble\n",
    "from sklearn import metrics\n",
    "#from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "rso = np.random.RandomState(66)\n",
    "\n",
    "result = []\n",
    "for os in [None]:\n",
    "    train_data = data_util.load_train_data()\n",
    "    test_data= data_util.load_test_data()\n",
    "    \n",
    "    lv1_pre = data_preprocess.preprocess_cell()\n",
    "    \n",
    "    X_train = train_data.drop(['id','target'],axis=1)\n",
    "    y_train = train_data['target']\n",
    "    \n",
    "    X_train, X_train_test, y_train, y_train_test = train_test_split(X_train,y_train,test_size =0.1 ,random_state=rso)\n",
    "\n",
    "    X_train_af, y_train_af, col = lv1_pre.process(X_train, y=y_train, oversample=os, rso =rso)\n",
    "    X_train_test_af = lv1_pre.process(X_train_test,test=True,rso =rso)\n",
    "    \n",
    "    print(type(X_train_af), type(y_train_af), type(X_train_test_af))\n",
    "    stack_model = Ensemble(n_splits=5)\n",
    "    y_pred= stack_model.fit_predict(X_train_af, y_train_af, X_train_test_af)\n",
    "\n",
    "    result.append(metrics.roc_auc_score(y_train_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.64313174736945844, 0.59310226899327134]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "metrics.roc_auc_score(y_train_test,y_pred)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# change to the standard lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Stacker import Ensemble\n",
    "\n",
    "tic= datetime.now()\n",
    "stack_model = Ensemble(n_splits=5)\n",
    "y_pred= stack_model.fit_predict(X_train,y_train, X_test)\n",
    "toc = datetime.now()\n",
    "print('This is round you consume {} time to run this model.'.format(toc-tic))\n",
    "\n",
    "result['target'] = y_pred\n",
    "result.to_csv('submission08.csv', index=False)\n",
    "stack_model.miscellany"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
