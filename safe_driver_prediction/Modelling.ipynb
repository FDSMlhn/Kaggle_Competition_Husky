{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.2 (default, Aug 18 2017, 17:48:00) \n",
      "[GCC 5.4.0 20160609]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mThe directory '/home/FDSM_lhn/.cache/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "\u001b[33mThe directory '/home/FDSM_lhn/.cache/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "Collecting scipy\n",
      "  Downloading scipy-0.19.1-cp35-cp35m-manylinux1_x86_64.whl (47.9MB)\n",
      "\u001b[K    100% |████████████████████████████████| 47.9MB 33kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.8.2 in /home/FDSM_lhn/.local/lib/python3.5/site-packages (from scipy)\n",
      "Installing collected packages: scipy\n",
      "Successfully installed scipy-0.19.1\n"
     ]
    }
   ],
   "source": [
    "#! pip3 install pandas\n",
    "! sudo pip3 install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train = pd.read_csv('datasets/train.csv')\n",
    "\n",
    "# # train.head\n",
    "# train.iloc[:10,:10]\n",
    "# N, _ = train.shape\n",
    "\n",
    "# for i in range(3):\n",
    "#     file = \"datasets/train_batch_{}\".format(i)\n",
    "#     with open(file,'wb') as f:\n",
    "#         pickle.dump(train.iloc[int(2e5)*i:min(N+1,int(2e5) * (i+1)),:] ,file = f)\n",
    "    \n",
    "# test = pd.read_csv('datasets/test.csv')\n",
    "# test.shape\n",
    "\n",
    "# N, _ = test.shape\n",
    "\n",
    "# for i in range(5):\n",
    "#     file = \"datasets/test_batch_{}\".format(i)\n",
    "#     with open(file,'wb') as f:\n",
    "#         pickle.dump(test.iloc[int(2e5)*i:min(N+1,int(2e5) * (i+1)),:] ,file = f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/model_selection/_split.py:2010: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import data_util\n",
    "import data_preprocess\n",
    "import datetime\n",
    "\n",
    "tic= datetime.datetime.now()\n",
    "train_data = data_util.load_train_data()\n",
    "test_data= data_util.load_test_data()\n",
    "\n",
    "naive_pre = data_preprocess.naive_preprocess()\n",
    "train_data = naive_pre.dtype_change(train_data)\n",
    "#train_prop is specified\n",
    "X_train, X_val, y_train, y_val = data_util.split_train(train_data,prop=0.75)\n",
    "\n",
    "X_train = naive_pre.scale(X_train)\n",
    "X_val = naive_pre.scale(X_val,test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(446409, 235) (148803, 235)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape ,X_val.shape)\n",
    "# dev_train.dtypes\n",
    "# dev_train.head\n",
    "# train_data.isnull().any()\n",
    "import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100, 100, 100), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=<mtrand.RandomState object at 0x7fb955bcc480>,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "tic= datetime.datetime.now()\n",
    "hidden_layer_size = [(300,200,100)]\n",
    "learning_rate = [0.001]\n",
    "reg = []\n",
    "momentum =[0.9]\n",
    "rso= np.random.RandomState(66)\n",
    "\n",
    "#for now I don't mess up with inits\n",
    "mlp = MLPClassifier(hidden_layer_sizes= hidden_layer_size[0],random_state=rso,alpha= 0.001)\n",
    "mlp.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The val score it have: 0.5953008753764144 \n",
      "The time it cost: -1 day, 23:57:44.336906\n",
      "The test score it have: 0.7112570696063083 \n",
      "The time it cost: 0:29:13.188801\n"
     ]
    }
   ],
   "source": [
    "#check the performance\n",
    "from sklearn.metrics import classification_report,confusion_matrix,roc_auc_score\n",
    "test_predictions = mlp.predict_proba(X_val)\n",
    "auc_score = roc_auc_score(y_val,test_predictions[:,1])\n",
    "\n",
    "print('The val score it have: {} '.format(auc_score))\n",
    "print(\"The time it cost: {}\".format(toc-tic))\n",
    "\n",
    "test_predictions = mlp.predict_proba(X_train)\n",
    "auc_score = roc_auc_score(y_train,test_predictions[:,1])\n",
    "toc = datetime.datetime.now()\n",
    "print('The test score it have: {} '.format(auc_score))\n",
    "print(\"The time it cost: {}\".format(toc-tic))\n",
    "\n",
    "#classification_report(y_train,test_predictions[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.62780916443936541"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_val,test_predictions[:,1])\n",
    "# sum(test_predictions)\n",
    "# sum(dev_y)\n",
    "# test_predictions\n",
    "# sum(test_predictions[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 30)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_layer_size[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try Different Package for NN"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#! sudo pip3 install scikit-neuralnetwork\n",
    "!sudo pip3 install scikit-neuralnetwork --upgrade\n",
    "!sudo pip3 install Theano --upgrade"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_data = data_util.load_train_data()\n",
    "test_data= data_util.load_test_data()\n",
    "#train_prop is specified\n",
    "X_train, X_val, y_train, y_val = data_util.split_train(train_data,prop=0.75)\n",
    "train_mean = X_train.mean()\n",
    "X_train.fillna(train_mean)\n",
    "X_val.fillna(train_mean)\n",
    "\n",
    "weight = np.zeros_like(y_train)\n",
    "weight[y_train==1]=100\n",
    "weight[y_train==0]=1\n",
    "\n",
    "from sknn.mlp import Classifier, Layer\n",
    "\n",
    "nn= Classifier(\n",
    "    layers= [\n",
    "        Layer('Rectifier', units= 100),\n",
    "        Layer('Rectifier', units= 80),\n",
    "        Layer('Rectifier', units= 50),\n",
    "        Layer('Softmax')],\n",
    "    learning_rate=0.000000,\n",
    "    batch_size = 2000,\n",
    "    n_iter= 20,\n",
    "    regularize= 'L2'\n",
    ")\n",
    "\n",
    "nn.fit(X_train, y_train)\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "pipeline = Pipeline([\n",
    "        ('min/max scaler', MinMaxScaler(feature_range=(-1, 1.0))),\n",
    "        ('neural network', nn)])\n",
    "pipeline.fit(X_train, y_train,weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(X_train.isnull().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
