{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mThe directory '/home/FDSM_lhn/.cache/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "\u001b[33mThe directory '/home/FDSM_lhn/.cache/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "Collecting imblearn\n",
      "  Downloading imblearn-0.0-py2.py3-none-any.whl\n",
      "Collecting imbalanced-learn (from imblearn)\n",
      "  Downloading imbalanced_learn-0.3.1-py3-none-any.whl (144kB)\n",
      "\u001b[K    100% |████████████████████████████████| 153kB 3.4MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/FDSM_lhn/.local/lib/python3.5/site-packages (from imbalanced-learn->imblearn)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.5/dist-packages (from imbalanced-learn->imblearn)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.5/dist-packages (from imbalanced-learn->imblearn)\n",
      "Installing collected packages: imbalanced-learn, imblearn\n",
      "Successfully installed imbalanced-learn-0.3.1 imblearn-0.0\n"
     ]
    }
   ],
   "source": [
    "#! pip3 install pandas\n",
    "# ! sudo pip3 install scipy --upgrade\n",
    "# ! sudo pip3 install numpy --upgrade\n",
    "# ! sudo pip3 install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import data_vis\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train = pd.read_csv('datasets/train.csv')\n",
    "\n",
    "# # train.head\n",
    "# train.iloc[:10,:10]\n",
    "# N, _ = train.shape\n",
    "\n",
    "# for i in range(3):\n",
    "#     file = \"datasets/train_batch_{}\".format(i)\n",
    "#     with open(file,'wb') as f:\n",
    "#         pickle.dump(train.iloc[int(2e5)*i:min(N+1,int(2e5) * (i+1)),:] ,file = f)\n",
    "    \n",
    "# test = pd.read_csv('datasets/test.csv')\n",
    "# test.shape\n",
    "\n",
    "# N, _ = test.shape\n",
    "\n",
    "# for i in range(5):\n",
    "#     file = \"datasets/test_batch_{}\".format(i)\n",
    "#     with open(file,'wb') as f:\n",
    "#         pickle.dump(test.iloc[int(2e5)*i:min(N+1,int(2e5) * (i+1)),:] ,file = f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-ac2e691f4de5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mdata_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_test_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m# dev_train.dtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# dev_train.head\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "import data_util\n",
    "\n",
    "train_data = data_util.load_train_data()\n",
    "test_data= data_util.load_test_data()\n",
    "\n",
    "print(X_train.shape ,X_val.shape)\n",
    "# dev_train.dtypes\n",
    "# dev_train.head\n",
    "# train_data.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                False\n",
       "ps_ind_01         False\n",
       "ps_ind_02_cat      True\n",
       "ps_ind_03         False\n",
       "ps_ind_04_cat      True\n",
       "ps_ind_05_cat      True\n",
       "ps_ind_06_bin     False\n",
       "ps_ind_07_bin     False\n",
       "ps_ind_08_bin     False\n",
       "ps_ind_09_bin     False\n",
       "ps_ind_10_bin     False\n",
       "ps_ind_11_bin     False\n",
       "ps_ind_12_bin     False\n",
       "ps_ind_13_bin     False\n",
       "ps_ind_14         False\n",
       "ps_ind_15         False\n",
       "ps_ind_16_bin     False\n",
       "ps_ind_17_bin     False\n",
       "ps_ind_18_bin     False\n",
       "ps_reg_01         False\n",
       "ps_reg_02         False\n",
       "ps_reg_03          True\n",
       "ps_car_01_cat      True\n",
       "ps_car_02_cat      True\n",
       "ps_car_03_cat      True\n",
       "ps_car_04_cat     False\n",
       "ps_car_05_cat      True\n",
       "ps_car_06_cat     False\n",
       "ps_car_07_cat      True\n",
       "ps_car_08_cat     False\n",
       "ps_car_09_cat      True\n",
       "ps_car_10_cat     False\n",
       "ps_car_11_cat     False\n",
       "ps_car_11          True\n",
       "ps_car_12         False\n",
       "ps_car_13         False\n",
       "ps_car_14          True\n",
       "ps_car_15         False\n",
       "ps_calc_01        False\n",
       "ps_calc_02        False\n",
       "ps_calc_03        False\n",
       "ps_calc_04        False\n",
       "ps_calc_05        False\n",
       "ps_calc_06        False\n",
       "ps_calc_07        False\n",
       "ps_calc_08        False\n",
       "ps_calc_09        False\n",
       "ps_calc_10        False\n",
       "ps_calc_11        False\n",
       "ps_calc_12        False\n",
       "ps_calc_13        False\n",
       "ps_calc_14        False\n",
       "ps_calc_15_bin    False\n",
       "ps_calc_16_bin    False\n",
       "ps_calc_17_bin    False\n",
       "ps_calc_18_bin    False\n",
       "ps_calc_19_bin    False\n",
       "ps_calc_20_bin    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.isnull().any()\n",
    "test_data[test_data==-1] = None\n",
    "test_data.isnull().any()\n",
    "#train_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  7.43803558e+05,   3.64475179e-02,   1.90037835e+00,\n",
       "         4.42331808e+00,   1.24510259e-02,   7.29992171e+00,\n",
       "         6.10991378e-01,   4.39184358e-01,   8.94047327e-01,\n",
       "         2.34609976e+00,   3.79947134e-01,   8.13264676e-01,\n",
       "         3.74690639e-01,   3.06589944e+00,   4.49756389e-01,\n",
       "         4.49589222e-01,   4.49848793e-01,   2.37208087e+00,\n",
       "         1.88588604e+00,   7.68944511e+00,   3.00582314e+00,\n",
       "         9.22590438e+00,   2.33903382e+00,   8.43359005e+00,\n",
       "         5.44138223e+00,   1.44191817e+00,   2.87228752e+00,\n",
       "         7.53902643e+00])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test_data.ps_car_03_cat\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# pd.get_dummies(test_data.ps_car_03_cat, dummy_na=True)\n",
    "# OneHotEncoder().fit_transform( X= test_data.ps_car_03_cat)\n",
    "train_data[train_data==-1] = np.nan\n",
    "train_data.isnull().any()\n",
    "\n",
    "train_data.head(100)\n",
    "train_data[[i for i in train_data.columns if 'bin' not in i and 'cat' not in i]].mean(axis=0, ).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try Different Package for NN"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#! sudo pip3 install scikit-neuralnetwork\n",
    "!sudo pip3 install scikit-neuralnetwork --upgrade\n",
    "!sudo pip3 install Theano --upgrade"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_data = data_util.load_train_data()\n",
    "test_data= data_util.load_test_data()\n",
    "#train_prop is specified\n",
    "X_train, X_val, y_train, y_val = data_util.split_train(train_data,prop=0.75)\n",
    "train_mean = X_train.mean()\n",
    "X_train.fillna(train_mean)\n",
    "X_val.fillna(train_mean)\n",
    "\n",
    "weight = np.zeros_like(y_train)\n",
    "weight[y_train==1]=100\n",
    "weight[y_train==0]=1\n",
    "\n",
    "from sknn.mlp import Classifier, Layer\n",
    "\n",
    "nn= Classifier(\n",
    "    layers= [\n",
    "        Layer('Rectifier', units= 100),\n",
    "        Layer('Rectifier', units= 80),\n",
    "        Layer('Rectifier', units= 50),\n",
    "        Layer('Softmax')],\n",
    "    learning_rate=0.000000,\n",
    "    batch_size = 2000,\n",
    "    n_iter= 20,\n",
    "    regularize= 'L2'\n",
    ")\n",
    "\n",
    "nn.fit(X_train, y_train)\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "pipeline = Pipeline([\n",
    "        ('min/max scaler', MinMaxScaler(feature_range=(-1, 1.0))),\n",
    "        ('neural network', nn)])\n",
    "pipeline.fit(X_train, y_train,weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: iteration 0, the loss is Variable containing:\n",
      " 0.7110\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 0: iteration 10, the loss is Variable containing:\n",
      " 0.1808\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 0: iteration 20, the loss is Variable containing:\n",
      " 0.1641\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 0: iteration 30, the loss is Variable containing:\n",
      " 0.1527\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 0: iteration 40, the loss is Variable containing:\n",
      " 0.1607\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 0: iteration 50, the loss is Variable containing:\n",
      " 0.1577\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 0: iteration 60, the loss is Variable containing:\n",
      " 0.1616\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 0: iteration 70, the loss is Variable containing:\n",
      " 0.1566\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 0: iteration 80, the loss is Variable containing:\n",
      " 0.1624\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 0: iteration 90, the loss is Variable containing:\n",
      " 0.1706\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 0: iteration 100, the loss is Variable containing:\n",
      " 0.1526\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 0: iteration 110, the loss is Variable containing:\n",
      " 0.1547\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 1: iteration 0, the loss is Variable containing:\n",
      " 0.1544\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 1: iteration 10, the loss is Variable containing:\n",
      " 0.1592\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 1: iteration 20, the loss is Variable containing:\n",
      " 0.1562\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 1: iteration 30, the loss is Variable containing:\n",
      " 0.1551\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 1: iteration 40, the loss is Variable containing:\n",
      " 0.1581\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 1: iteration 50, the loss is Variable containing:\n",
      " 0.1578\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 1: iteration 60, the loss is Variable containing:\n",
      " 0.1534\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 1: iteration 70, the loss is Variable containing:\n",
      " 0.1326\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 1: iteration 80, the loss is Variable containing:\n",
      " 0.1657\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 1: iteration 90, the loss is Variable containing:\n",
      " 0.1502\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 1: iteration 100, the loss is Variable containing:\n",
      " 0.1504\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 1: iteration 110, the loss is Variable containing:\n",
      " 0.1466\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 2: iteration 0, the loss is Variable containing:\n",
      " 0.1595\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 2: iteration 10, the loss is Variable containing:\n",
      " 0.1350\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 2: iteration 20, the loss is Variable containing:\n",
      " 0.1517\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 2: iteration 30, the loss is Variable containing:\n",
      " 0.1556\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 2: iteration 40, the loss is Variable containing:\n",
      " 0.1521\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 2: iteration 50, the loss is Variable containing:\n",
      " 0.1583\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 2: iteration 60, the loss is Variable containing:\n",
      " 0.1444\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 2: iteration 70, the loss is Variable containing:\n",
      " 0.1473\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 2: iteration 80, the loss is Variable containing:\n",
      " 0.1472\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 2: iteration 90, the loss is Variable containing:\n",
      " 0.1435\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 2: iteration 100, the loss is Variable containing:\n",
      " 0.1655\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 2: iteration 110, the loss is Variable containing:\n",
      " 0.1572\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 3: iteration 0, the loss is Variable containing:\n",
      " 0.1642\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 3: iteration 10, the loss is Variable containing:\n",
      " 0.1584\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 3: iteration 20, the loss is Variable containing:\n",
      " 0.1716\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 3: iteration 30, the loss is Variable containing:\n",
      " 0.1544\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 3: iteration 40, the loss is Variable containing:\n",
      " 0.1529\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 3: iteration 50, the loss is Variable containing:\n",
      " 0.1534\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 3: iteration 60, the loss is Variable containing:\n",
      " 0.1587\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 3: iteration 70, the loss is Variable containing:\n",
      " 0.1458\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 3: iteration 80, the loss is Variable containing:\n",
      " 0.1458\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 3: iteration 90, the loss is Variable containing:\n",
      " 0.1574\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 3: iteration 100, the loss is Variable containing:\n",
      " 0.1595\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 3: iteration 110, the loss is Variable containing:\n",
      " 0.1538\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 4: iteration 0, the loss is Variable containing:\n",
      " 0.1407\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 4: iteration 10, the loss is Variable containing:\n",
      " 0.1642\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 4: iteration 20, the loss is Variable containing:\n",
      " 0.1550\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 4: iteration 30, the loss is Variable containing:\n",
      " 0.1563\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 4: iteration 40, the loss is Variable containing:\n",
      " 0.1673\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 4: iteration 50, the loss is Variable containing:\n",
      " 0.1601\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 4: iteration 60, the loss is Variable containing:\n",
      " 0.1530\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 4: iteration 70, the loss is Variable containing:\n",
      " 0.1541\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 4: iteration 80, the loss is Variable containing:\n",
      " 0.1415\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 4: iteration 90, the loss is Variable containing:\n",
      " 0.1515\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 4: iteration 100, the loss is Variable containing:\n",
      " 0.1457\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 4: iteration 110, the loss is Variable containing:\n",
      " 0.1627\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.utils.data as Data\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "input_size = 235\n",
    "hidden_size = [300,250,200]\n",
    "num_classes = 2\n",
    "num_epochs = 5\n",
    "batch_size = 5000\n",
    "learning_rate = 0.0005\n",
    "weight_decay = 1e-5\n",
    "\n",
    "# X_dev_tensor = torch.from_numpy(X_dev.values)\n",
    "# y_dev_tensor = torch.from_numpy(y_dev.values)\n",
    "\n",
    "# X_dev_tensor= X_dev_tensor.float()\n",
    "# y_dev_tensor =y_dev_tensor.type(torch.LongTensor)\n",
    "# #print(X_train_tensor)\n",
    "# X_dev_Variable = Variable(X_dev_tensor)\n",
    "# y_dev_Variable = Variable(y_dev_tensor)\n",
    "\n",
    "X_train_tensor = torch.from_numpy(X_train.values)\n",
    "y_train_tensor = torch.from_numpy(y_train.values)\n",
    "\n",
    "X_train_tensor= X_train_tensor.float()\n",
    "y_train_tensor =y_train_tensor.type(torch.LongTensor)\n",
    "#print(X_train_tensor)\n",
    "X_train_Variable = Variable(X_train_tensor)\n",
    "y_train_Variable = Variable(y_train_tensor)\n",
    "\n",
    "\n",
    "train_set = Data.TensorDataset(data_tensor=X_train_tensor, target_tensor=y_train_tensor )\n",
    "\n",
    "\n",
    "\n",
    "train_loader = Data.DataLoader(\n",
    "    dataset=train_set,      \n",
    "    batch_size=batch_size,      # mini batch size\n",
    "    shuffle=True,               # random shuffle for training\n",
    "    num_workers=2)              # subprocesses for loading data\n",
    "\n",
    "\n",
    "# train_loader = Data.DataLoader(dataset=train_dataset, \n",
    "#                                            batch_size=batch_size, \n",
    "#                                            shuffle=True)\n",
    "\n",
    "# test_loader = Data.DataLoader(dataset=test_dataset, \n",
    "#                                           batch_size=batch_size, \n",
    "#                                           shuffle=False)\n",
    "    \n",
    "class My_Net(nn.Module):\n",
    "    def __init__(self,input_size, hidden_size, num_classes):\n",
    "        super(My_Net,self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size,hidden_size[0])\n",
    "        self.fc2 = nn.Linear(hidden_size[0],hidden_size[1])\n",
    "        self.fc3 = nn.Linear(hidden_size[1],hidden_size[2])\n",
    "        self.fc4 = nn.Linear(hidden_size[2], num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc4(out)\n",
    "        return out\n",
    "        \n",
    "net = My_Net(input_size, hidden_size,num_classes)\n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate,weight_decay=weight_decay)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i,  (batch_x, batch_y) in enumerate(train_loader):\n",
    "#         print(batch_x)\n",
    "        x,  y = Variable(batch_x), Variable(batch_y)\n",
    "        out = net(x)\n",
    "        loss = criterion(out, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 10 ==0:\n",
    "            print('epoch {}: iteration {}, the loss is {}'.format(epoch,i ,loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: iteration 0, the loss is Variable containing:\n",
      " 0.7395\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 0: iteration 10, the loss is Variable containing:\n",
      " 0.2306\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 0: iteration 20, the loss is Variable containing:\n",
      " 0.1929\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 0: iteration 30, the loss is Variable containing:\n",
      " 0.1489\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 0: iteration 40, the loss is Variable containing:\n",
      " 0.1502\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 0: iteration 50, the loss is Variable containing:\n",
      " 0.1642\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 0: iteration 60, the loss is Variable containing:\n",
      " 0.1539\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 0: iteration 70, the loss is Variable containing:\n",
      " 0.1650\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 0: iteration 80, the loss is Variable containing:\n",
      " 0.1600\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 1: iteration 0, the loss is Variable containing:\n",
      " 0.1538\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 1: iteration 10, the loss is Variable containing:\n",
      " 0.1595\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 1: iteration 20, the loss is Variable containing:\n",
      " 0.1613\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 1: iteration 30, the loss is Variable containing:\n",
      " 0.1547\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 1: iteration 40, the loss is Variable containing:\n",
      " 0.1740\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 1: iteration 50, the loss is Variable containing:\n",
      " 0.1568\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 1: iteration 60, the loss is Variable containing:\n",
      " 0.1484\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 1: iteration 70, the loss is Variable containing:\n",
      " 0.1547\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 1: iteration 80, the loss is Variable containing:\n",
      " 0.1507\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 2: iteration 0, the loss is Variable containing:\n",
      " 0.1461\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 2: iteration 10, the loss is Variable containing:\n",
      " 0.1452\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 2: iteration 20, the loss is Variable containing:\n",
      " 0.1532\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 2: iteration 30, the loss is Variable containing:\n",
      " 0.1564\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 2: iteration 40, the loss is Variable containing:\n",
      " 0.1495\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 2: iteration 50, the loss is Variable containing:\n",
      " 0.1527\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 2: iteration 60, the loss is Variable containing:\n",
      " 0.1593\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 2: iteration 70, the loss is Variable containing:\n",
      " 0.1564\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 2: iteration 80, the loss is Variable containing:\n",
      " 0.1441\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 3: iteration 0, the loss is Variable containing:\n",
      " 0.1665\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 3: iteration 10, the loss is Variable containing:\n",
      " 0.1506\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 3: iteration 20, the loss is Variable containing:\n",
      " 0.1509\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 3: iteration 30, the loss is Variable containing:\n",
      " 0.1462\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 3: iteration 40, the loss is Variable containing:\n",
      " 0.1593\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 3: iteration 50, the loss is Variable containing:\n",
      " 0.1637\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 3: iteration 60, the loss is Variable containing:\n",
      " 0.1407\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 3: iteration 70, the loss is Variable containing:\n",
      " 0.1329\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 3: iteration 80, the loss is Variable containing:\n",
      " 0.1531\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 4: iteration 0, the loss is Variable containing:\n",
      " 0.1551\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 4: iteration 10, the loss is Variable containing:\n",
      " 0.1419\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 4: iteration 20, the loss is Variable containing:\n",
      " 0.1571\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 4: iteration 30, the loss is Variable containing:\n",
      " 0.1558\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 4: iteration 40, the loss is Variable containing:\n",
      " 0.1564\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 4: iteration 50, the loss is Variable containing:\n",
      " 0.1630\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 4: iteration 60, the loss is Variable containing:\n",
      " 0.1532\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 4: iteration 70, the loss is Variable containing:\n",
      " 0.1442\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 4: iteration 80, the loss is Variable containing:\n",
      " 0.1502\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 5: iteration 0, the loss is Variable containing:\n",
      " 0.1598\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 5: iteration 10, the loss is Variable containing:\n",
      " 0.1626\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 5: iteration 20, the loss is Variable containing:\n",
      " 0.1501\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 5: iteration 30, the loss is Variable containing:\n",
      " 0.1433\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 5: iteration 40, the loss is Variable containing:\n",
      " 0.1560\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 5: iteration 50, the loss is Variable containing:\n",
      " 0.1481\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 5: iteration 60, the loss is Variable containing:\n",
      " 0.1598\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 5: iteration 70, the loss is Variable containing:\n",
      " 0.1604\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 5: iteration 80, the loss is Variable containing:\n",
      " 0.1606\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 6: iteration 0, the loss is Variable containing:\n",
      " 0.1449\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 6: iteration 10, the loss is Variable containing:\n",
      " 0.1378\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 6: iteration 20, the loss is Variable containing:\n",
      " 0.1494\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 6: iteration 30, the loss is Variable containing:\n",
      " 0.1395\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 6: iteration 40, the loss is Variable containing:\n",
      " 0.1677\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 6: iteration 50, the loss is Variable containing:\n",
      " 0.1412\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 6: iteration 60, the loss is Variable containing:\n",
      " 0.1594\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 6: iteration 70, the loss is Variable containing:\n",
      " 0.1583\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 6: iteration 80, the loss is Variable containing:\n",
      " 0.1316\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 7: iteration 0, the loss is Variable containing:\n",
      " 0.1387\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 7: iteration 10, the loss is Variable containing:\n",
      " 0.1489\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 7: iteration 20, the loss is Variable containing:\n",
      " 0.1567\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 7: iteration 30, the loss is Variable containing:\n",
      " 0.1621\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 7: iteration 40, the loss is Variable containing:\n",
      " 0.1537\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 7: iteration 50, the loss is Variable containing:\n",
      " 0.1644\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 7: iteration 60, the loss is Variable containing:\n",
      " 0.1528\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 7: iteration 70, the loss is Variable containing:\n",
      " 0.1734\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 7: iteration 80, the loss is Variable containing:\n",
      " 0.1447\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 8: iteration 0, the loss is Variable containing:\n",
      " 0.1554\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 8: iteration 10, the loss is Variable containing:\n",
      " 0.1403\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 8: iteration 20, the loss is Variable containing:\n",
      " 0.1585\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 8: iteration 30, the loss is Variable containing:\n",
      " 0.1489\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 8: iteration 40, the loss is Variable containing:\n",
      " 0.1540\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 8: iteration 50, the loss is Variable containing:\n",
      " 0.1367\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 8: iteration 60, the loss is Variable containing:\n",
      " 0.1579\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 8: iteration 70, the loss is Variable containing:\n",
      " 0.1542\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 8: iteration 80, the loss is Variable containing:\n",
      " 0.1441\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 9: iteration 0, the loss is Variable containing:\n",
      " 0.1555\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 9: iteration 10, the loss is Variable containing:\n",
      " 0.1555\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 9: iteration 20, the loss is Variable containing:\n",
      " 0.1489\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 9: iteration 30, the loss is Variable containing:\n",
      " 0.1637\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 9: iteration 40, the loss is Variable containing:\n",
      " 0.1591\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 9: iteration 50, the loss is Variable containing:\n",
      " 0.1611\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 9: iteration 60, the loss is Variable containing:\n",
      " 0.1703\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 9: iteration 70, the loss is Variable containing:\n",
      " 0.1652\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "epoch 9: iteration 80, the loss is Variable containing:\n",
      " 0.1558\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "(446409,)\n",
      "(148803,)\n",
      "test set auc is 0.6344029175708916; Val set auc is 0.6293206671721198\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score            \n",
    "#try to test it's accuracy\n",
    "out=  net(X_train_Variable)\n",
    "out = F.softmax(out)\n",
    "score_1 = roc_auc_score(y_train_Variable.data.numpy(), out.data.numpy()[:,1])\n",
    "\n",
    "X_val_tensor = torch.from_numpy(X_val.values)\n",
    "y_val_tensor = torch.from_numpy(y_val.values)\n",
    "\n",
    "X_val_tensor= X_val_tensor.float()\n",
    "y_val_tensor =y_val_tensor.type(torch.LongTensor)\n",
    "#print(X_train_tensor)\n",
    "X_val_Variable = Variable(X_val_tensor)\n",
    "y_val_Variable = Variable(y_val_tensor)\n",
    "out2=  net(X_val_Variable)\n",
    "out2 = F.softmax(out2)\n",
    "score_2 = roc_auc_score(y_val_Variable.data.numpy(), out2.data.numpy()[:,1])\n",
    "\n",
    "print(out.data.numpy()[:,1].shape)\n",
    "print(out2.data.numpy()[:,1].shape)\n",
    "print('test set auc is {}; Val set auc is {}'.format(score_1, score_2))\n",
    "\n",
    "\n",
    "#So here I've tested that with Variable type, I can run this function\n",
    "# for t in range(100):\n",
    "#     out = net(X_train_tensor)                 # input x and predict based on x\n",
    "#     loss = criterion(out, y_train_tensor)     # must be (1. nn output, 2. target), the target label is NOT one-hotted\n",
    "\n",
    "#     optimizer.zero_grad()   # clear gradients for next train\n",
    "#     loss.backward()         # backpropagation, compute gradients\n",
    "#     optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(446409,)\n",
      "(446409,)\n"
     ]
    }
   ],
   "source": [
    "print(out.data.numpy()[:,1].shape)\n",
    "print(out2.data.numpy()[:,1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test set auc is 0.6344029175708916; Val set auc is 0.6293206671721198\n"
     ]
    }
   ],
   "source": [
    "print('test set auc is {}; Val set auc is {}'.format(score_1, score_2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import data_util\n",
    "import data_preprocess\n",
    "import datetime\n",
    "\n",
    "tic= datetime.datetime.now()\n",
    "train_data = data_util.load_train_data()\n",
    "test_data= data_util.load_test_data()\n",
    "\n",
    "naive_pre = data_preprocess.naive_preprocess()\n",
    "y_test_index = test_data['id']\n",
    "test_data.drop(['id'], axis=1, inplace=True)\n",
    "train_data = naive_pre.dtype_change(train_data)\n",
    "test_data = naive_pre.dtype_change(test_data)\n",
    "#train_prop is specified\n",
    "#X_train, X_val, y_train, y_val = data_util.split_train(train_data,prop=0.75)\n",
    "y_train = train_data['target']\n",
    "train_data.drop(['target','id'], axis=1, inplace=True)\n",
    "X_train = train_data\n",
    "\n",
    "X_train = naive_pre.scale(X_train)\n",
    "X_test = naive_pre.scale(test_data,test=True)\n",
    "\n",
    "#result = y_test_index,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test_tensor = torch.from_numpy(X_test.values)\n",
    "\n",
    "X_test_tensor= X_test_tensor.float()\n",
    "\n",
    "#print(X_train_tensor)\n",
    "X_test_Variable = Variable(X_test_tensor)\n",
    "\n",
    "out=  net(X_test_Variable)\n",
    "out = F.softmax(out)\n",
    "#print(out)\n",
    "prob = out.data.numpy()[:,1]\n",
    "\n",
    "result = np.hstack((y_test_index, prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt(\"inter.csv\", result, delimiter=\",\",header= 'id, target')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import special\n",
    "import data_util\n",
    "import data_preprocess\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n",
    "import numpy as np \n",
    "\n",
    "rso = np.random.RandomState(66)\n",
    "tic= datetime.datetime.now()\n",
    "train_data = data_util.load_train_data()\n",
    "test_data= data_util.load_test_data()\n",
    "\n",
    "naive_pre = data_preprocess.naive_preprocess()\n",
    "train_data = naive_pre.dtype_change(train_data)\n",
    "\n",
    "X, y = data_util.abandon_col(train_data)\n",
    "\n",
    "X_train, X_train_test, y_train, y_train_test = train_test_split(X,y,test_size =0.1 ,random_state=rso)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train,y_train, test_size =0.2, random_state=rso)\n",
    "\n",
    "X_train = naive_pre.scale(X_train)\n",
    "X_val = naive_pre.scale(X_val,test=True)\n",
    "X_train_test = naive_pre.scale(X_train_test,test=True)\n",
    "X_dev, y_dev = X_train[:10000,:], y_train[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data ={}\n",
    "#enter df here\n",
    "data['X_train'] = X_train\n",
    "data['X_val'] = X_val\n",
    "data['y_train'] = y_train\n",
    "data['y_val'] =y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from MY_NN import NeuralNetwork\n",
    "\n",
    "#only use step decay for now\n",
    "#coarse search\n",
    "train_hist={}\n",
    "for i in range(100):\n",
    "    #learnning_rate 5e-4 too large\n",
    "    weight_decay = 10** (np.random.uniform(-5,3))#L2 \n",
    "    learning_rate = 10** (np.random.uniform(-6,-2))\n",
    "    \n",
    "    nn_model = NeuralNetwork(data,learning_rate = learning_rate,num_epochs=5,verbose=None,\n",
    "                             weight_decay=weight_decay,batchnorm=True)\n",
    "    print('Learning rate is {}. Weight decay is {}'.format(learning_rate, weight_decay))\n",
    "    describe= 'Learning rate is {}. Weight decay is {}'.format(learning_rate, weight_decay)\n",
    "    nn_model.train()\n",
    "    train_hist[(nn_model.auc_history['val'][-1],nn_model.auc_history['train'][-1])]= describe\n",
    "    if i+1 %10 ==0:\n",
    "        print('You have finished {}!!'.format(i+1))\n",
    "\n",
    "        \n",
    "filename= 'coarse_search_lr_wd.pkl'\n",
    "with open(filename, 'wb') as f:\n",
    "    pickle.dump(train_hist, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#choose the best and plot it to see how it's going.\n",
    "\n",
    "lr =0.0021465415548623146\n",
    "wd = 0.0804501958708467\n",
    "nn_model = NeuralNetwork(data,learning_rate = lr,num_epochs=8,verbose=True,\n",
    "                             weight_decay=wd,batchnorm=True)\n",
    "nn_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate is 0.0021465415548623146. Weight decay is 1.280121140426467e-05\n",
      "Learning rate is 0.0017971189876773489. Weight decay is 1.5106300733936622e-05\n",
      "Learning rate is 0.001099954554802058. Weight decay is 0.0010046656425585008\n",
      "Learning rate is 0.00870610484058397. Weight decay is 3.456569495233953e-05\n",
      "Learning rate is 0.0014013584836759304. Weight decay is 0.004239152526071397\n",
      "Learning rate is 0.0018023955710091929. Weight decay is 8.4753451426549e-05\n",
      "Learning rate is 0.0015241883796123918. Weight decay is 4.2478230879085716e-05\n",
      "Learning rate is 0.0008147044361914067. Weight decay is 0.0022323437500745627\n",
      "Learning rate is 0.0049586139060953695. Weight decay is 0.00111846699640101\n",
      "Learning rate is 0.006824257588204783. Weight decay is 0.0006103289317873098\n",
      "Learning rate is 0.0019428041601537536. Weight decay is 0.0037230961634756765\n",
      "Learning rate is 0.0015651286837204412. Weight decay is 0.007209162252832926\n",
      "Learning rate is 0.0003602157173199339. Weight decay is 4.127835596796216e-05\n",
      "Learning rate is 0.0005348530931481333. Weight decay is 0.00021475130118434038\n",
      "Learning rate is 0.0001725878736171109. Weight decay is 0.0002504298051405482\n",
      "Learning rate is 0.000369219139900573. Weight decay is 0.00010078230366585651\n",
      "Learning rate is 0.0005289698696419238. Weight decay is 0.0013850476397544221\n",
      "Learning rate is 0.0002668031417400006. Weight decay is 0.001269317465628375\n",
      "Learning rate is 0.000125409642198912. Weight decay is 0.0003688540312532336\n",
      "Learning rate is 0.00013103144425543722. Weight decay is 2.8321142540716588e-05\n",
      "Learning rate is 0.001351524305360822. Weight decay is 0.07837783348120834\n",
      "Learning rate is 0.0016869991759926517. Weight decay is 0.060205833671201366\n",
      "Learning rate is 6.06407103552065e-05. Weight decay is 0.0015705003834968088\n",
      "Learning rate is 6.193805991283037e-05. Weight decay is 0.0007168962155118211\n",
      "Learning rate is 2.5303907400747695e-05. Weight decay is 1.3601651794315251e-05\n",
      "Learning rate is 4.437806080473289e-05. Weight decay is 0.02390602229934318\n",
      "Learning rate is 0.00014683104273734726. Weight decay is 0.0047840247635864905\n",
      "Learning rate is 7.69094011813857e-05. Weight decay is 7.667195677014131\n",
      "Learning rate is 0.00011116674051491084. Weight decay is 0.07386426968239779\n",
      "Learning rate is 2.499531308321811e-05. Weight decay is 5.538795299971478e-05\n",
      "Learning rate is 5.572229644935542e-05. Weight decay is 1.647969907197511\n",
      "Learning rate is 3.3133771075768756e-05. Weight decay is 0.20648545107635963\n",
      "Learning rate is 2.1330134307410822e-05. Weight decay is 1.950722992439517e-05\n",
      "Learning rate is 5.422535579470406e-06. Weight decay is 278.06526510901153\n",
      "Learning rate is 1.4026259474254873e-05. Weight decay is 1.4864357232861236e-05\n",
      "Learning rate is 5.493617495715185e-05. Weight decay is 238.34697428251226\n",
      "Learning rate is 2.6324099843625135e-05. Weight decay is 4.472078245573766\n",
      "Learning rate is 1.1510089401239123e-05. Weight decay is 0.0020075022793379\n",
      "Learning rate is 9.313914856442796e-05. Weight decay is 791.1982606467674\n",
      "Learning rate is 1.4630491921661305e-05. Weight decay is 0.007941486889099136\n",
      "Learning rate is 1.8047090162663243e-05. Weight decay is 0.00047511478640232944\n",
      "Learning rate is 1.8155553651447172e-05. Weight decay is 0.1383578099222878\n",
      "Learning rate is 1.3413306189120525e-05. Weight decay is 0.028336773675780275\n",
      "Learning rate is 1.3325915491647777e-06. Weight decay is 1.2020837808772427\n",
      "Learning rate is 1.7250236728218746e-05. Weight decay is 1.9766633725644238\n",
      "Learning rate is 1.7470466533651758e-05. Weight decay is 4.4487338226188474e-05\n",
      "Learning rate is 3.746242063214872e-06. Weight decay is 3.0688391217150765e-05\n",
      "Learning rate is 2.8140285126339848e-05. Weight decay is 0.029399519575895466\n",
      "Learning rate is 2.2791685324416556e-05. Weight decay is 31.004307153993356\n",
      "Learning rate is 6.066345190343032e-06. Weight decay is 2.087669088414076e-05\n",
      "Learning rate is 1.5816991530609837e-05. Weight decay is 6.497870699371525e-05\n",
      "Learning rate is 1.2773119350104022e-06. Weight decay is 1.200249213920693\n",
      "Learning rate is 8.567012012128683e-05. Weight decay is 0.8593269764285293\n",
      "Learning rate is 1.1478468162759344e-05. Weight decay is 2.7946169238649166\n",
      "Learning rate is 2.489925717028496e-06. Weight decay is 0.014942932454575165\n",
      "Learning rate is 5.726183354953971e-06. Weight decay is 0.0629756472036835\n",
      "Learning rate is 1.0801296105511623e-05. Weight decay is 2.084070038031838e-05\n",
      "Learning rate is 6.695804019081695e-05. Weight decay is 4.726400938613581\n",
      "Learning rate is 0.0008717908504345675. Weight decay is 0.35503591341165847\n",
      "Learning rate is 0.00010854486225480558. Weight decay is 90.30217222977906\n",
      "Learning rate is 6.926194456419545e-06. Weight decay is 0.5918028520984112\n",
      "Learning rate is 3.375319925207577e-05. Weight decay is 13.941501782990603\n",
      "Learning rate is 1.7108117105596439e-06. Weight decay is 0.003979341020525913\n",
      "Learning rate is 0.0032645149487107836. Weight decay is 0.03052077919765806\n",
      "Learning rate is 0.00023894563161198473. Weight decay is 70.36846662104449\n",
      "Learning rate is 0.00014660679254509119. Weight decay is 2.1213494020075734\n",
      "Learning rate is 0.0001335864884463259. Weight decay is 1.6095204494401882\n",
      "Learning rate is 1.7549768534628807e-05. Weight decay is 2.967995094950235\n",
      "Learning rate is 5.0743230296155484e-06. Weight decay is 0.0008899985947456078\n",
      "Learning rate is 6.705670414927071e-05. Weight decay is 848.5639349271987\n",
      "Learning rate is 5.010570140432403e-05. Weight decay is 3.7063387473624436\n",
      "Learning rate is 8.482454075404609e-06. Weight decay is 119.89788522304053\n",
      "Learning rate is 3.4570607849363356e-06. Weight decay is 0.10088882356995434\n",
      "Learning rate is 1.6816237558982147e-06. Weight decay is 263.04440338214613\n",
      "Learning rate is 7.547852349746691e-06. Weight decay is 17.07185309732603\n",
      "Learning rate is 8.146175684713504e-05. Weight decay is 15.3564465077748\n",
      "Learning rate is 0.000642476704539946. Weight decay is 0.3486334448732021\n",
      "Learning rate is 6.892578957133873e-06. Weight decay is 433.3272800958669\n",
      "Learning rate is 8.592809264955555e-06. Weight decay is 0.9134500776620532\n",
      "Learning rate is 2.852971551621623e-06. Weight decay is 938.4193663736665\n",
      "Learning rate is 7.90554544256466e-06. Weight decay is 7.613870328619848\n",
      "Learning rate is 6.706494374093027e-06. Weight decay is 0.5126946766803211\n",
      "Learning rate is 3.9060492935780775e-06. Weight decay is 133.12342671131336\n",
      "Learning rate is 3.125804083096541e-06. Weight decay is 0.10470268273329282\n",
      "Learning rate is 6.802328961836098e-06. Weight decay is 63.034804671531674\n",
      "Learning rate is 3.6084940916996726e-06. Weight decay is 121.94865945550642\n",
      "Learning rate is 1.29186107323058e-06. Weight decay is 0.022287918112396495\n"
     ]
    }
   ],
   "source": [
    "a =sorted(train_hist, key=lambda x:x[0],reverse=True)\n",
    "for i in a:\n",
    "    print(train_hist[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3oAAALJCAYAAADielEXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc83ee3f320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_vis(nn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finer search here but should cost considerable time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate is 0.00020406927268633738. Weight decay is 0.0008021124318505044\n",
      "Val aus is 0.6317421740076681. Train auc is 0.6791059413712075\n",
      "Learning rate is 0.00015493820527177172. Weight decay is 0.0008104257925743996\n",
      "Val aus is 0.6330116090802047. Train auc is 0.6654130268843802\n",
      "Learning rate is 0.00011280697210265712. Weight decay is 0.00022878098723526239\n",
      "Val aus is 0.6150286814657072. Train auc is 0.6313906017203497\n",
      "Learning rate is 7.651260359148915e-06. Weight decay is 2.6327288973450546e-05\n",
      "Val aus is 0.5515363038949513. Train auc is 0.5470427497716127\n",
      "Learning rate is 5.55017362248129e-05. Weight decay is 0.0006954451651473917\n",
      "Val aus is 0.6209540032022394. Train auc is 0.6257346869824794\n",
      "Learning rate is 3.513556802668994e-06. Weight decay is 0.00010345946256136488\n",
      "Val aus is 0.530607868619493. Train auc is 0.5191439264170947\n",
      "Learning rate is 0.00041679711472788464. Weight decay is 0.00017450057767011\n",
      "Val aus is 0.6299209582212122. Train auc is 0.7003492741546931\n",
      "Learning rate is 0.00046931647302294216. Weight decay is 0.0005242045085095721\n",
      "Val aus is 0.6284672207696417. Train auc is 0.7037803306035294\n",
      "Learning rate is 0.0007480411204707365. Weight decay is 0.00013297987797352255\n",
      "Val aus is 0.6254457315422669. Train auc is 0.7189093244563526\n",
      "Learning rate is 0.0005948852474826226. Weight decay is 0.0005612195248733336\n"
     ]
    }
   ],
   "source": [
    "from MY_NN import NeuralNetwork\n",
    "train_hist={}\n",
    "best_net = None\n",
    "best_auc =0\n",
    "for i in range(10):\n",
    "    #learnning_rate 5e-4 too large\n",
    "    weight_decay = 10** (np.random.uniform(-5,-3))#L2 \n",
    "    learning_rate = 10** (np.random.uniform(-5.5,-3))\n",
    "    dropout = np.random.uniform(0,1)\n",
    "    nn_model = NeuralNetwork(data,learning_rate = learning_rate,num_epochs=80,verbose=None,dropout=dropout,\n",
    "                             weight_decay=weight_decay,batchnorm=True)\n",
    "    print('Learning rate is {}. Weight decay is {}'.format(learning_rate, weight_decay))\n",
    "    describe= 'Learning rate is {}. Weight decay is {}'.format(learning_rate, weight_decay)\n",
    "    nn_model.train()\n",
    "    print('Val aus is {}. Train auc is {}'.format(nn_model.auc_history['val'][-1],nn_model.auc_history['train'][-1]))\n",
    "    if nn_model.auc_history['val'][-1]> best_auc:\n",
    "        best_auc =nn_model.auc_history['val'][-1]\n",
    "        best_net = nn_model\n",
    "    train_hist[(nn_model.auc_history['val'][-1],nn_model.auc_history['train'][-1])]= describe\n",
    "    if (i+1) %10 ==0:\n",
    "        print('You have finished {}!!'.format(i+1))\n",
    "\n",
    "train_hist['best_net'] = best_net\n",
    "filename= 'search_lr_wd.pkl'\n",
    "with open(filename, 'wb') as f:\n",
    "    pickle.dump(train_hist, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: iteration 0, the loss is [ 0.96388286]\n",
      "  acc for train: 0.43534506897646025, acc for val: 0.4365957923425862\n",
      "  auc for train: 0.46641468653097995, auc for val: 0.4694342688795342\n",
      "--------------------------------------------------------------\n",
      "Epoch 0: iteration 100, the loss is [ 0.63663191]\n",
      "  acc for train: 0.8803925777968601, acc for val: 0.8812652840262092\n",
      "  auc for train: 0.5210164423005953, auc for val: 0.5161746400086831\n",
      "--------------------------------------------------------------\n",
      "Epoch 0: iteration 200, the loss is [ 0.45769787]\n",
      "  acc for train: 0.9634326756146279, acc for val: 0.9639250312680842\n",
      "  auc for train: 0.5328574174698167, auc for val: 0.5266127325223127\n",
      "--------------------------------------------------------------\n",
      "Epoch 0: iteration 300, the loss is [ 0.35054463]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.5385647540666245, auc for val: 0.5334625369541983\n",
      "--------------------------------------------------------------\n",
      "Epoch 0: iteration 400, the loss is [ 0.28321517]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.5381127645821319, auc for val: 0.5351305491640541\n",
      "--------------------------------------------------------------\n",
      "Epoch 1: iteration 0, the loss is [ 0.26585665]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.5383247945897658, auc for val: 0.5361717450885745\n",
      "--------------------------------------------------------------\n",
      "Epoch 1: iteration 100, the loss is [ 0.23649496]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.5388659136886669, auc for val: 0.5375353924920464\n",
      "--------------------------------------------------------------\n",
      "Epoch 1: iteration 200, the loss is [ 0.19210169]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.539700480622576, auc for val: 0.5395077106185315\n",
      "--------------------------------------------------------------\n",
      "Epoch 1: iteration 300, the loss is [ 0.19306733]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.5402410461300294, auc for val: 0.5403708201655841\n",
      "--------------------------------------------------------------\n",
      "Epoch 1: iteration 400, the loss is [ 0.18179266]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.5437046176125472, auc for val: 0.5442375452729399\n",
      "--------------------------------------------------------------\n",
      "Epoch 2: iteration 0, the loss is [ 0.16861475]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.5439648702121508, auc for val: 0.5441813230987249\n",
      "--------------------------------------------------------------\n",
      "Epoch 2: iteration 100, the loss is [ 0.16604416]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.5482940262136249, auc for val: 0.5471999781000279\n",
      "--------------------------------------------------------------\n",
      "Epoch 2: iteration 200, the loss is [ 0.17256184]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.5488250069831402, auc for val: 0.5482692118127055\n",
      "--------------------------------------------------------------\n",
      "Epoch 2: iteration 300, the loss is [ 0.1600001]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.5511139970094885, auc for val: 0.5496321675748018\n",
      "--------------------------------------------------------------\n",
      "Epoch 2: iteration 400, the loss is [ 0.1667404]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.5543011332750472, auc for val: 0.5533454247281478\n",
      "--------------------------------------------------------------\n",
      "Epoch 3: iteration 0, the loss is [ 0.18265635]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.5546307761896491, auc for val: 0.553618740851038\n",
      "--------------------------------------------------------------\n",
      "Epoch 3: iteration 100, the loss is [ 0.17564861]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.555781672944528, auc for val: 0.5548447290672331\n",
      "--------------------------------------------------------------\n",
      "Epoch 3: iteration 200, the loss is [ 0.17820887]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.5595637759373198, auc for val: 0.5577635621752117\n",
      "--------------------------------------------------------------\n",
      "Epoch 3: iteration 300, the loss is [ 0.17906691]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.5637497971675178, auc for val: 0.5617780053411554\n",
      "--------------------------------------------------------------\n",
      "Epoch 3: iteration 400, the loss is [ 0.19748439]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.5646040256414129, auc for val: 0.5633606148768011\n",
      "--------------------------------------------------------------\n",
      "Epoch 4: iteration 0, the loss is [ 0.17283474]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.5653359095551043, auc for val: 0.5642541716800643\n",
      "--------------------------------------------------------------\n",
      "Epoch 4: iteration 100, the loss is [ 0.15941714]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.5691205612946704, auc for val: 0.5673109534121997\n",
      "--------------------------------------------------------------\n",
      "Epoch 4: iteration 200, the loss is [ 0.17626843]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.5719771810501869, auc for val: 0.5700469939742199\n",
      "--------------------------------------------------------------\n",
      "Epoch 4: iteration 300, the loss is [ 0.15491308]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.574190160852859, auc for val: 0.5718262125946811\n",
      "--------------------------------------------------------------\n",
      "Epoch 4: iteration 400, the loss is [ 0.17707901]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.576863273700561, auc for val: 0.5750557981679804\n",
      "--------------------------------------------------------------\n",
      "Epoch 5: iteration 0, the loss is [ 0.22013016]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.5776004878450043, auc for val: 0.5758284630754321\n",
      "--------------------------------------------------------------\n",
      "Epoch 5: iteration 100, the loss is [ 0.1902959]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.5808161684666863, auc for val: 0.5787942651482793\n",
      "--------------------------------------------------------------\n",
      "Epoch 5: iteration 200, the loss is [ 0.19321831]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.5823809930910991, auc for val: 0.5805494128947052\n",
      "--------------------------------------------------------------\n",
      "Epoch 5: iteration 300, the loss is [ 0.15423577]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.5843784169677513, auc for val: 0.5816539917370307\n",
      "--------------------------------------------------------------\n",
      "Epoch 5: iteration 400, the loss is [ 0.18230116]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.5851784401663402, auc for val: 0.5815656295393976\n",
      "--------------------------------------------------------------\n",
      "Epoch 6: iteration 0, the loss is [ 0.16687998]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.5846053165715284, auc for val: 0.58183695969928\n",
      "--------------------------------------------------------------\n",
      "Epoch 6: iteration 100, the loss is [ 0.18872426]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.588103699309612, auc for val: 0.5849085879144951\n",
      "--------------------------------------------------------------\n",
      "Epoch 6: iteration 200, the loss is [ 0.15764053]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.5903533753768748, auc for val: 0.5876596077747868\n",
      "--------------------------------------------------------------\n",
      "Epoch 6: iteration 300, the loss is [ 0.15410705]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.592665444570389, auc for val: 0.5895194176511931\n",
      "--------------------------------------------------------------\n",
      "Epoch 6: iteration 400, the loss is [ 0.16872516]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.595482227616337, auc for val: 0.592469813161803\n",
      "--------------------------------------------------------------\n",
      "Epoch 7: iteration 0, the loss is [ 0.1569102]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.5965923349154612, auc for val: 0.5935479417334041\n",
      "--------------------------------------------------------------\n",
      "Epoch 7: iteration 100, the loss is [ 0.17336418]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.5975215492137784, auc for val: 0.5946412701266137\n",
      "--------------------------------------------------------------\n",
      "Epoch 7: iteration 200, the loss is [ 0.1538175]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6006628838644317, auc for val: 0.5979568460186977\n",
      "--------------------------------------------------------------\n",
      "Epoch 7: iteration 300, the loss is [ 0.15632018]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6035827566603094, auc for val: 0.6008125593333073\n",
      "--------------------------------------------------------------\n",
      "Epoch 7: iteration 400, the loss is [ 0.18321206]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6090953660354035, auc for val: 0.6070414101428947\n",
      "--------------------------------------------------------------\n",
      "Epoch 8: iteration 0, the loss is [ 0.13602434]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6090755448798993, auc for val: 0.6073512892856812\n",
      "--------------------------------------------------------------\n",
      "Epoch 8: iteration 100, the loss is [ 0.15391308]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6078402098928137, auc for val: 0.606563073724039\n",
      "--------------------------------------------------------------\n",
      "Epoch 8: iteration 200, the loss is [ 0.17425214]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6101721415177092, auc for val: 0.6081981252128301\n",
      "--------------------------------------------------------------\n",
      "Epoch 8: iteration 300, the loss is [ 0.14944799]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6107643576019294, auc for val: 0.608497840234531\n",
      "--------------------------------------------------------------\n",
      "Epoch 8: iteration 400, the loss is [ 0.14402528]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6115619674757233, auc for val: 0.610986558862029\n",
      "--------------------------------------------------------------\n",
      "Epoch 9: iteration 0, the loss is [ 0.16321748]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6139510098421255, auc for val: 0.6136712889059801\n",
      "--------------------------------------------------------------\n",
      "Epoch 9: iteration 100, the loss is [ 0.15570393]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6163597710245906, auc for val: 0.6150441506766198\n",
      "--------------------------------------------------------------\n",
      "Epoch 9: iteration 200, the loss is [ 0.20813507]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6151537722102703, auc for val: 0.6139607972054961\n",
      "--------------------------------------------------------------\n",
      "Epoch 9: iteration 300, the loss is [ 0.1457784]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6137912970862425, auc for val: 0.6114843915486475\n",
      "--------------------------------------------------------------\n",
      "Epoch 9: iteration 400, the loss is [ 0.15647461]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6127867461185462, auc for val: 0.6106959792701647\n",
      "--------------------------------------------------------------\n",
      "Epoch 10: iteration 0, the loss is [ 0.15343946]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6145621168286978, auc for val: 0.6124608638737106\n",
      "--------------------------------------------------------------\n",
      "Epoch 10: iteration 100, the loss is [ 0.13971567]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6146298963023429, auc for val: 0.6124346215965881\n",
      "--------------------------------------------------------------\n",
      "Epoch 10: iteration 200, the loss is [ 0.17039542]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.618049938507484, auc for val: 0.615628291937294\n",
      "--------------------------------------------------------------\n",
      "Epoch 10: iteration 300, the loss is [ 0.15597004]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6162608552397792, auc for val: 0.6151096786850683\n",
      "--------------------------------------------------------------\n",
      "Epoch 10: iteration 400, the loss is [ 0.1421611]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6180260086243905, auc for val: 0.6156939865912188\n",
      "--------------------------------------------------------------\n",
      "Epoch 11: iteration 0, the loss is [ 0.14657588]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6187103267272531, auc for val: 0.6163218791807967\n",
      "--------------------------------------------------------------\n",
      "Epoch 11: iteration 100, the loss is [ 0.18852615]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6141472287152995, auc for val: 0.6136550115271557\n",
      "--------------------------------------------------------------\n",
      "Epoch 11: iteration 200, the loss is [ 0.15463713]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6194132362118228, auc for val: 0.6183813102903286\n",
      "--------------------------------------------------------------\n",
      "Epoch 11: iteration 300, the loss is [ 0.1724001]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6135108477515698, auc for val: 0.6124847417898259\n",
      "--------------------------------------------------------------\n",
      "Epoch 11: iteration 400, the loss is [ 0.14499983]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6187197654537075, auc for val: 0.616490257519535\n",
      "--------------------------------------------------------------\n",
      "Epoch 12: iteration 0, the loss is [ 0.18036851]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6184496207886603, auc for val: 0.615580993440393\n",
      "--------------------------------------------------------------\n",
      "Epoch 12: iteration 100, the loss is [ 0.16675496]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6193558009831758, auc for val: 0.6166276222580379\n",
      "--------------------------------------------------------------\n",
      "Epoch 12: iteration 200, the loss is [ 0.14156191]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6146306293742908, auc for val: 0.6126242278124755\n",
      "--------------------------------------------------------------\n",
      "Epoch 12: iteration 300, the loss is [ 0.13646029]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6157533541834899, auc for val: 0.6142570728149215\n",
      "--------------------------------------------------------------\n",
      "Epoch 12: iteration 400, the loss is [ 0.15638264]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6239308109505078, auc for val: 0.620265897717828\n",
      "--------------------------------------------------------------\n",
      "Epoch 13: iteration 0, the loss is [ 0.15997514]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6264678603422097, auc for val: 0.6221465219901261\n",
      "--------------------------------------------------------------\n",
      "Epoch 13: iteration 100, the loss is [ 0.13045749]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.621698326286133, auc for val: 0.6181389538898131\n",
      "--------------------------------------------------------------\n",
      "Epoch 13: iteration 200, the loss is [ 0.13245548]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.623359232109268, auc for val: 0.6200201833483149\n",
      "--------------------------------------------------------------\n",
      "Epoch 13: iteration 300, the loss is [ 0.16689554]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.624943279234373, auc for val: 0.6221362275597957\n",
      "--------------------------------------------------------------\n",
      "Epoch 13: iteration 400, the loss is [ 0.16730256]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6242355075442143, auc for val: 0.6207378527426146\n",
      "--------------------------------------------------------------\n",
      "Epoch 14: iteration 0, the loss is [ 0.14993529]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.621256273775857, auc for val: 0.617978391599809\n",
      "--------------------------------------------------------------\n",
      "Epoch 14: iteration 100, the loss is [ 0.1801305]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6232797344604397, auc for val: 0.6206051791369206\n",
      "--------------------------------------------------------------\n",
      "Epoch 14: iteration 200, the loss is [ 0.11670103]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6226702846153152, auc for val: 0.6196630357799916\n",
      "--------------------------------------------------------------\n",
      "Epoch 14: iteration 300, the loss is [ 0.12997773]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6228369716804327, auc for val: 0.618685859283818\n",
      "--------------------------------------------------------------\n",
      "Epoch 14: iteration 400, the loss is [ 0.1768474]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6253411834064088, auc for val: 0.619595415305787\n",
      "--------------------------------------------------------------\n",
      "Epoch 15: iteration 0, the loss is [ 0.14106265]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.62602371080094, auc for val: 0.6207027832671382\n",
      "--------------------------------------------------------------\n",
      "Epoch 15: iteration 100, the loss is [ 0.16962788]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6268588709789654, auc for val: 0.6215036927033759\n",
      "--------------------------------------------------------------\n",
      "Epoch 15: iteration 200, the loss is [ 0.14117277]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.623461078790549, auc for val: 0.6192977576665841\n",
      "--------------------------------------------------------------\n",
      "Epoch 15: iteration 300, the loss is [ 0.13755955]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6270790505761974, auc for val: 0.6236725008821936\n",
      "--------------------------------------------------------------\n",
      "Epoch 15: iteration 400, the loss is [ 0.14386225]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6276589311248751, auc for val: 0.6225686249580054\n",
      "--------------------------------------------------------------\n",
      "Epoch 16: iteration 0, the loss is [ 0.16202977]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6274329221988771, auc for val: 0.6214527593303324\n",
      "--------------------------------------------------------------\n",
      "Epoch 16: iteration 100, the loss is [ 0.12860292]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.627400343860058, auc for val: 0.6216069402262674\n",
      "--------------------------------------------------------------\n",
      "Epoch 16: iteration 200, the loss is [ 0.16169819]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6294742946045133, auc for val: 0.6241616241499677\n",
      "--------------------------------------------------------------\n",
      "Epoch 16: iteration 300, the loss is [ 0.12861703]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6273431859682046, auc for val: 0.6216547749958291\n",
      "--------------------------------------------------------------\n",
      "Epoch 16: iteration 400, the loss is [ 0.15848528]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6271481009449256, auc for val: 0.6194590930412418\n",
      "--------------------------------------------------------------\n",
      "Epoch 17: iteration 0, the loss is [ 0.14947848]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6274787405869386, auc for val: 0.619954508741966\n",
      "--------------------------------------------------------------\n",
      "Epoch 17: iteration 100, the loss is [ 0.15718378]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6291224149680387, auc for val: 0.6216572759309482\n",
      "--------------------------------------------------------------\n",
      "Epoch 17: iteration 200, the loss is [ 0.14947823]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6312681325595576, auc for val: 0.6229153540516211\n",
      "--------------------------------------------------------------\n",
      "Epoch 17: iteration 300, the loss is [ 0.11146899]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.632860778207398, auc for val: 0.6246263495176212\n",
      "--------------------------------------------------------------\n",
      "Epoch 17: iteration 400, the loss is [ 0.16322629]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6343023374485246, auc for val: 0.6252818889676224\n",
      "--------------------------------------------------------------\n",
      "Epoch 18: iteration 0, the loss is [ 0.1542685]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6343954898377596, auc for val: 0.6247513423957198\n",
      "--------------------------------------------------------------\n",
      "Epoch 18: iteration 100, the loss is [ 0.16573492]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6341772819752722, auc for val: 0.6231563083747826\n",
      "--------------------------------------------------------------\n",
      "Epoch 18: iteration 200, the loss is [ 0.15205076]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6317388334659022, auc for val: 0.6215273563610217\n",
      "--------------------------------------------------------------\n",
      "Epoch 18: iteration 300, the loss is [ 0.15458079]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.632997211291198, auc for val: 0.6234646463601561\n",
      "--------------------------------------------------------------\n",
      "Epoch 18: iteration 400, the loss is [ 0.14919011]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6353497780720911, auc for val: 0.6265941419038004\n",
      "--------------------------------------------------------------\n",
      "Epoch 19: iteration 0, the loss is [ 0.14959593]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6331379653564435, auc for val: 0.6245522223519968\n",
      "--------------------------------------------------------------\n",
      "Epoch 19: iteration 100, the loss is [ 0.15470241]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6335742295046481, auc for val: 0.6243659327569815\n",
      "--------------------------------------------------------------\n",
      "Epoch 19: iteration 200, the loss is [ 0.1737093]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6319417384128306, auc for val: 0.623095882474453\n",
      "--------------------------------------------------------------\n",
      "Epoch 19: iteration 300, the loss is [ 0.14675422]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6358510746838234, auc for val: 0.6257713405144553\n",
      "--------------------------------------------------------------\n",
      "Epoch 19: iteration 400, the loss is [ 0.13503832]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6351128585558647, auc for val: 0.6240452479706741\n",
      "--------------------------------------------------------------\n",
      "Epoch 20: iteration 0, the loss is [ 0.15876572]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.633104301091033, auc for val: 0.6227842241092871\n",
      "--------------------------------------------------------------\n",
      "Epoch 20: iteration 100, the loss is [ 0.1643507]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6371876772531776, auc for val: 0.6270384701157399\n",
      "--------------------------------------------------------------\n",
      "Epoch 20: iteration 200, the loss is [ 0.15111077]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6360860591034435, auc for val: 0.6263453351956729\n",
      "--------------------------------------------------------------\n",
      "Epoch 20: iteration 300, the loss is [ 0.15367527]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6369566688445154, auc for val: 0.6269504349441922\n",
      "--------------------------------------------------------------\n",
      "Epoch 20: iteration 400, the loss is [ 0.15759331]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6377156019845656, auc for val: 0.627889995922724\n",
      "--------------------------------------------------------------\n",
      "Epoch 21: iteration 0, the loss is [ 0.16278806]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6375084844246933, auc for val: 0.6268414989215808\n",
      "--------------------------------------------------------------\n",
      "Epoch 21: iteration 100, the loss is [ 0.18399349]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6372680721499151, auc for val: 0.627235913680911\n",
      "--------------------------------------------------------------\n",
      "Epoch 21: iteration 200, the loss is [ 0.14086421]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6366488145689013, auc for val: 0.6265896800651434\n",
      "--------------------------------------------------------------\n",
      "Epoch 21: iteration 300, the loss is [ 0.15637629]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.637025623366637, auc for val: 0.6268888638260774\n",
      "--------------------------------------------------------------\n",
      "Epoch 21: iteration 400, the loss is [ 0.18616274]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6368207869559979, auc for val: 0.6257319520393017\n",
      "--------------------------------------------------------------\n",
      "Epoch 22: iteration 0, the loss is [ 0.15989184]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6394228307044718, auc for val: 0.627724633491216\n",
      "--------------------------------------------------------------\n",
      "Epoch 22: iteration 100, the loss is [ 0.16831556]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6397839539731796, auc for val: 0.6285584397468696\n",
      "--------------------------------------------------------------\n",
      "Epoch 22: iteration 200, the loss is [ 0.16448854]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6412085717082558, auc for val: 0.630125823148463\n",
      "--------------------------------------------------------------\n",
      "Epoch 22: iteration 300, the loss is [ 0.17586648]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6399002884873867, auc for val: 0.6278415509550646\n",
      "--------------------------------------------------------------\n",
      "Epoch 22: iteration 400, the loss is [ 0.13990514]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6380008744132282, auc for val: 0.6270388673083415\n",
      "--------------------------------------------------------------\n",
      "Epoch 23: iteration 0, the loss is [ 0.18364224]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6392836453544499, auc for val: 0.6272383657500634\n",
      "--------------------------------------------------------------\n",
      "Epoch 23: iteration 100, the loss is [ 0.15869644]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6424636580528732, auc for val: 0.6311090151704418\n",
      "--------------------------------------------------------------\n",
      "Epoch 23: iteration 200, the loss is [ 0.17769535]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6417339873455379, auc for val: 0.6288461963894236\n",
      "--------------------------------------------------------------\n",
      "Epoch 23: iteration 300, the loss is [ 0.17808154]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.639516548975163, auc for val: 0.6277352424178964\n",
      "--------------------------------------------------------------\n",
      "Epoch 23: iteration 400, the loss is [ 0.18508072]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6409010409149383, auc for val: 0.6293027486108934\n",
      "--------------------------------------------------------------\n",
      "Epoch 24: iteration 0, the loss is [ 0.14584696]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6433862936410942, auc for val: 0.6308587988671195\n",
      "--------------------------------------------------------------\n",
      "Epoch 24: iteration 100, the loss is [ 0.12991081]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.643813771901907, auc for val: 0.629644985794488\n",
      "--------------------------------------------------------------\n",
      "Epoch 24: iteration 200, the loss is [ 0.16287664]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6429920629448409, auc for val: 0.6296061486276772\n",
      "--------------------------------------------------------------\n",
      "Epoch 24: iteration 300, the loss is [ 0.17248602]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6422047663204895, auc for val: 0.6301863542985671\n",
      "--------------------------------------------------------------\n",
      "Epoch 24: iteration 400, the loss is [ 0.12774318]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6429416878236005, auc for val: 0.631165084481889\n",
      "--------------------------------------------------------------\n",
      "Epoch 25: iteration 0, the loss is [ 0.12718853]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6427159989788904, auc for val: 0.630062492855545\n",
      "--------------------------------------------------------------\n",
      "Epoch 25: iteration 100, the loss is [ 0.12756006]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.64076629279902, auc for val: 0.6265728701725787\n",
      "--------------------------------------------------------------\n",
      "Epoch 25: iteration 200, the loss is [ 0.17339201]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6439818049161841, auc for val: 0.630164611449307\n",
      "--------------------------------------------------------------\n",
      "Epoch 25: iteration 300, the loss is [ 0.13919944]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.646838637002852, auc for val: 0.6338327515326272\n",
      "--------------------------------------------------------------\n",
      "Epoch 25: iteration 400, the loss is [ 0.13727504]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6462169611501217, auc for val: 0.6325895411955837\n",
      "--------------------------------------------------------------\n",
      "Epoch 26: iteration 0, the loss is [ 0.14273988]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6443071803379307, auc for val: 0.6315044849334648\n",
      "--------------------------------------------------------------\n",
      "Epoch 26: iteration 100, the loss is [ 0.14557837]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.646651088769255, auc for val: 0.6327054775811766\n",
      "--------------------------------------------------------------\n",
      "Epoch 26: iteration 200, the loss is [ 0.14873542]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6407942844912433, auc for val: 0.6261733733527057\n",
      "--------------------------------------------------------------\n",
      "Epoch 26: iteration 300, the loss is [ 0.17443523]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6439902445190622, auc for val: 0.6288730300700409\n",
      "--------------------------------------------------------------\n",
      "Epoch 26: iteration 400, the loss is [ 0.14894634]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6436499141667744, auc for val: 0.6297408081963793\n",
      "--------------------------------------------------------------\n",
      "Epoch 27: iteration 0, the loss is [ 0.16958211]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6448029220204555, auc for val: 0.6307656114685126\n",
      "--------------------------------------------------------------\n",
      "Epoch 27: iteration 100, the loss is [ 0.17017768]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6445717411651973, auc for val: 0.6295598086555089\n",
      "--------------------------------------------------------------\n",
      "Epoch 27: iteration 200, the loss is [ 0.14546952]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6465422493050791, auc for val: 0.6312338927749779\n",
      "--------------------------------------------------------------\n",
      "Epoch 27: iteration 300, the loss is [ 0.12959281]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6463968019252405, auc for val: 0.6310005740723656\n",
      "--------------------------------------------------------------\n",
      "Epoch 27: iteration 400, the loss is [ 0.17139147]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6494408787827821, auc for val: 0.6335584305249846\n",
      "--------------------------------------------------------------\n",
      "Epoch 28: iteration 0, the loss is [ 0.16347478]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6493730731059115, auc for val: 0.6322269720108973\n",
      "--------------------------------------------------------------\n",
      "Epoch 28: iteration 100, the loss is [ 0.16536799]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6486261436711768, auc for val: 0.6330553679269495\n",
      "--------------------------------------------------------------\n",
      "Epoch 28: iteration 200, the loss is [ 0.18812957]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6469505739356243, auc for val: 0.6312581478361188\n",
      "--------------------------------------------------------------\n",
      "Epoch 28: iteration 300, the loss is [ 0.11982883]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6499312812682597, auc for val: 0.6342039161155688\n",
      "--------------------------------------------------------------\n",
      "Epoch 28: iteration 400, the loss is [ 0.16739157]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.64880020095385, auc for val: 0.6321403739999617\n",
      "--------------------------------------------------------------\n",
      "Epoch 29: iteration 0, the loss is [ 0.12575097]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6491918665166512, auc for val: 0.6320603390642536\n",
      "--------------------------------------------------------------\n",
      "Epoch 29: iteration 100, the loss is [ 0.15028465]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.649829712619433, auc for val: 0.6328063081181736\n",
      "--------------------------------------------------------------\n",
      "Epoch 29: iteration 200, the loss is [ 0.15579619]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6498620669554572, auc for val: 0.6324648765965438\n",
      "--------------------------------------------------------------\n",
      "Epoch 29: iteration 300, the loss is [ 0.1464572]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6508310603780836, auc for val: 0.6339826034050968\n",
      "--------------------------------------------------------------\n",
      "Epoch 29: iteration 400, the loss is [ 0.16893944]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6478166087790395, auc for val: 0.6280932432611778\n",
      "--------------------------------------------------------------\n",
      "Epoch 30: iteration 0, the loss is [ 0.13606025]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6517124762829603, auc for val: 0.632821247321293\n",
      "--------------------------------------------------------------\n",
      "Epoch 30: iteration 100, the loss is [ 0.13336651]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6519049797088136, auc for val: 0.6329728871869567\n",
      "--------------------------------------------------------------\n",
      "Epoch 30: iteration 200, the loss is [ 0.16220525]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6514194861927936, auc for val: 0.632688685230241\n",
      "--------------------------------------------------------------\n",
      "Epoch 30: iteration 300, the loss is [ 0.16195978]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6521427486269415, auc for val: 0.633321242640184\n",
      "--------------------------------------------------------------\n",
      "Epoch 30: iteration 400, the loss is [ 0.18410546]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6524861269013136, auc for val: 0.6336320377066038\n",
      "--------------------------------------------------------------\n",
      "Epoch 31: iteration 0, the loss is [ 0.13743696]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6525489933088316, auc for val: 0.6337507206101232\n",
      "--------------------------------------------------------------\n",
      "Epoch 31: iteration 100, the loss is [ 0.14711069]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6522512573430298, auc for val: 0.6334370098743534\n",
      "--------------------------------------------------------------\n",
      "Epoch 31: iteration 200, the loss is [ 0.15588121]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6528217345666609, auc for val: 0.6338692857341328\n",
      "--------------------------------------------------------------\n",
      "Epoch 31: iteration 300, the loss is [ 0.1214186]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6529018967261953, auc for val: 0.6338709484299762\n",
      "--------------------------------------------------------------\n",
      "Epoch 31: iteration 400, the loss is [ 0.13872708]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6533324853639778, auc for val: 0.6342006157833524\n",
      "--------------------------------------------------------------\n",
      "Epoch 32: iteration 0, the loss is [ 0.15164171]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6530366465426178, auc for val: 0.6339932862572142\n",
      "--------------------------------------------------------------\n",
      "Epoch 32: iteration 100, the loss is [ 0.12067741]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6529345163408914, auc for val: 0.6337989575841805\n",
      "--------------------------------------------------------------\n",
      "Epoch 32: iteration 200, the loss is [ 0.18278855]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6537747687563614, auc for val: 0.6347810194240564\n",
      "--------------------------------------------------------------\n",
      "Epoch 32: iteration 300, the loss is [ 0.17340234]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6537523958395527, auc for val: 0.6344441950860104\n",
      "--------------------------------------------------------------\n",
      "Epoch 32: iteration 400, the loss is [ 0.14560066]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6533190628970005, auc for val: 0.6339284461312207\n",
      "--------------------------------------------------------------\n",
      "Epoch 33: iteration 0, the loss is [ 0.15282324]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6534647231490599, auc for val: 0.6340081941359959\n",
      "--------------------------------------------------------------\n",
      "Epoch 33: iteration 100, the loss is [ 0.167539]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6535152847074092, auc for val: 0.6339616950361922\n",
      "--------------------------------------------------------------\n",
      "Epoch 33: iteration 200, the loss is [ 0.16867645]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6537423897630247, auc for val: 0.6339806349837179\n",
      "--------------------------------------------------------------\n",
      "Epoch 33: iteration 300, the loss is [ 0.15445772]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6540171584084669, auc for val: 0.6343357640117221\n",
      "--------------------------------------------------------------\n",
      "Epoch 33: iteration 400, the loss is [ 0.12816384]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6540217381757494, auc for val: 0.6342652466627702\n",
      "--------------------------------------------------------------\n",
      "Epoch 34: iteration 0, the loss is [ 0.17336862]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6543961752337503, auc for val: 0.6345241084832882\n",
      "--------------------------------------------------------------\n",
      "Epoch 34: iteration 100, the loss is [ 0.15659039]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6546479471864854, auc for val: 0.6346378947663437\n",
      "--------------------------------------------------------------\n",
      "Epoch 34: iteration 200, the loss is [ 0.17279351]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6548561675234009, auc for val: 0.6348250714666017\n",
      "--------------------------------------------------------------\n",
      "Epoch 34: iteration 300, the loss is [ 0.14513145]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6543158815170192, auc for val: 0.6344273313155848\n",
      "--------------------------------------------------------------\n",
      "Epoch 34: iteration 400, the loss is [ 0.139401]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6543073163860048, auc for val: 0.6342226618522011\n",
      "--------------------------------------------------------------\n",
      "Epoch 35: iteration 0, the loss is [ 0.11815596]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.654561375207851, auc for val: 0.6344673262299159\n",
      "--------------------------------------------------------------\n",
      "Epoch 35: iteration 100, the loss is [ 0.1500929]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6547134086408726, auc for val: 0.6345461169629317\n",
      "--------------------------------------------------------------\n",
      "Epoch 35: iteration 200, the loss is [ 0.13842703]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6549207068406795, auc for val: 0.6346894596380346\n",
      "--------------------------------------------------------------\n",
      "Epoch 35: iteration 300, the loss is [ 0.15832052]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6550046823811234, auc for val: 0.6346050255123448\n",
      "--------------------------------------------------------------\n",
      "Epoch 35: iteration 400, the loss is [ 0.12525477]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6551223722745592, auc for val: 0.6346431071361311\n",
      "--------------------------------------------------------------\n",
      "Epoch 36: iteration 0, the loss is [ 0.13735898]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.655465006037227, auc for val: 0.6349116657186168\n",
      "--------------------------------------------------------------\n",
      "Epoch 36: iteration 100, the loss is [ 0.15141906]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6552380411186255, auc for val: 0.6346902439994497\n",
      "--------------------------------------------------------------\n",
      "Epoch 36: iteration 200, the loss is [ 0.16827235]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6557432691417253, auc for val: 0.6349888175622862\n",
      "--------------------------------------------------------------\n",
      "Epoch 36: iteration 300, the loss is [ 0.17265323]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6561149298172615, auc for val: 0.6355444273632413\n",
      "--------------------------------------------------------------\n",
      "Epoch 36: iteration 400, the loss is [ 0.12214474]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6559203399499186, auc for val: 0.6352090514625689\n",
      "--------------------------------------------------------------\n",
      "Epoch 37: iteration 0, the loss is [ 0.14459829]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.655833054841614, auc for val: 0.6349652127943951\n",
      "--------------------------------------------------------------\n",
      "Epoch 37: iteration 100, the loss is [ 0.14725827]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6552657185267344, auc for val: 0.6344434646024559\n",
      "--------------------------------------------------------------\n",
      "Epoch 37: iteration 200, the loss is [ 0.12199733]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6558527761311378, auc for val: 0.6345964564265332\n",
      "--------------------------------------------------------------\n",
      "Epoch 37: iteration 300, the loss is [ 0.11667851]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6556974802033066, auc for val: 0.6343961648525815\n",
      "--------------------------------------------------------------\n",
      "Epoch 37: iteration 400, the loss is [ 0.14200793]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6562363229453052, auc for val: 0.6349183691268772\n",
      "--------------------------------------------------------------\n",
      "Epoch 38: iteration 0, the loss is [ 0.15161608]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6564593418436613, auc for val: 0.6350328746167254\n",
      "--------------------------------------------------------------\n",
      "Epoch 38: iteration 100, the loss is [ 0.14840356]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6563331723082003, auc for val: 0.6348717898367691\n",
      "--------------------------------------------------------------\n",
      "Epoch 38: iteration 200, the loss is [ 0.15138702]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6565676640453453, auc for val: 0.6350254858319571\n",
      "--------------------------------------------------------------\n",
      "Epoch 38: iteration 300, the loss is [ 0.13404694]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6565521458614001, auc for val: 0.6347057796179582\n",
      "--------------------------------------------------------------\n",
      "Epoch 38: iteration 400, the loss is [ 0.14480501]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6567096521356485, auc for val: 0.6347952995631112\n",
      "--------------------------------------------------------------\n",
      "Epoch 39: iteration 0, the loss is [ 0.15640117]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6566616053139875, auc for val: 0.6347086602040498\n",
      "--------------------------------------------------------------\n",
      "Epoch 39: iteration 100, the loss is [ 0.12120549]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6561760670437861, auc for val: 0.6340087792746234\n",
      "--------------------------------------------------------------\n",
      "Epoch 39: iteration 200, the loss is [ 0.18062577]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6563932462557587, auc for val: 0.6340938035508347\n",
      "--------------------------------------------------------------\n",
      "Epoch 39: iteration 300, the loss is [ 0.13715008]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6571757528686586, auc for val: 0.6347488405584597\n",
      "--------------------------------------------------------------\n",
      "Epoch 39: iteration 400, the loss is [ 0.13674502]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6574096781832777, auc for val: 0.6350412707921932\n",
      "--------------------------------------------------------------\n",
      "Epoch 40: iteration 0, the loss is [ 0.12805067]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6574919885435263, auc for val: 0.6351573349810837\n",
      "--------------------------------------------------------------\n",
      "Epoch 40: iteration 100, the loss is [ 0.12733939]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.657733355264995, auc for val: 0.6350889802644044\n",
      "--------------------------------------------------------------\n",
      "Epoch 40: iteration 200, the loss is [ 0.16350387]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6577144367680957, auc for val: 0.6351034145192004\n",
      "--------------------------------------------------------------\n",
      "Epoch 40: iteration 300, the loss is [ 0.16156903]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6576422041374431, auc for val: 0.6348682188622752\n",
      "--------------------------------------------------------------\n",
      "Epoch 40: iteration 400, the loss is [ 0.14749761]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6570085282122928, auc for val: 0.6344295340430095\n",
      "--------------------------------------------------------------\n",
      "Epoch 41: iteration 0, the loss is [ 0.13185717]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6573572280603038, auc for val: 0.6344786092563428\n",
      "--------------------------------------------------------------\n",
      "Epoch 41: iteration 100, the loss is [ 0.131928]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6573560591181837, auc for val: 0.6347587741324201\n",
      "--------------------------------------------------------------\n",
      "Epoch 41: iteration 200, the loss is [ 0.12908915]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6576425701323474, auc for val: 0.6347571841090403\n",
      "--------------------------------------------------------------\n",
      "Epoch 41: iteration 300, the loss is [ 0.12025452]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6573863998252208, auc for val: 0.6343463478789324\n",
      "--------------------------------------------------------------\n",
      "Epoch 41: iteration 400, the loss is [ 0.1657837]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6580137750727292, auc for val: 0.6348740075998758\n",
      "--------------------------------------------------------------\n",
      "Epoch 42: iteration 0, the loss is [ 0.14923674]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6578327106301699, auc for val: 0.6347509129766397\n",
      "--------------------------------------------------------------\n",
      "Epoch 42: iteration 100, the loss is [ 0.15432025]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6578559365044802, auc for val: 0.6345090740541824\n",
      "--------------------------------------------------------------\n",
      "Epoch 42: iteration 200, the loss is [ 0.13459463]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6580781512961855, auc for val: 0.6346445355259287\n",
      "--------------------------------------------------------------\n",
      "Epoch 42: iteration 300, the loss is [ 0.14135003]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6582543279482956, auc for val: 0.6348955399495867\n",
      "--------------------------------------------------------------\n",
      "Epoch 42: iteration 400, the loss is [ 0.17272034]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6586824720338404, auc for val: 0.6351312869148549\n",
      "--------------------------------------------------------------\n",
      "Epoch 43: iteration 0, the loss is [ 0.15808085]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6586359187187355, auc for val: 0.6350889451811461\n",
      "--------------------------------------------------------------\n",
      "Epoch 43: iteration 100, the loss is [ 0.14713787]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6585067242179985, auc for val: 0.6348279771121634\n",
      "--------------------------------------------------------------\n",
      "Epoch 43: iteration 200, the loss is [ 0.15926798]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6586722624378651, auc for val: 0.6351579978040687\n",
      "--------------------------------------------------------------\n",
      "Epoch 43: iteration 300, the loss is [ 0.19064288]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6592201717277618, auc for val: 0.6356018022730824\n",
      "--------------------------------------------------------------\n",
      "Epoch 43: iteration 400, the loss is [ 0.1388202]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6589269553932267, auc for val: 0.6351234019525778\n",
      "--------------------------------------------------------------\n",
      "Epoch 44: iteration 0, the loss is [ 0.13468476]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6588216802633748, auc for val: 0.6351959190472461\n",
      "--------------------------------------------------------------\n",
      "Epoch 44: iteration 100, the loss is [ 0.18273884]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.659009489292496, auc for val: 0.6353409331890069\n",
      "--------------------------------------------------------------\n",
      "Epoch 44: iteration 200, the loss is [ 0.13728036]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6590817397011483, auc for val: 0.6351292771453503\n",
      "--------------------------------------------------------------\n",
      "Epoch 44: iteration 300, the loss is [ 0.17283112]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6591748454038104, auc for val: 0.6350476145970572\n",
      "--------------------------------------------------------------\n",
      "Epoch 44: iteration 400, the loss is [ 0.15686035]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6593621666200822, auc for val: 0.6351694412111046\n",
      "--------------------------------------------------------------\n",
      "Epoch 45: iteration 0, the loss is [ 0.14657667]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6595294033345694, auc for val: 0.6352476217460229\n",
      "--------------------------------------------------------------\n",
      "Epoch 45: iteration 100, the loss is [ 0.14400983]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6596326601976953, auc for val: 0.6354074209751615\n",
      "--------------------------------------------------------------\n",
      "Epoch 45: iteration 200, the loss is [ 0.17292447]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.659794277441107, auc for val: 0.6353911761736483\n",
      "--------------------------------------------------------------\n",
      "Epoch 45: iteration 300, the loss is [ 0.15273194]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6597591083066798, auc for val: 0.6355157681002248\n",
      "--------------------------------------------------------------\n",
      "Epoch 45: iteration 400, the loss is [ 0.13064499]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6599860099974395, auc for val: 0.6354784307427013\n",
      "--------------------------------------------------------------\n",
      "Epoch 46: iteration 0, the loss is [ 0.17426381]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6598295108855151, auc for val: 0.6352357786404383\n",
      "--------------------------------------------------------------\n",
      "Epoch 46: iteration 100, the loss is [ 0.15886514]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6602541761257251, auc for val: 0.6355639862796796\n",
      "--------------------------------------------------------------\n",
      "Epoch 46: iteration 200, the loss is [ 0.1394805]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6598959094723671, auc for val: 0.6353343939202758\n",
      "--------------------------------------------------------------\n",
      "Epoch 46: iteration 300, the loss is [ 0.17220391]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.660264012229118, auc for val: 0.6354899418103065\n",
      "--------------------------------------------------------------\n",
      "Epoch 46: iteration 400, the loss is [ 0.14530903]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6600710339760846, auc for val: 0.6352393583857467\n",
      "--------------------------------------------------------------\n",
      "Epoch 47: iteration 0, the loss is [ 0.14796503]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6598088167529362, auc for val: 0.6350132016796981\n",
      "--------------------------------------------------------------\n",
      "Epoch 47: iteration 100, the loss is [ 0.17519449]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.659978916266425, auc for val: 0.6352151158543409\n",
      "--------------------------------------------------------------\n",
      "Epoch 47: iteration 200, the loss is [ 0.16136977]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.659986501752367, auc for val: 0.634915878215546\n",
      "--------------------------------------------------------------\n",
      "Epoch 47: iteration 300, the loss is [ 0.15900448]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6601546651644061, auc for val: 0.6349024613752373\n",
      "--------------------------------------------------------------\n",
      "Epoch 47: iteration 400, the loss is [ 0.15860574]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6602771919877534, auc for val: 0.6349451288820577\n",
      "--------------------------------------------------------------\n",
      "Epoch 48: iteration 0, the loss is [ 0.15197082]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6605128240676277, auc for val: 0.6351669427819324\n",
      "--------------------------------------------------------------\n",
      "Epoch 48: iteration 100, the loss is [ 0.16275121]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6604949972171982, auc for val: 0.6352302655570092\n",
      "--------------------------------------------------------------\n",
      "Epoch 48: iteration 200, the loss is [ 0.14570932]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.660920669619732, auc for val: 0.6354678919825373\n",
      "--------------------------------------------------------------\n",
      "Epoch 48: iteration 300, the loss is [ 0.16602191]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6606323953371929, auc for val: 0.6352796226893815\n",
      "--------------------------------------------------------------\n",
      "Epoch 48: iteration 400, the loss is [ 0.15267968]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6603046339533115, auc for val: 0.6347969284286697\n",
      "--------------------------------------------------------------\n",
      "Epoch 49: iteration 0, the loss is [ 0.16676706]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6605872385998982, auc for val: 0.6349573328440121\n",
      "--------------------------------------------------------------\n",
      "Epoch 49: iteration 100, the loss is [ 0.15726909]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6607035174612369, auc for val: 0.6348913951132269\n",
      "--------------------------------------------------------------\n",
      "Epoch 49: iteration 200, the loss is [ 0.1851248]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6608510705291821, auc for val: 0.6349362265052934\n",
      "--------------------------------------------------------------\n",
      "Epoch 49: iteration 300, the loss is [ 0.15030107]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6606384407845973, auc for val: 0.6349018436592986\n",
      "--------------------------------------------------------------\n",
      "Epoch 49: iteration 400, the loss is [ 0.17383251]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6610488664655101, auc for val: 0.6352439292330988\n",
      "--------------------------------------------------------------\n",
      "Epoch 50: iteration 0, the loss is [ 0.18290454]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6611060232752245, auc for val: 0.6350636313573921\n",
      "--------------------------------------------------------------\n",
      "Epoch 50: iteration 100, the loss is [ 0.14288951]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6611598770872086, auc for val: 0.63506834003183\n",
      "--------------------------------------------------------------\n",
      "Epoch 50: iteration 200, the loss is [ 0.11855413]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6616644867451057, auc for val: 0.635108562987339\n",
      "--------------------------------------------------------------\n",
      "Epoch 50: iteration 300, the loss is [ 0.18191589]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6614140909641293, auc for val: 0.6351114849215564\n",
      "--------------------------------------------------------------\n",
      "Epoch 50: iteration 400, the loss is [ 0.13867436]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6610998587152228, auc for val: 0.6348434212636059\n",
      "--------------------------------------------------------------\n",
      "Epoch 51: iteration 0, the loss is [ 0.15093258]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.661309444402506, auc for val: 0.6350084754636314\n",
      "--------------------------------------------------------------\n",
      "Epoch 51: iteration 100, the loss is [ 0.14946704]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6613844298404499, auc for val: 0.6349100243233232\n",
      "--------------------------------------------------------------\n",
      "Epoch 51: iteration 200, the loss is [ 0.13721251]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6613333265168877, auc for val: 0.6347715544621875\n",
      "--------------------------------------------------------------\n",
      "Epoch 51: iteration 300, the loss is [ 0.17163725]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6618190743355972, auc for val: 0.6352485552112852\n",
      "--------------------------------------------------------------\n",
      "Epoch 51: iteration 400, the loss is [ 0.14007276]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6617773380854215, auc for val: 0.6349852478407658\n",
      "--------------------------------------------------------------\n",
      "Epoch 52: iteration 0, the loss is [ 0.15163447]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6617663168851176, auc for val: 0.6349464520220807\n",
      "--------------------------------------------------------------\n",
      "Epoch 52: iteration 100, the loss is [ 0.16360675]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.661408423260568, auc for val: 0.6345932688619323\n",
      "--------------------------------------------------------------\n",
      "Epoch 52: iteration 200, the loss is [ 0.1230457]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6618600098801309, auc for val: 0.6350987459399148\n",
      "--------------------------------------------------------------\n",
      "Epoch 52: iteration 300, the loss is [ 0.13937868]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6619861682850182, auc for val: 0.6352243101739323\n",
      "--------------------------------------------------------------\n",
      "Epoch 52: iteration 400, the loss is [ 0.15979338]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6618619979242577, auc for val: 0.6348625178328201\n",
      "--------------------------------------------------------------\n",
      "Epoch 53: iteration 0, the loss is [ 0.13169563]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6621814985673959, auc for val: 0.634832900045071\n",
      "--------------------------------------------------------------\n",
      "Epoch 53: iteration 100, the loss is [ 0.15829748]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6619521115122952, auc for val: 0.634564672247591\n",
      "--------------------------------------------------------------\n",
      "Epoch 53: iteration 200, the loss is [ 0.16846205]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6623528885317671, auc for val: 0.6349588126057234\n",
      "--------------------------------------------------------------\n",
      "Epoch 53: iteration 300, the loss is [ 0.13931811]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6625844636721376, auc for val: 0.6347409155010304\n",
      "--------------------------------------------------------------\n",
      "Epoch 53: iteration 400, the loss is [ 0.17851749]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6622850764393575, auc for val: 0.6343926352262135\n",
      "--------------------------------------------------------------\n",
      "Epoch 54: iteration 0, the loss is [ 0.15530837]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.662143192543591, auc for val: 0.6343461812334561\n",
      "--------------------------------------------------------------\n",
      "Epoch 54: iteration 100, the loss is [ 0.1504434]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6627223342231472, auc for val: 0.6350117306888015\n",
      "--------------------------------------------------------------\n",
      "Epoch 54: iteration 200, the loss is [ 0.17408456]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6628045456449632, auc for val: 0.6347332873983222\n",
      "--------------------------------------------------------------\n",
      "Epoch 54: iteration 300, the loss is [ 0.11592411]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6623206476657576, auc for val: 0.6344085730492175\n",
      "--------------------------------------------------------------\n",
      "Epoch 54: iteration 400, the loss is [ 0.12637722]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6623887122057612, auc for val: 0.6342284343011461\n",
      "--------------------------------------------------------------\n",
      "Epoch 55: iteration 0, the loss is [ 0.17753069]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6623386359868008, auc for val: 0.6340696863167795\n",
      "--------------------------------------------------------------\n",
      "Epoch 55: iteration 100, the loss is [ 0.14746629]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6625638022356188, auc for val: 0.6343022093812152\n",
      "--------------------------------------------------------------\n",
      "Epoch 55: iteration 200, the loss is [ 0.14771758]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6627831369913926, auc for val: 0.6344981706787279\n",
      "--------------------------------------------------------------\n",
      "Epoch 55: iteration 300, the loss is [ 0.14464614]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6631033168314102, auc for val: 0.6344880742182116\n",
      "--------------------------------------------------------------\n",
      "Epoch 55: iteration 400, the loss is [ 0.17558801]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6630028109977757, auc for val: 0.6341906358493723\n",
      "--------------------------------------------------------------\n",
      "Epoch 56: iteration 0, the loss is [ 0.15343197]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.663233924605818, auc for val: 0.6346861079339045\n",
      "--------------------------------------------------------------\n",
      "Epoch 56: iteration 100, the loss is [ 0.16128986]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6633058897935205, auc for val: 0.6345179977314964\n",
      "--------------------------------------------------------------\n",
      "Epoch 56: iteration 200, the loss is [ 0.1497016]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6631789992442332, auc for val: 0.6341087615485566\n",
      "--------------------------------------------------------------\n",
      "Epoch 56: iteration 300, the loss is [ 0.16717288]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6633893026989255, auc for val: 0.6343205692020073\n",
      "--------------------------------------------------------------\n",
      "Epoch 56: iteration 400, the loss is [ 0.14175165]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6637467766853894, auc for val: 0.6347261379314935\n",
      "--------------------------------------------------------------\n",
      "Epoch 57: iteration 0, the loss is [ 0.18589948]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6638868456019902, auc for val: 0.6348811119596581\n",
      "--------------------------------------------------------------\n",
      "Epoch 57: iteration 100, the loss is [ 0.16384074]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6639123244126209, auc for val: 0.6347959648920433\n",
      "--------------------------------------------------------------\n",
      "Epoch 57: iteration 200, the loss is [ 0.15932071]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6635257953776721, auc for val: 0.6340455452762277\n",
      "--------------------------------------------------------------\n",
      "Epoch 57: iteration 300, the loss is [ 0.16196261]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.663946112814846, auc for val: 0.6343207045231462\n",
      "--------------------------------------------------------------\n",
      "Epoch 57: iteration 400, the loss is [ 0.14321655]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6643384794491956, auc for val: 0.6350403598804539\n",
      "--------------------------------------------------------------\n",
      "Epoch 58: iteration 0, the loss is [ 0.1587327]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6641962110848643, auc for val: 0.6348843709437488\n",
      "--------------------------------------------------------------\n",
      "Epoch 58: iteration 100, the loss is [ 0.14931785]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6644394295009326, auc for val: 0.6348776036338397\n",
      "--------------------------------------------------------------\n",
      "Epoch 58: iteration 200, the loss is [ 0.12924281]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6642741938162415, auc for val: 0.6348829124825872\n",
      "--------------------------------------------------------------\n",
      "Epoch 58: iteration 300, the loss is [ 0.15369336]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6639143498678426, auc for val: 0.634473153809695\n",
      "--------------------------------------------------------------\n",
      "Epoch 58: iteration 400, the loss is [ 0.13707678]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6643585716805454, auc for val: 0.6346810797512223\n",
      "--------------------------------------------------------------\n",
      "Epoch 59: iteration 0, the loss is [ 0.14036013]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6644657724769342, auc for val: 0.6348291173180545\n",
      "--------------------------------------------------------------\n",
      "Epoch 59: iteration 100, the loss is [ 0.14638147]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6651331328416596, auc for val: 0.6351532415166377\n",
      "--------------------------------------------------------------\n",
      "Epoch 59: iteration 200, the loss is [ 0.14300735]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.664912616699166, auc for val: 0.6348700156262836\n",
      "--------------------------------------------------------------\n",
      "Epoch 59: iteration 300, the loss is [ 0.18748599]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6640851142785156, auc for val: 0.6344827002148419\n",
      "--------------------------------------------------------------\n",
      "Epoch 59: iteration 400, the loss is [ 0.14957976]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6647919835419566, auc for val: 0.6348800318964956\n",
      "--------------------------------------------------------------\n",
      "Epoch 60: iteration 0, the loss is [ 0.11319157]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6646764243804182, auc for val: 0.6348418851180868\n",
      "--------------------------------------------------------------\n",
      "Epoch 60: iteration 100, the loss is [ 0.15139657]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6647050250850706, auc for val: 0.6349055098597788\n",
      "--------------------------------------------------------------\n",
      "Epoch 60: iteration 200, the loss is [ 0.14325242]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6647564756206942, auc for val: 0.634805399782548\n",
      "--------------------------------------------------------------\n",
      "Epoch 60: iteration 300, the loss is [ 0.1438643]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.664726535691604, auc for val: 0.6347173520812652\n",
      "--------------------------------------------------------------\n",
      "Epoch 60: iteration 400, the loss is [ 0.11620539]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6648635637200248, auc for val: 0.6348957015831691\n",
      "--------------------------------------------------------------\n",
      "Epoch 61: iteration 0, the loss is [ 0.14825115]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6645930784130463, auc for val: 0.6347674171436687\n",
      "--------------------------------------------------------------\n",
      "Epoch 61: iteration 100, the loss is [ 0.16153833]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6647925554524735, auc for val: 0.6348014453981612\n",
      "--------------------------------------------------------------\n",
      "Epoch 61: iteration 200, the loss is [ 0.11324553]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6648380638849771, auc for val: 0.6348587275879626\n",
      "--------------------------------------------------------------\n",
      "Epoch 61: iteration 300, the loss is [ 0.11744325]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6648868315658731, auc for val: 0.6349057115885134\n",
      "--------------------------------------------------------------\n",
      "Epoch 61: iteration 400, the loss is [ 0.14235155]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6649862735021763, auc for val: 0.6349820815767147\n",
      "--------------------------------------------------------------\n",
      "Epoch 62: iteration 0, the loss is [ 0.16556248]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6648559718185438, auc for val: 0.6347760551430233\n",
      "--------------------------------------------------------------\n",
      "Epoch 62: iteration 100, the loss is [ 0.13003139]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6648200372252914, auc for val: 0.6348242006500147\n",
      "--------------------------------------------------------------\n",
      "Epoch 62: iteration 200, the loss is [ 0.11932897]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.664906976358265, auc for val: 0.6348722997969862\n",
      "--------------------------------------------------------------\n",
      "Epoch 62: iteration 300, the loss is [ 0.14890839]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6648638672600437, auc for val: 0.6347849750614167\n",
      "--------------------------------------------------------------\n",
      "Epoch 62: iteration 400, the loss is [ 0.12815721]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6650850387923564, auc for val: 0.6349962126119221\n",
      "--------------------------------------------------------------\n",
      "Epoch 63: iteration 0, the loss is [ 0.13658714]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.665073115319926, auc for val: 0.6348855637745272\n",
      "--------------------------------------------------------------\n",
      "Epoch 63: iteration 100, the loss is [ 0.167189]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6649905958749434, auc for val: 0.6349867739624968\n",
      "--------------------------------------------------------------\n",
      "Epoch 63: iteration 200, the loss is [ 0.15503065]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6650540363572568, auc for val: 0.6349074143795089\n",
      "--------------------------------------------------------------\n",
      "Epoch 63: iteration 300, the loss is [ 0.17574918]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6647558734875778, auc for val: 0.6346700473194968\n",
      "--------------------------------------------------------------\n",
      "Epoch 63: iteration 400, the loss is [ 0.16191101]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6649578477948763, auc for val: 0.6349318185444971\n",
      "--------------------------------------------------------------\n",
      "Epoch 64: iteration 0, the loss is [ 0.16561857]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6649489935781298, auc for val: 0.6349059083053539\n",
      "--------------------------------------------------------------\n",
      "Epoch 64: iteration 100, the loss is [ 0.14629951]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.665092210514682, auc for val: 0.6349670571713968\n",
      "--------------------------------------------------------------\n",
      "Epoch 64: iteration 200, the loss is [ 0.16787753]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6650720137023175, auc for val: 0.6349197248442113\n",
      "--------------------------------------------------------------\n",
      "Epoch 64: iteration 300, the loss is [ 0.17268896]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6650657931802648, auc for val: 0.6348555412763354\n",
      "--------------------------------------------------------------\n",
      "Epoch 64: iteration 400, the loss is [ 0.1449488]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6652005647940609, auc for val: 0.6350203361108447\n",
      "--------------------------------------------------------------\n",
      "Epoch 65: iteration 0, the loss is [ 0.13254686]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6650293560106939, auc for val: 0.6349046979329466\n",
      "--------------------------------------------------------------\n",
      "Epoch 65: iteration 100, the loss is [ 0.1226358]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6651884483916873, auc for val: 0.6349921454599197\n",
      "--------------------------------------------------------------\n",
      "Epoch 65: iteration 200, the loss is [ 0.15822001]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6651260871112442, auc for val: 0.6349624537467337\n",
      "--------------------------------------------------------------\n",
      "Epoch 65: iteration 300, the loss is [ 0.16167544]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6651401264748069, auc for val: 0.634988721083326\n",
      "--------------------------------------------------------------\n",
      "Epoch 65: iteration 400, the loss is [ 0.1403264]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6650657240779535, auc for val: 0.6348644223525501\n",
      "--------------------------------------------------------------\n",
      "Epoch 66: iteration 0, the loss is [ 0.16077609]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6651612694632274, auc for val: 0.6348451616438067\n",
      "--------------------------------------------------------------\n",
      "Epoch 66: iteration 100, the loss is [ 0.14042361]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6651862704321476, auc for val: 0.6349584442315125\n",
      "--------------------------------------------------------------\n",
      "Epoch 66: iteration 200, the loss is [ 0.14873977]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6651047307868287, auc for val: 0.6348975459601709\n",
      "--------------------------------------------------------------\n",
      "Epoch 66: iteration 300, the loss is [ 0.14453503]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6650873251976934, auc for val: 0.6348814878517102\n",
      "--------------------------------------------------------------\n",
      "Epoch 66: iteration 400, the loss is [ 0.12113326]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6651727698965634, auc for val: 0.6348908613465131\n",
      "--------------------------------------------------------------\n",
      "Epoch 67: iteration 0, the loss is [ 0.15729558]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6652424780512759, auc for val: 0.6349371787651583\n",
      "--------------------------------------------------------------\n",
      "Epoch 67: iteration 100, the loss is [ 0.19274545]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6652014535394486, auc for val: 0.6348511257976981\n",
      "--------------------------------------------------------------\n",
      "Epoch 67: iteration 200, the loss is [ 0.15643083]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6651551042848605, auc for val: 0.6348596259699668\n",
      "--------------------------------------------------------------\n",
      "Epoch 67: iteration 300, the loss is [ 0.15099595]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6651118599965467, auc for val: 0.6347617562093658\n",
      "--------------------------------------------------------------\n",
      "Epoch 67: iteration 400, the loss is [ 0.17724574]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6652693171880569, auc for val: 0.6349855347716987\n",
      "--------------------------------------------------------------\n",
      "Epoch 68: iteration 0, the loss is [ 0.15273753]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6653361376545684, auc for val: 0.6349078491613156\n",
      "--------------------------------------------------------------\n",
      "Epoch 68: iteration 100, the loss is [ 0.14030449]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6654183549508539, auc for val: 0.63497958941241\n",
      "--------------------------------------------------------------\n",
      "Epoch 68: iteration 200, the loss is [ 0.14902261]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6652494426212585, auc for val: 0.6349388677734452\n",
      "--------------------------------------------------------------\n",
      "Epoch 68: iteration 300, the loss is [ 0.16701064]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6652923781134493, auc for val: 0.6348812999056841\n",
      "--------------------------------------------------------------\n",
      "Epoch 68: iteration 400, the loss is [ 0.17315805]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6652808579697225, auc for val: 0.6348399592978072\n",
      "--------------------------------------------------------------\n",
      "Epoch 69: iteration 0, the loss is [ 0.16832104]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.665436859220363, auc for val: 0.6349681434994271\n",
      "--------------------------------------------------------------\n",
      "Epoch 69: iteration 100, the loss is [ 0.14585838]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6652745974858187, auc for val: 0.6347947758201854\n",
      "--------------------------------------------------------------\n",
      "Epoch 69: iteration 200, the loss is [ 0.15451501]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.665416918256601, auc for val: 0.6349176687146869\n",
      "--------------------------------------------------------------\n",
      "Epoch 69: iteration 300, the loss is [ 0.16481081]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.66523148869678, auc for val: 0.634932745744892\n",
      "--------------------------------------------------------------\n",
      "Epoch 69: iteration 400, the loss is [ 0.17883734]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6654113169500034, auc for val: 0.6349493012838348\n",
      "--------------------------------------------------------------\n",
      "Epoch 70: iteration 0, the loss is [ 0.17301202]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6653616902825452, auc for val: 0.6348273706729863\n",
      "--------------------------------------------------------------\n",
      "Epoch 70: iteration 100, the loss is [ 0.13609441]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6653949230836356, auc for val: 0.6349891320586363\n",
      "--------------------------------------------------------------\n",
      "Epoch 70: iteration 200, the loss is [ 0.15097551]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6654368420607286, auc for val: 0.635003226757612\n",
      "--------------------------------------------------------------\n",
      "Epoch 70: iteration 300, the loss is [ 0.13358028]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6654548149999373, auc for val: 0.6349339085043061\n",
      "--------------------------------------------------------------\n",
      "Epoch 70: iteration 400, the loss is [ 0.14231737]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6655145189332176, auc for val: 0.6349153745201963\n",
      "--------------------------------------------------------------\n",
      "Epoch 71: iteration 0, the loss is [ 0.11813228]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6653676550332908, auc for val: 0.6349055236424874\n",
      "--------------------------------------------------------------\n",
      "Epoch 71: iteration 100, the loss is [ 0.18390839]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6655600182448345, auc for val: 0.6349361701214855\n",
      "--------------------------------------------------------------\n",
      "Epoch 71: iteration 200, the loss is [ 0.13800529]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6655741767982901, auc for val: 0.6351366521474101\n",
      "--------------------------------------------------------------\n",
      "Epoch 71: iteration 300, the loss is [ 0.17814086]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6655748272411877, auc for val: 0.6349495656612445\n",
      "--------------------------------------------------------------\n",
      "Epoch 71: iteration 400, the loss is [ 0.1413094]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6656484464012161, auc for val: 0.6349782963437511\n",
      "--------------------------------------------------------------\n",
      "Epoch 72: iteration 0, the loss is [ 0.1659932]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6656199644999784, auc for val: 0.6349806030679768\n",
      "--------------------------------------------------------------\n",
      "Epoch 72: iteration 100, the loss is [ 0.1684428]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6656351648442093, auc for val: 0.6350517017966357\n",
      "--------------------------------------------------------------\n",
      "Epoch 72: iteration 200, the loss is [ 0.13797933]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6656302718749515, auc for val: 0.635008247422453\n",
      "--------------------------------------------------------------\n",
      "Epoch 72: iteration 300, the loss is [ 0.13277356]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6656274122450714, auc for val: 0.6349295970224698\n",
      "--------------------------------------------------------------\n",
      "Epoch 72: iteration 400, the loss is [ 0.16537061]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6656624876971701, auc for val: 0.6350160471825317\n",
      "--------------------------------------------------------------\n",
      "Epoch 73: iteration 0, the loss is [ 0.15646537]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6657088306135149, auc for val: 0.635008521823651\n",
      "--------------------------------------------------------------\n",
      "Epoch 73: iteration 100, the loss is [ 0.19060238]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6655743388099732, auc for val: 0.6348522847981917\n",
      "--------------------------------------------------------------\n",
      "Epoch 73: iteration 200, the loss is [ 0.16306312]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6655801423219921, auc for val: 0.6348705080448718\n",
      "--------------------------------------------------------------\n",
      "Epoch 73: iteration 300, the loss is [ 0.15904258]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6657031291317544, auc for val: 0.6350230563163276\n",
      "--------------------------------------------------------------\n",
      "Epoch 73: iteration 400, the loss is [ 0.18205537]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6657656708975411, auc for val: 0.6348608401012947\n",
      "--------------------------------------------------------------\n",
      "Epoch 74: iteration 0, the loss is [ 0.1536527]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6656607236558382, auc for val: 0.6348604541854547\n",
      "--------------------------------------------------------------\n",
      "Epoch 74: iteration 100, the loss is [ 0.13777389]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.665841138505752, auc for val: 0.6350036176853462\n",
      "--------------------------------------------------------------\n",
      "Epoch 74: iteration 200, the loss is [ 0.15707913]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6656331255530663, auc for val: 0.6348820955438608\n",
      "--------------------------------------------------------------\n",
      "Epoch 74: iteration 300, the loss is [ 0.15348805]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6658601772746833, auc for val: 0.6349990405731265\n",
      "--------------------------------------------------------------\n",
      "Epoch 74: iteration 400, the loss is [ 0.16732743]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.665869846032993, auc for val: 0.6349650160775546\n",
      "--------------------------------------------------------------\n",
      "Epoch 75: iteration 0, the loss is [ 0.14748378]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.665722838981522, auc for val: 0.6349879191802819\n",
      "--------------------------------------------------------------\n",
      "Epoch 75: iteration 100, the loss is [ 0.16767153]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6658388071143464, auc for val: 0.6349075271471245\n",
      "--------------------------------------------------------------\n",
      "Epoch 75: iteration 200, the loss is [ 0.17082788]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6658589786510334, auc for val: 0.6349089054179817\n",
      "--------------------------------------------------------------\n",
      "Epoch 75: iteration 300, the loss is [ 0.13648954]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6657660300131327, auc for val: 0.634929345174795\n",
      "--------------------------------------------------------------\n",
      "Epoch 75: iteration 400, the loss is [ 0.13671337]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6659785817664063, auc for val: 0.6350485154850084\n",
      "--------------------------------------------------------------\n",
      "Epoch 76: iteration 0, the loss is [ 0.15425931]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6658826725505064, auc for val: 0.6349890781807755\n",
      "--------------------------------------------------------------\n",
      "Epoch 76: iteration 100, the loss is [ 0.14666693]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6658713165054452, auc for val: 0.6349396771943305\n",
      "--------------------------------------------------------------\n",
      "Epoch 76: iteration 200, the loss is [ 0.13050027]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6656819966555425, auc for val: 0.6348504729985012\n",
      "--------------------------------------------------------------\n",
      "Epoch 76: iteration 300, the loss is [ 0.15261956]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6657900768445452, auc for val: 0.6349159822123471\n",
      "--------------------------------------------------------------\n",
      "Epoch 76: iteration 400, the loss is [ 0.16681609]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6658023756646536, auc for val: 0.6348039400684128\n",
      "--------------------------------------------------------------\n",
      "Epoch 77: iteration 0, the loss is [ 0.15316153]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6658898067844417, auc for val: 0.6350020439506219\n",
      "--------------------------------------------------------------\n",
      "Epoch 77: iteration 100, the loss is [ 0.11443379]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6657366246607602, auc for val: 0.6348193867258023\n",
      "--------------------------------------------------------------\n",
      "Epoch 77: iteration 200, the loss is [ 0.14515038]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6658239211315254, auc for val: 0.6348446892727946\n",
      "--------------------------------------------------------------\n",
      "Epoch 77: iteration 300, the loss is [ 0.20660317]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6658542948438053, auc for val: 0.6349153043536799\n",
      "--------------------------------------------------------------\n",
      "Epoch 77: iteration 400, the loss is [ 0.18324302]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6658758168127994, auc for val: 0.634863929933962\n",
      "--------------------------------------------------------------\n",
      "Epoch 78: iteration 0, the loss is [ 0.14407592]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6658430258336463, auc for val: 0.6349169307132916\n",
      "--------------------------------------------------------------\n",
      "Epoch 78: iteration 100, the loss is [ 0.17466654]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.665982048321733, auc for val: 0.634901834888484\n",
      "--------------------------------------------------------------\n",
      "Epoch 78: iteration 200, the loss is [ 0.15614462]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6659099174121561, auc for val: 0.6348531919510104\n",
      "--------------------------------------------------------------\n",
      "Epoch 78: iteration 300, the loss is [ 0.1427035]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6660327371087225, auc for val: 0.6350086508799222\n",
      "--------------------------------------------------------------\n",
      "Epoch 78: iteration 400, the loss is [ 0.13873269]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6659455827073626, auc for val: 0.6348690646193923\n",
      "--------------------------------------------------------------\n",
      "Epoch 79: iteration 0, the loss is [ 0.15731195]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6660185111534597, auc for val: 0.6349631591708178\n",
      "--------------------------------------------------------------\n",
      "Epoch 79: iteration 100, the loss is [ 0.17357309]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.665963514834468, auc for val: 0.6348629087605542\n",
      "--------------------------------------------------------------\n",
      "Epoch 79: iteration 200, the loss is [ 0.13935228]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.666127789811521, auc for val: 0.6349534523850622\n",
      "--------------------------------------------------------------\n",
      "Epoch 79: iteration 300, the loss is [ 0.14641897]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6659603260797088, auc for val: 0.6348726781949852\n",
      "--------------------------------------------------------------\n",
      "Epoch 79: iteration 400, the loss is [ 0.16289262]\n",
      "  acc for train: 0.9634420093710915, acc for val: 0.9639343650245478\n",
      "  auc for train: 0.6660520331948737, auc for val: 0.6349203287774414\n",
      "--------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from MY_NN import NeuralNetwork\n",
    "#choose the best among finer search\n",
    "lr =0.00015493820527177172\n",
    "wd = 0.0008104257925743996\n",
    "nn_model = NeuralNetwork(data,learning_rate = lr,num_epochs=80,verbose=True,\n",
    "                             weight_decay=wd,batchnorm=True)\n",
    "nn_model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most expensive search final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_size = [230,220,210,200]\n",
    "from MY_NN import NeuralNetwork\n",
    "from datetime import datetime\n",
    "train_hist={}\n",
    "best_net = None\n",
    "best_auc =0\n",
    "\n",
    "#10.22. for 4 layers, best one has lr 3.305e-4, wd is 9.904e-3 0.6374\n",
    "\n",
    "for i in range(5):\n",
    "    #learnning_rate 5e-4 too large\n",
    "    tic = datetime.now()\n",
    "    weight_decay = 10** (np.random.uniform(-3,-1))#L2 \n",
    "    learning_rate = 10** (np.random.uniform(-4,np.log10(5e-4)))\n",
    "    dropout = np.random.uniform(0,1)\n",
    "    nn_model = NeuralNetwork(data,hidden_size=hidden_size,learning_rate = learning_rate,num_epochs=55,verbose=None,dropout=dropout,\n",
    "                             weight_decay=weight_decay,batchnorm=True)\n",
    "    print('Learning rate is {}. Weight decay is {}. dropout is {}'.format(learning_rate, weight_decay,dropout))\n",
    "    describe= 'Learning rate is {}. Weight decay is {}. dropout is {}'.format(learning_rate, weight_decay, dropout)\n",
    "    nn_model.train()\n",
    "    print('Val aus is {}. Train auc is {}'.format(nn_model.auc_history['val'][-1],nn_model.auc_history['train'][-1]))\n",
    "    if nn_model.auc_history['val'][-1]> best_auc:\n",
    "        best_auc =nn_model.auc_history['val'][-1]\n",
    "        best_net = nn_model\n",
    "    train_hist[(nn_model.auc_history['val'][-1],nn_model.auc_history['train'][-1])]= describe\n",
    "    toc = datetime.now()\n",
    "    print('This is round you consume {} time to run this model.'.format(toc-tic))\n",
    "    print('You have finished {}!!'.format(i+1))\n",
    "\n",
    "train_hist['best_net'] = best_net\n",
    "filename= 'search_lr_wd2.pkl'\n",
    "with open(filename, 'wb') as f:\n",
    "    pickle.dump(train_hist, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3oAAALJCAYAAADielEXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc83e40f278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nn_model= best_net\n",
    "\n",
    "data_vis.nnplot(nn_model, loss=False, acc=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New processing: preprocess cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/deprecation.py:75: DeprecationWarning: Function _ratio_float is deprecated; Use a float for 'ratio' is deprecated from version 0.2. The support will be removed in 0.4. Use a dict, str, or a callable instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import data_util\n",
    "import data_preprocess\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "import numpy as np \n",
    "\n",
    "rso = np.random.RandomState(66)\n",
    "\n",
    "train_data = data_util.load_train_data()\n",
    "test_data= data_util.load_test_data()\n",
    "\n",
    "lv1_pre = data_preprocess.preprocess_cell()\n",
    "\n",
    "X_train = train_data.drop(['id','target'],axis=1)\n",
    "y_train = train_data['target']\n",
    "y_train\n",
    "\n",
    "X_train, X_train_test, y_train, y_train_test = train_test_split(X_train,y_train,test_size =0.1 ,random_state=rso)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train,y_train, test_size =0.2, random_state=rso)\n",
    "\n",
    "X_train, y_train = lv1_pre.process(X_train, y=y_train, rso =rso)\n",
    "X_val = lv1_pre.process(X_val,test=True,rso =rso)\n",
    "X_train_test = lv1_pre.process(X_train_test,test=True,rso =rso)\n",
    "X_dev, y_dev = X_train[:10000,:], y_train[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data ={}\n",
    "#enter df here\n",
    "data['X_train'] = X_train\n",
    "data['X_val'] = X_val\n",
    "data['y_train'] = y_train\n",
    "data['y_val'] =y_val\n",
    "X_train = None\n",
    "X_val = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MY_NN import NeuralNetwork\n",
    "from datetime import datetime\n",
    "#only use step decay for now\n",
    "#coarse search\n",
    "best_net = None\n",
    "best_auc =0\n",
    "\n",
    "#10.22. for 4 layers, best one has lr 3.305e-4, wd is 9.904e-3 0.6374\n",
    "input_size = 237\n",
    "hidden_size= [230,220,210]\n",
    "lr_decay = {'step_size': 25, 'gamma':0.1}\n",
    "\n",
    "train_hist={}\n",
    "for i in range(100):\n",
    "    #learnning_rate 5e-4 too large\n",
    "    tic = datetime.now()\n",
    "    dropout = np.random.uniform(0,1)\n",
    "    weight_decay = 10** (np.random.uniform(-3,3))#L2 \n",
    "    learning_rate = 10** (np.random.uniform(-6,-2))\n",
    "    \n",
    "    nn_model = NeuralNetwork(data,input_size = input_size, hidden_size=hidden_size,\n",
    "                             learning_rate = learning_rate,num_epochs=5,\n",
    "                             verbose=None,dropout=dropout,\n",
    "                             weight_decay=weight_decay,lr_decay=lr_decay ,batchnorm=True)\n",
    "    nn_model.train()\n",
    "    describe= 'Learning rate is {}. Weight decay is {}. dropout is {}\\n Val aus is {}. Train auc is {}' \\\n",
    "                .format(learning_rate, weight_decay, dropout,nn_model.auc_history['val'][-1],nn_model.auc_history['train'][-1])\n",
    "    train_hist[(nn_model.auc_history['val'][-1],nn_model.auc_history['train'][-1])]= describe\n",
    "    print(describe)\n",
    "    toc = datetime.now()\n",
    "    print('This is round you consume {} time to run this model.'.format(toc-tic))\n",
    "    print('You have finished {}!!'.format(i+1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate is 0.0001697748323692106. Weight decay is 0.01611014195754915. dropout is 0.8633795643083599\n",
      " Val aus is 0.6003277112111378. Train auc is 0.6346519767601385\n",
      "Learning rate is 0.00011292573215690098. Weight decay is 0.0025168409474830154. dropout is 0.7474963413998843\n",
      " Val aus is 0.5997970907137983. Train auc is 0.633021357700171\n",
      "Learning rate is 0.0004576885112175419. Weight decay is 0.7826435311885301. dropout is 0.6981187592989551\n",
      " Val aus is 0.5951053802842754. Train auc is 0.6484250603014419\n",
      "Learning rate is 0.0035761607375581514. Weight decay is 0.00729795832118471. dropout is 0.8336651558501216\n",
      " Val aus is 0.5949200266604695. Train auc is 0.6517912182364991\n",
      "Learning rate is 6.0618015129868076e-05. Weight decay is 0.00496681831649462. dropout is 0.5197983413792601\n",
      " Val aus is 0.593480985335158. Train auc is 0.657112285152382\n",
      "Learning rate is 2.520241184671594e-05. Weight decay is 0.008209244622133769. dropout is 0.39878113804746995\n",
      " Val aus is 0.5896564177744055. Train auc is 0.6488002163005206\n",
      "Learning rate is 0.0003364093995809248. Weight decay is 40.38236910619775. dropout is 0.6260098224546417\n",
      " Val aus is 0.5890169752750437. Train auc is 0.6344304459205093\n",
      "Learning rate is 0.00015379848171195436. Weight decay is 15.147496390606072. dropout is 0.6937951813450229\n",
      " Val aus is 0.5889955694756569. Train auc is 0.6605906989586742\n",
      "Learning rate is 0.00035241178980443123. Weight decay is 48.06564546712159. dropout is 0.08535113360113433\n",
      " Val aus is 0.586640694731108. Train auc is 0.64094130348404\n",
      "Learning rate is 7.722564749536545e-05. Weight decay is 0.00275276739393203. dropout is 0.8542123583365075\n",
      " Val aus is 0.583571184041119. Train auc is 0.5570400951185741\n",
      "Learning rate is 3.1368595238183194e-05. Weight decay is 0.003912627924794611. dropout is 0.4207448172906385\n",
      " Val aus is 0.5835634519416097. Train auc is 0.6510281360933134\n",
      "Learning rate is 5.018100322682305e-05. Weight decay is 0.025056925665561048. dropout is 0.23123493479657753\n",
      " Val aus is 0.5834165007028079. Train auc is 0.7817011446319728\n",
      "Learning rate is 0.003771630228931296. Weight decay is 0.04266050550835299. dropout is 0.7488935611820214\n",
      " Val aus is 0.5823809732304315. Train auc is 0.6247156694421353\n",
      "Learning rate is 0.0031052414349108793. Weight decay is 0.010860049501368147. dropout is 0.6680190203441491\n",
      " Val aus is 0.5820469593119607. Train auc is 0.7563138393925822\n",
      "Learning rate is 0.0004510417532794107. Weight decay is 2.403802912643618. dropout is 0.21029939040229606\n",
      " Val aus is 0.5818580434844756. Train auc is 0.6821845542620208\n",
      "Learning rate is 0.0017781294971446228. Weight decay is 0.006916546689426339. dropout is 0.7247146158702842\n",
      " Val aus is 0.5816502115159612. Train auc is 0.8445689332178898\n",
      "Learning rate is 3.1254982574072496e-05. Weight decay is 0.015781545267140793. dropout is 0.32576011853108977\n",
      " Val aus is 0.5811370173741517. Train auc is 0.6603426075210468\n",
      "Learning rate is 0.007901117497878645. Weight decay is 0.0051825762071712874. dropout is 0.8387346508697067\n",
      " Val aus is 0.5800359969765849. Train auc is 0.6274128972267646\n",
      "Learning rate is 0.00018762062985101246. Weight decay is 52.9263831683967. dropout is 0.49545084875952916\n",
      " Val aus is 0.577188283897815. Train auc is 0.687257267489177\n",
      "Learning rate is 1.755351409528121e-05. Weight decay is 0.02325101164006467. dropout is 0.1896390014783761\n",
      " Val aus is 0.5770486136880599. Train auc is 0.6624212884678343\n",
      "Learning rate is 3.626401736980522e-05. Weight decay is 0.1637147012998134. dropout is 0.14752682467927547\n",
      " Val aus is 0.5769620382306475. Train auc is 0.6667046529898075\n",
      "Learning rate is 0.0024109731209465504. Weight decay is 0.0063640841849510865. dropout is 0.245604749550976\n",
      " Val aus is 0.5762202490963455. Train auc is 0.9393720760032822\n",
      "Learning rate is 0.00014018087473032192. Weight decay is 14.6918419459913. dropout is 0.5003159200678932\n",
      " Val aus is 0.5757088241531272. Train auc is 0.6422380842886335\n",
      "Learning rate is 0.00011471675580587781. Weight decay is 4.587713042352929. dropout is 0.5849028328049316\n",
      " Val aus is 0.5719758489557077. Train auc is 0.6111850846564661\n",
      "Learning rate is 0.0022554809362409756. Weight decay is 0.04356481033818433. dropout is 0.3340528920959568\n",
      " Val aus is 0.5712700740389062. Train auc is 0.8319610180890069\n",
      "Learning rate is 3.335870719713195e-05. Weight decay is 0.01587646065075224. dropout is 0.8099788988197253\n",
      " Val aus is 0.5699475655145766. Train auc is 0.5468426709180226\n",
      "Learning rate is 0.002640403321308396. Weight decay is 0.0012651477012070903. dropout is 0.6581287746898374\n",
      " Val aus is 0.5682888403443517. Train auc is 0.9261186453441255\n",
      "Learning rate is 0.0007943630083064011. Weight decay is 0.33291362749871434. dropout is 0.09702959304623904\n",
      " Val aus is 0.5664802921200476. Train auc is 0.8338499615650166\n",
      "Learning rate is 9.500653825094759e-05. Weight decay is 0.027304774063278078. dropout is 0.40460175213793337\n",
      " Val aus is 0.562718097580454. Train auc is 0.9111976494945541\n",
      "Learning rate is 1.1521254754982775e-05. Weight decay is 0.05917321633171188. dropout is 0.11764138557249781\n",
      " Val aus is 0.5620613815883302. Train auc is 0.6049268652614097\n",
      "Learning rate is 0.0007901117645121056. Weight decay is 0.6222540364493497. dropout is 0.11792882402766114\n",
      " Val aus is 0.5608915654274096. Train auc is 0.7249714904445085\n",
      "Learning rate is 0.00046508036211673475. Weight decay is 1.041346742955071. dropout is 0.22814998819016674\n",
      " Val aus is 0.5605394736071856. Train auc is 0.7469529481740194\n",
      "Learning rate is 0.00017749913020671736. Weight decay is 6.768472084172184. dropout is 0.2879182058272358\n",
      " Val aus is 0.560059356712977. Train auc is 0.7403106334091154\n",
      "Learning rate is 0.0013017991503556205. Weight decay is 0.022202257863861714. dropout is 0.14153258013013204\n",
      " Val aus is 0.5547908086053498. Train auc is 0.9395704652086299\n",
      "Learning rate is 7.018288227782286e-05. Weight decay is 0.25194005625624305. dropout is 0.07379514287915756\n",
      " Val aus is 0.5539171239619014. Train auc is 0.8967470169412963\n",
      "Learning rate is 2.4896649339302976e-05. Weight decay is 0.16109732359234818. dropout is 0.44216334256440226\n",
      " Val aus is 0.5538093644814126. Train auc is 0.5908349343205337\n",
      "Learning rate is 3.0384377520841735e-05. Weight decay is 0.13953059385994274. dropout is 0.8427163563596116\n",
      " Val aus is 0.5528732967930172. Train auc is 0.5463319299244515\n",
      "Learning rate is 0.0001087160828936583. Weight decay is 0.12490998006041626. dropout is 0.10900698885737004\n",
      " Val aus is 0.5526972402326304. Train auc is 0.950426356771988\n",
      "Learning rate is 7.16680617528309e-05. Weight decay is 1.5491820848850462. dropout is 0.2388698390232401\n",
      " Val aus is 0.5520245889234496. Train auc is 0.6170653573926642\n",
      "Learning rate is 0.0005248998615901393. Weight decay is 0.008297491466537639. dropout is 0.40495918079861837\n",
      " Val aus is 0.5502700776875651. Train auc is 0.9642572552478692\n",
      "Learning rate is 0.00015878158909487215. Weight decay is 0.055189645529418276. dropout is 0.169195675032499\n",
      " Val aus is 0.5496702203801975. Train auc is 0.956207451617736\n",
      "Learning rate is 0.00016336042051423588. Weight decay is 444.0930476340161. dropout is 0.3105727888888956\n",
      " Val aus is 0.5469457748350075. Train auc is 0.5563126216264027\n",
      "Learning rate is 2.8135290665976623e-05. Weight decay is 0.14123945365551777. dropout is 0.6130413383059504\n",
      " Val aus is 0.5413981444204128. Train auc is 0.549275396670923\n",
      "Learning rate is 0.0003733689352171557. Weight decay is 0.002160412061341069. dropout is 0.01542396692177761\n",
      " Val aus is 0.5405147028722683. Train auc is 0.9903297165555587\n",
      "Learning rate is 0.001900877725788026. Weight decay is 0.05682903368288279. dropout is 0.0540283828721182\n",
      " Val aus is 0.5331663590767064. Train auc is 0.8979339425923258\n",
      "Learning rate is 7.014159805540533e-06. Weight decay is 0.04854854165407018. dropout is 0.44588400993212196\n",
      " Val aus is 0.5319980727964375. Train auc is 0.5560735046851301\n",
      "Learning rate is 4.544261337296862e-06. Weight decay is 3.04554289802152. dropout is 0.28676332243586367\n",
      " Val aus is 0.5306594723333626. Train auc is 0.5035423506002611\n",
      "Learning rate is 0.0018309948209134302. Weight decay is 0.22325126914162188. dropout is 0.01598267485227023\n",
      " Val aus is 0.5301217800535519. Train auc is 0.787134869345429\n",
      "Learning rate is 3.65330913995289e-06. Weight decay is 0.0166582418441922. dropout is 0.5490905784937966\n",
      " Val aus is 0.5282901733578058. Train auc is 0.5451618333594357\n",
      "Learning rate is 1.5106280337897548e-06. Weight decay is 0.01602868285913926. dropout is 0.2546604338370424\n",
      " Val aus is 0.5276546877487778. Train auc is 0.539891212850796\n",
      "Learning rate is 2.798165257167215e-05. Weight decay is 0.5347286550336511. dropout is 0.3859584612079314\n",
      " Val aus is 0.5261230278748315. Train auc is 0.5338411199217475\n",
      "Learning rate is 2.4022993161278127e-05. Weight decay is 203.06247599965764. dropout is 0.20115112323477857\n",
      " Val aus is 0.5216965957431539. Train auc is 0.49244751970626816\n",
      "Learning rate is 6.044237095310103e-06. Weight decay is 25.2892790591031. dropout is 0.15761964670455642\n",
      " Val aus is 0.5214751038574703. Train auc is 0.5139788994030765\n",
      "Learning rate is 8.732661453272546e-06. Weight decay is 0.015198859565749082. dropout is 0.8536759460780138\n",
      " Val aus is 0.521325690525728. Train auc is 0.5183502301835817\n",
      "Learning rate is 2.043766696705395e-06. Weight decay is 1.6438473825682478. dropout is 0.9284744125949635\n",
      " Val aus is 0.5212392140532226. Train auc is 0.5029285795545582\n",
      "Learning rate is 4.121859976227454e-06. Weight decay is 0.1958491861038254. dropout is 0.7497143006110264\n",
      " Val aus is 0.5202000817760677. Train auc is 0.5151449329150424\n",
      "Learning rate is 6.299197878264026e-06. Weight decay is 24.172350337060692. dropout is 0.23686893766922534\n",
      " Val aus is 0.5189686644256684. Train auc is 0.4905454603639363\n",
      "Learning rate is 6.212576200350693e-05. Weight decay is 20.875928080276577. dropout is 0.9245401165054907\n",
      " Val aus is 0.5174701569777334. Train auc is 0.5128413649428024\n",
      "Learning rate is 6.856621071873929e-05. Weight decay is 15.096960719837686. dropout is 0.3603361315125574\n",
      " Val aus is 0.5172058133981239. Train auc is 0.5038131995110013\n",
      "Learning rate is 5.294511142818919e-06. Weight decay is 1.8941669377270256. dropout is 0.07899763240582547\n",
      " Val aus is 0.5154703887076655. Train auc is 0.49826430997578164\n",
      "Learning rate is 1.3390520497004374e-06. Weight decay is 2.733899158364737. dropout is 0.637037249453082\n",
      " Val aus is 0.5147574279878036. Train auc is 0.49172721733012287\n",
      "Learning rate is 4.541926054519885e-06. Weight decay is 0.03127751958168317. dropout is 0.8173055503052234\n",
      " Val aus is 0.5144247134028591. Train auc is 0.5227305276557334\n",
      "Learning rate is 3.3390745616531355e-06. Weight decay is 0.07799188675171832. dropout is 0.1746285872273856\n",
      " Val aus is 0.509920713440308. Train auc is 0.5301563954422241\n",
      "Learning rate is 9.705969793522295e-06. Weight decay is 26.373580230052266. dropout is 0.8196955372711414\n",
      " Val aus is 0.5085140439385132. Train auc is 0.47579080255498546\n",
      "Learning rate is 6.559577726619036e-06. Weight decay is 11.171410805830547. dropout is 0.7420606591588331\n",
      " Val aus is 0.5066191420331073. Train auc is 0.4961477655476386\n",
      "Learning rate is 1.2275309042370284e-06. Weight decay is 193.45436495936227. dropout is 0.6757156916505088\n",
      " Val aus is 0.5022106324344682. Train auc is 0.4962166898170241\n",
      "Learning rate is 2.937557799885094e-06. Weight decay is 0.24181778398425083. dropout is 0.9636947065131417\n",
      " Val aus is 0.5002828048912558. Train auc is 0.4957897909081861\n",
      "Learning rate is 0.0025742386200934726. Weight decay is 6.850320002649148. dropout is 0.09285809463682404\n",
      " Val aus is 0.5. Train auc is 0.5\n",
      "Learning rate is 5.374937508142974e-05. Weight decay is 53.0419985498642. dropout is 0.9164272379772147\n",
      " Val aus is 0.4994024769822753. Train auc is 0.5054342373282134\n",
      "Learning rate is 3.926423053962582e-06. Weight decay is 0.019412373661134172. dropout is 0.9811476064514884\n",
      " Val aus is 0.4991486496093068. Train auc is 0.5082592856164465\n",
      "Learning rate is 6.37160131502232e-05. Weight decay is 360.76606047119895. dropout is 0.3903792173073096\n",
      " Val aus is 0.4949480383867779. Train auc is 0.4968479410332753\n",
      "Learning rate is 1.7097005461568339e-06. Weight decay is 0.07750058446041747. dropout is 0.4931605065748573\n",
      " Val aus is 0.4940655090033464. Train auc is 0.48830189993892537\n",
      "Learning rate is 2.5443112222643715e-06. Weight decay is 8.403351014022038. dropout is 0.16322023375693728\n",
      " Val aus is 0.49390026059242753. Train auc is 0.517043961558091\n",
      "Learning rate is 2.8336485720738767e-05. Weight decay is 0.34487278294430734. dropout is 0.8908734697052788\n",
      " Val aus is 0.4873858232903998. Train auc is 0.49584853025926473\n",
      "Learning rate is 2.1466692927276458e-05. Weight decay is 246.3961144953998. dropout is 0.8352870764977088\n",
      " Val aus is 0.48559494449848606. Train auc is 0.5063186515319011\n",
      "Learning rate is 4.430318063980075e-06. Weight decay is 2.420414080517866. dropout is 0.7924023737369421\n",
      " Val aus is 0.4821049947894345. Train auc is 0.5073759201590522\n",
      "Learning rate is 2.477641684294093e-05. Weight decay is 62.04750075993437. dropout is 0.2312545509514622\n",
      " Val aus is 0.4776928503146527. Train auc is 0.49501063557714364\n",
      "Learning rate is 3.3931720907214596e-06. Weight decay is 12.366419363285551. dropout is 0.9180339580723893\n",
      " Val aus is 0.47547018181668005. Train auc is 0.5155908656416941\n",
      "Learning rate is 6.268459976871477e-06. Weight decay is 2.5558205581597826. dropout is 0.8368644355131424\n",
      " Val aus is 0.47341383552792143. Train auc is 0.49654387343975354\n",
      "Learning rate is 1.6242320804503657e-06. Weight decay is 3.049371487672463. dropout is 0.49690927080781366\n",
      " Val aus is 0.47258493090462567. Train auc is 0.49904922564485915\n",
      "Learning rate is 3.579963047217718e-05. Weight decay is 17.736147240888567. dropout is 0.8677209711669119\n",
      " Val aus is 0.4709738487990159. Train auc is 0.49795454302418984\n",
      "Learning rate is 9.108388560257222e-06. Weight decay is 0.45670916648916166. dropout is 0.6806163586323362\n",
      " Val aus is 0.463705433436414. Train auc is 0.48596411075872875\n"
     ]
    }
   ],
   "source": [
    "for i in sorted(train_hist, key =lambda x:x[0], reverse=True):\n",
    "    print(train_hist[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finer Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MY_NN import NeuralNetwork\n",
    "from datetime import datetime\n",
    "best_net = None\n",
    "best_auc =0\n",
    "\n",
    "#10.22. for 4 layers, best one has lr 3.305e-4, wd is 9.904e-3 0.6374\n",
    "input_size = 237\n",
    "hidden_size= [230,220,210]\n",
    "lr_decay = {'step_size': 5, 'gamma':0.5}\n",
    "\n",
    "train_hist={}\n",
    "for i in range(5):\n",
    "    tic = datetime.now()\n",
    "#     dropout = np.random.uniform(0.5,1)\n",
    "#     weight_decay = 10** (np.random.uniform(-1,1))#L2 \n",
    "#     learning_rate = 10** (np.random.uniform(-4,-2))\n",
    "    \n",
    "    \n",
    "    dropout = 0.8\n",
    "    weight_decay = 10** (np.random.uniform(-1,1))#L2 \n",
    "    learning_rate = 10** (np.random.uniform(-4,-2))\n",
    "    \n",
    "    nn_model = NeuralNetwork(input_size = input_size, hidden_size=hidden_size,\n",
    "                             learning_rate = learning_rate,num_epochs=10,\n",
    "                             verbose=None,dropout=dropout,\n",
    "                             weight_decay=weight_decay,lr_decay=lr_decay)\n",
    "    nn_model.train(data)\n",
    "    describe= 'Learning rate is {}. Weight decay is {}. dropout is {}\\n Val aus is {}. Train auc is {}' \\\n",
    "                .format(learning_rate, weight_decay, dropout,nn_model.auc_history['val'][-1],nn_model.auc_history['train'][-1])\n",
    "    train_hist[(nn_model.auc_history['val'][-1],nn_model.auc_history['train'][-1])]= describe\n",
    "    print(describe)\n",
    "    if nn_model.auc_history['val'][-1]> best_auc:\n",
    "        best_auc =nn_model.auc_history['val'][-1]\n",
    "        best_net = nn_model\n",
    "    toc = datetime.now()\n",
    "    print('This is round you consume {} time to run this model.'.format(toc-tic))\n",
    "    print('You have finished {}!!'.format(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3oAAALJCAYAAADielEXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f620d3101d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_vis.nnplot(nn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try a specific one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We train for ensemble this time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MY_NN import NeuralNetwork\n",
    "from datetime import datetime\n",
    "\n",
    "best_net = None\n",
    "best_auc =0\n",
    "\n",
    "#10.22. for 4 layers, best one has lr 3.305e-4, wd is 9.904e-3 0.6374\n",
    "input_size = 237\n",
    "hidden_size= [230,220,210]\n",
    "lr_decay = {'step_size': 25, 'gamma':0.1}\n",
    "for i in range(5):\n",
    "    #learnning_rate 5e-4 too large\n",
    "    train_hist={}\n",
    "    tic = datetime.now()\n",
    "    weight_decay = 10** (np.random.uniform(-3,-1))#L2 \n",
    "    learning_rate = 10** (np.random.uniform(-4,np.log10(5e-4)))\n",
    "    dropout = np.random.uniform(0,1)\n",
    "    nn_model = NeuralNetwork(data,input_size = input_size, hidden_size=hidden_size,learning_rate = learning_rate,num_epochs=70,verbose=None,dropout=dropout,\n",
    "                             weight_decay=weight_decay,lr_decay=lr_decay ,batchnorm=True)\n",
    "    nn_model.train()\n",
    "    describe= 'Learning rate is {}. Weight decay is {}. dropout is {}\\n Val aus is {}. Train auc is {}' \\\n",
    "                .format(learning_rate, weight_decay, dropout,nn_model.auc_history['val'][-1],nn_model.auc_history['train'][-1])\n",
    "\n",
    "    print(describe)\n",
    "    train_hist['describe']= describe\n",
    "    train_hist['net'] = nn_model\n",
    "    toc = datetime.now()\n",
    "    print('This is round you consume {} time to run this model.'.format(toc-tic))\n",
    "    print('You have finished {}!!'.format(i+1))\n",
    "\n",
    "    filename= 'search_lr_wd_ensemble{}.pkl'.format(i)\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(train_hist, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm search_lr_wd_ensemble0.pkl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
