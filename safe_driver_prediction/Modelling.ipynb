{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.2 (default, Aug 18 2017, 17:48:00) \n",
      "[GCC 5.4.0 20160609]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mThe directory '/home/FDSM_lhn/.cache/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "\u001b[33mThe directory '/home/FDSM_lhn/.cache/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "Collecting scipy\n",
      "  Downloading scipy-0.19.1-cp35-cp35m-manylinux1_x86_64.whl (47.9MB)\n",
      "\u001b[K    100% |████████████████████████████████| 47.9MB 33kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.8.2 in /home/FDSM_lhn/.local/lib/python3.5/site-packages (from scipy)\n",
      "Installing collected packages: scipy\n",
      "Successfully installed scipy-0.19.1\n"
     ]
    }
   ],
   "source": [
    "#! pip3 install pandas\n",
    "! sudo pip3 install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train = pd.read_csv('datasets/train.csv')\n",
    "\n",
    "# # train.head\n",
    "# train.iloc[:10,:10]\n",
    "# N, _ = train.shape\n",
    "\n",
    "# for i in range(3):\n",
    "#     file = \"datasets/train_batch_{}\".format(i)\n",
    "#     with open(file,'wb') as f:\n",
    "#         pickle.dump(train.iloc[int(2e5)*i:min(N+1,int(2e5) * (i+1)),:] ,file = f)\n",
    "    \n",
    "# test = pd.read_csv('datasets/test.csv')\n",
    "# test.shape\n",
    "\n",
    "# N, _ = test.shape\n",
    "\n",
    "# for i in range(5):\n",
    "#     file = \"datasets/test_batch_{}\".format(i)\n",
    "#     with open(file,'wb') as f:\n",
    "#         pickle.dump(test.iloc[int(2e5)*i:min(N+1,int(2e5) * (i+1)),:] ,file = f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/model_selection/_split.py:2010: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import data_util\n",
    "import data_preprocess\n",
    "import datetime\n",
    "\n",
    "tic= datetime.datetime.now()\n",
    "train_data = data_util.load_train_data()\n",
    "test_data= data_util.load_test_data()\n",
    "\n",
    "naive_pre = data_preprocess.naive_preprocess()\n",
    "train_data = naive_pre.dtype_change(train_data)\n",
    "#train_prop is specified\n",
    "X_train, X_val, y_train, y_val = data_util.split_train(train_data,prop=0.75)\n",
    "\n",
    "X_train = naive_pre.scale(X_train)\n",
    "X_val = naive_pre.scale(X_val,test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(446409, 235) (148803, 235)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape ,X_val.shape)\n",
    "# dev_train.dtypes\n",
    "# dev_train.head\n",
    "# train_data.isnull().any()\n",
    "import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100, 100, 100), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=<mtrand.RandomState object at 0x7fb955bcc480>,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "tic= datetime.datetime.now()\n",
    "hidden_layer_size = [(300,200,100)]\n",
    "learning_rate = [0.001]\n",
    "reg = []\n",
    "momentum =[0.9]\n",
    "rso= np.random.RandomState(66)\n",
    "\n",
    "#for now I don't mess up with inits\n",
    "mlp = MLPClassifier(hidden_layer_sizes= hidden_layer_size[0],random_state=rso,alpha= 0.001)\n",
    "mlp.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The val score it have: 0.5953008753764144 \n",
      "The time it cost: -1 day, 23:57:44.336906\n",
      "The test score it have: 0.7112570696063083 \n",
      "The time it cost: 0:29:13.188801\n"
     ]
    }
   ],
   "source": [
    "#check the performance\n",
    "from sklearn.metrics import classification_report,confusion_matrix,roc_auc_score\n",
    "test_predictions = mlp.predict_proba(X_val)\n",
    "auc_score = roc_auc_score(y_val,test_predictions[:,1])\n",
    "\n",
    "print('The val score it have: {} '.format(auc_score))\n",
    "print(\"The time it cost: {}\".format(toc-tic))\n",
    "\n",
    "test_predictions = mlp.predict_proba(X_train)\n",
    "auc_score = roc_auc_score(y_train,test_predictions[:,1])\n",
    "toc = datetime.datetime.now()\n",
    "print('The test score it have: {} '.format(auc_score))\n",
    "print(\"The time it cost: {}\".format(toc-tic))\n",
    "\n",
    "#classification_report(y_train,test_predictions[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.62780916443936541"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_val,test_predictions[:,1])\n",
    "# sum(test_predictions)\n",
    "# sum(dev_y)\n",
    "# test_predictions\n",
    "# sum(test_predictions[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 30)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_layer_size[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try Different Package for NN"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#! sudo pip3 install scikit-neuralnetwork\n",
    "!sudo pip3 install scikit-neuralnetwork --upgrade\n",
    "!sudo pip3 install Theano --upgrade"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_data = data_util.load_train_data()\n",
    "test_data= data_util.load_test_data()\n",
    "#train_prop is specified\n",
    "X_train, X_val, y_train, y_val = data_util.split_train(train_data,prop=0.75)\n",
    "train_mean = X_train.mean()\n",
    "X_train.fillna(train_mean)\n",
    "X_val.fillna(train_mean)\n",
    "\n",
    "weight = np.zeros_like(y_train)\n",
    "weight[y_train==1]=100\n",
    "weight[y_train==0]=1\n",
    "\n",
    "from sknn.mlp import Classifier, Layer\n",
    "\n",
    "nn= Classifier(\n",
    "    layers= [\n",
    "        Layer('Rectifier', units= 100),\n",
    "        Layer('Rectifier', units= 80),\n",
    "        Layer('Rectifier', units= 50),\n",
    "        Layer('Softmax')],\n",
    "    learning_rate=0.000000,\n",
    "    batch_size = 2000,\n",
    "    n_iter= 20,\n",
    "    regularize= 'L2'\n",
    ")\n",
    "\n",
    "nn.fit(X_train, y_train)\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "pipeline = Pipeline([\n",
    "        ('min/max scaler', MinMaxScaler(feature_range=(-1, 1.0))),\n",
    "        ('neural network', nn)])\n",
    "pipeline.fit(X_train, y_train,weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.utils.data as Data\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "input_size = 235\n",
    "hidden_size = [300,200,100]\n",
    "num_classes = 2\n",
    "num_epochs = 5\n",
    "batch_size = 10000\n",
    "learning_rate = 0.0005\n",
    "weight_decay = 1e-5\n",
    "\n",
    "X_dev_tensor = torch.from_numpy(X_dev.values)\n",
    "y_dev_tensor = torch.from_numpy(y_dev.values)\n",
    "\n",
    "X_dev_tensor= X_dev_tensor.float()\n",
    "y_dev_tensor =y_dev_tensor.type(torch.LongTensor)\n",
    "#print(X_dev_tensor)\n",
    "X_dev_Variable = Variable(X_dev_tensor)\n",
    "y_dev_Variable = Variable(y_dev_tensor)\n",
    "\n",
    "\n",
    "dev_set = Data.TensorDataset(data_tensor=X_dev_tensor, target_tensor=y_dev_tensor )\n",
    "\n",
    "\n",
    "\n",
    "dev_loader = Data.DataLoader(\n",
    "    dataset=dev_set,      \n",
    "    batch_size=batch_size,      # mini batch size\n",
    "    shuffle=True,               # random shuffle for training\n",
    "    num_workers=2)              # subprocesses for loading data\n",
    "\n",
    "\n",
    "# train_loader = Data.DataLoader(dataset=train_dataset, \n",
    "#                                            batch_size=batch_size, \n",
    "#                                            shuffle=True)\n",
    "\n",
    "# test_loader = Data.DataLoader(dataset=test_dataset, \n",
    "#                                           batch_size=batch_size, \n",
    "#                                           shuffle=False)\n",
    "    \n",
    "class My_Net(nn.Module):\n",
    "    def __init__(self,input_size, hidden_size, num_classes):\n",
    "        super(My_Net,self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size,hidden_size[0])\n",
    "        self.fc2 = nn.Linear(hidden_size[0],hidden_size[1])\n",
    "        self.fc3 = nn.Linear(hidden_size[1],hidden_size[2])\n",
    "        self.fc4 = nn.Linear(hidden_size[2], num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc4(out)\n",
    "        return out\n",
    "        \n",
    "net = My_Net(input_size, hidden_size,num_classes)\n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate,weight_decay=weight_decay)\n",
    "\n",
    "for epoch in range(1):\n",
    "    for i,  (batch_x, batch_y) in enumerate(dev_loader):\n",
    "#         print(batch_x)\n",
    "        x,  y = Variable(batch_x), Variable(batch_y)\n",
    "        out = net(x)\n",
    "        loss = criterion(out, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 10 ==0:\n",
    "            print('epoch {}: iteration {}, the loss is {}'.format(epoch,i ,loss))\n",
    "\n",
    "from sklearn.metrics import roc_auc_score            \n",
    "#try to test it's accuracy\n",
    "out=  net(X_dev_Variable)\n",
    "out = F.softmax(out)\n",
    "roc_auc_score(y_dev_Variable.data.numpy(), out.data.numpy()[:,1])\n",
    "print(out)\n",
    "\n",
    "\n",
    "\n",
    "#So here I've tested that with Variable type, I can run this function\n",
    "# for t in range(100):\n",
    "#     out = net(X_dev_tensor)                 # input x and predict based on x\n",
    "#     loss = criterion(out, y_dev_tensor)     # must be (1. nn output, 2. target), the target label is NOT one-hotted\n",
    "\n",
    "#     optimizer.zero_grad()   # clear gradients for next train\n",
    "#     loss.backward()         # backpropagation, compute gradients\n",
    "#     optimizer.step()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
